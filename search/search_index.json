{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The OpenVoiceOS Technical Manual the OVOS project documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests ! User oriented docs are automatically published at https://openvoiceos.github.io/community-docs Dev oriented docs (this repo) are automatically published at https://openvoiceos.github.io/ovos-technical-manual","title":"Introduction"},{"location":"#the-openvoiceos-technical-manual","text":"the OVOS project documentation is written and maintained by users just like you! Think of these docs both as your starting point and also forever changing and incomplete Please open Issues and Pull Requests ! User oriented docs are automatically published at https://openvoiceos.github.io/community-docs Dev oriented docs (this repo) are automatically published at https://openvoiceos.github.io/ovos-technical-manual","title":"The OpenVoiceOS Technical Manual"},{"location":"OCA/","text":"OCA - OVOS Config Assistant OCA is a user facing interface to configure ovos devices Web UI OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more CLI A command line interface is planned but not yet available to provide equivalent functionality to the Web UI Python utils from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"Config Assistant"},{"location":"OCA/#oca-ovos-config-assistant","text":"OCA is a user facing interface to configure ovos devices","title":"OCA - OVOS Config Assistant"},{"location":"OCA/#web-ui","text":"OCA provides a local Web UI similar to ovos-backend-manager, in here you can configure your device, view metrics, handle OAuth and more","title":"Web UI"},{"location":"OCA/#cli","text":"A command line interface is planned but not yet available to provide equivalent functionality to the Web UI","title":"CLI"},{"location":"OCA/#python-utils","text":"from ovos_config_assistant.module_helpers import pprint_core_module_info pprint_core_module_info() \"\"\" ## Mycroft module info can import mycroft : True is ovos-core : True mycroft module location: /home/user/ovos-core/mycroft ## Downstream ovos.conf overrides Module: neon_core can import neon_core : False neon_core module location: None xdg compliance : True base xdg folder : neon mycroft config filename : neon.conf default mycroft.conf path : /home/user/NeonCore/neon_core/configuration/neon.conf Module: hivemind can import hivemind : False hivemind module location: None xdg compliance : True base xdg folder : hivemind mycroft config filename : hivemind.conf default mycroft.conf path : /home/user/PycharmProjects/ovos_workspace/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf ## Downstream module overrides: Module: neon_speech uses config from : neon_core can import neon_speech : False neon_speech module location: None Module: neon_audio uses config from : neon_core can import neon_audio : False neon_audio module location: None Module: neon_enclosure uses config from : neon_core can import neon_enclosure : False neon_enclosure module location: None Module: hivemind_voice_satellite uses config from : hivemind can import hivemind_voice_satellite : True hivemind_voice_satellite module location: /home/user/HiveMind-voice-sat/hivemind_voice_satellite \"\"\" from ovos_config_assistant.config_helpers import pprint_ovos_conf pprint_ovos_conf() \"\"\" ## OVOS Configuration ovos.conf exists : True /home/user/.config/OpenVoiceOS/ovos.conf xdg compliance : True base xdg folder : mycroft mycroft config filename : mycroft.conf default mycroft.conf path : /home/user/ovos-core/.venv/lib/python3.9/site-packages/mycroft/configuration/mycroft.conf \"\"\"","title":"Python utils"},{"location":"OCP/","text":"OCP OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player Introduction OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from a typical mycroft-core audio service in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill OCP skills have a dedicated MycroftSkill subclass and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP mycroft-core CommonPlay skill framework is disabled when OCP loads WARNING : this will be removed in ovos-core 0.1.0 or earlier mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills WARNING : this will be removed in ovos-core 0.1.0 or earlier OCP Skills Skills provide search results, think about them as media providers/catalogs for OCP You can find OCP skills in the awesome-ocp-skills list Skills Menu Some skills provide featured_media, you can access these from the OCP menu Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework. File Browser integration selected files will be played in OCP folders are considered playlists MPRIS integration Sync with external players Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify Manage multiple players If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal) Configuration GUI Some OCP settings are exposed via the GUI Visualization can be either a wave or bars, just tap on it to change Timeout settings, return to homescreen widget after 30 seconds Advanced OCP contains an audio service plugin component that acts as a compatibility layer with MycroftAI CommonPlay skills framework. in mycroft-core you can set OCP as default-backend , ovos-core is not required for OCP For compatibility and historical reasons OCP reads its configuration from the \"Audio\" section \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true // all values below are optional // plugin config // operational mode refers to the OCP integration // \"external\" - OCP is already running elsewhere, connect by bus only // \"native\" - launch OCP service from the plugin // \"auto\" - if OCP is already running connect to it, else launch it // you should only change this if you want to run OCP as a standalone system service \"mode\": \"auto\", // MPRIS integrations // integration is enabled by default, but can be disabled \"disable_mpris\": False, // dbus type for MPRIS, \"session\" or \"system\" \"dbus_type\": \"session\", // allow OCP to control MPRIS enabled 3rd party applications // voice enable them (next/prev/stop/resume..) // and stop them when OCP starts it's own playback // NOTE: OCP can be controlled itself via MPRIS independentely of this setting \"manage_external_players\": False, // Playback settings // AUTO = 0 - play each entry as considered appropriate, // ie, make it happen the best way possible // AUDIO_ONLY = 10 - only consider audio entries // VIDEO_ONLY = 20 - only consider video entries // FORCE_AUDIO = 30 - cast video to audio unconditionally // (audio can still play in mycroft-gui) // FORCE_AUDIOSERVICE = 40 - cast everything to audio service backend, // mycroft-gui will not be used // EVENTS_ONLY = 50 - only emit ocp events, // do not display or play anything. // allows integration with external interfaces \"playback_mode\": 0, // ordered list of audio backend preferences, // when OCP selects a audio service for playback // this list is checked in order until a available backend is found \"preferred_audio_services\": [\"vlc\", \"mplayer\", \"simple\"], // when media playback ends \"click next\" \"autoplay\": True, // if True behaves as if the search results are part of the playlist // eg: // - click next in last track -> play next search result // - end of playlist + autoplay -> play next search result \"merge_search\": True, // search params // minimum time to wait for skill replies, // after this time, if at least 1 result was // found, selection is triggered \"min_timeout\": 5, // maximum time to wait for skill replies, // after this time, regardless of number of // results, selection is triggered \"max_timeout\": 15, // ignore results below min_Score \"min_score\": 50, // stop collecting results if we get a // match with confidence >= early_stop_thresh \"early_stop_thresh\": 85, // sleep this amount before early stop, // allows skills that \"just miss\" to also be taken into account \"early_stop_grace_period\": 0.5, // if True emits the regular mycroft-core // bus messages to get results from \"old style\" skills \"backwards_compatibility\": True, // allow skills to request more time, // extends min_timeout for individual queries (up to max_timeout) \"allow_extensions\": True, // if no results for a MediaType, perform a second query with MediaType.GENERIC \"search_fallback\": True, // stream extractor settings // how to handle bandcamp streams // \"pybandcamp\", \"youtube-dl\" \"bandcamp_backend\": \"pybandcamp\", // how to handle youtube streams // \"youtube-dl\", \"pytube\", \"pafy\", \"invidious\" \"youtube_backend\": \"invidious\", // the url to the invidious instance to be used\" // by default uses a random instance \"invidious_host\": None, // get final stream locally or from where invidious is hosted // This partially allows bypassing geoblocked content, // but it is a global flag, not per entry. \"invidious_proxy\": False, // different forks of youtube-dl are supported // \"yt-dlp\", \"youtube-dl\", \"youtube-dlc\" \"ydl_backend\": \"yt-dlp\", // how to extract live streams from a youtube channel // \"pytube\", \"youtube_searcher\", \"redirect\", \"youtube-dl\" // uses youtube auto redirect https://www.youtube.com/{channel_name}/live \"youtube_live_backend\": \"redirect\" } }","title":"Common Play"},{"location":"OCP/#ocp","text":"OCP stands for OpenVoiceOS Common Play, it is a full-fledged media player","title":"OCP"},{"location":"OCP/#introduction","text":"OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from a typical mycroft-core audio service in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill OCP skills have a dedicated MycroftSkill subclass and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP mycroft-core CommonPlay skill framework is disabled when OCP loads WARNING : this will be removed in ovos-core 0.1.0 or earlier mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills WARNING : this will be removed in ovos-core 0.1.0 or earlier","title":"Introduction"},{"location":"OCP/#ocp-skills","text":"Skills provide search results, think about them as media providers/catalogs for OCP You can find OCP skills in the awesome-ocp-skills list","title":"OCP Skills"},{"location":"OCP/#skills-menu","text":"Some skills provide featured_media, you can access these from the OCP menu","title":"Skills Menu"},{"location":"OCP/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"OCP/#file-browser-integration","text":"selected files will be played in OCP folders are considered playlists","title":"File Browser integration"},{"location":"OCP/#mpris-integration","text":"","title":"MPRIS integration"},{"location":"OCP/#sync-with-external-players","text":"Via MPRIS OCP can control and display data from external players, if using KDEConnect this includes playback in connected devices See a demo here This also includes voice intents, allowing you for example to voice control spotify","title":"Sync with external players"},{"location":"OCP/#manage-multiple-players","text":"If OCP is set to manage external players it will ensure only one of them is playing media at once, if using KDEConnect this includes playback in connected devices See a demo here ( warning : contains black metal)","title":"Manage multiple players"},{"location":"OCP/#configuration","text":"","title":"Configuration"},{"location":"OCP/#gui","text":"Some OCP settings are exposed via the GUI Visualization can be either a wave or bars, just tap on it to change Timeout settings, return to homescreen widget after 30 seconds","title":"GUI"},{"location":"OCP/#advanced","text":"OCP contains an audio service plugin component that acts as a compatibility layer with MycroftAI CommonPlay skills framework. in mycroft-core you can set OCP as default-backend , ovos-core is not required for OCP For compatibility and historical reasons OCP reads its configuration from the \"Audio\" section \"Audio\": { \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true // all values below are optional // plugin config // operational mode refers to the OCP integration // \"external\" - OCP is already running elsewhere, connect by bus only // \"native\" - launch OCP service from the plugin // \"auto\" - if OCP is already running connect to it, else launch it // you should only change this if you want to run OCP as a standalone system service \"mode\": \"auto\", // MPRIS integrations // integration is enabled by default, but can be disabled \"disable_mpris\": False, // dbus type for MPRIS, \"session\" or \"system\" \"dbus_type\": \"session\", // allow OCP to control MPRIS enabled 3rd party applications // voice enable them (next/prev/stop/resume..) // and stop them when OCP starts it's own playback // NOTE: OCP can be controlled itself via MPRIS independentely of this setting \"manage_external_players\": False, // Playback settings // AUTO = 0 - play each entry as considered appropriate, // ie, make it happen the best way possible // AUDIO_ONLY = 10 - only consider audio entries // VIDEO_ONLY = 20 - only consider video entries // FORCE_AUDIO = 30 - cast video to audio unconditionally // (audio can still play in mycroft-gui) // FORCE_AUDIOSERVICE = 40 - cast everything to audio service backend, // mycroft-gui will not be used // EVENTS_ONLY = 50 - only emit ocp events, // do not display or play anything. // allows integration with external interfaces \"playback_mode\": 0, // ordered list of audio backend preferences, // when OCP selects a audio service for playback // this list is checked in order until a available backend is found \"preferred_audio_services\": [\"vlc\", \"mplayer\", \"simple\"], // when media playback ends \"click next\" \"autoplay\": True, // if True behaves as if the search results are part of the playlist // eg: // - click next in last track -> play next search result // - end of playlist + autoplay -> play next search result \"merge_search\": True, // search params // minimum time to wait for skill replies, // after this time, if at least 1 result was // found, selection is triggered \"min_timeout\": 5, // maximum time to wait for skill replies, // after this time, regardless of number of // results, selection is triggered \"max_timeout\": 15, // ignore results below min_Score \"min_score\": 50, // stop collecting results if we get a // match with confidence >= early_stop_thresh \"early_stop_thresh\": 85, // sleep this amount before early stop, // allows skills that \"just miss\" to also be taken into account \"early_stop_grace_period\": 0.5, // if True emits the regular mycroft-core // bus messages to get results from \"old style\" skills \"backwards_compatibility\": True, // allow skills to request more time, // extends min_timeout for individual queries (up to max_timeout) \"allow_extensions\": True, // if no results for a MediaType, perform a second query with MediaType.GENERIC \"search_fallback\": True, // stream extractor settings // how to handle bandcamp streams // \"pybandcamp\", \"youtube-dl\" \"bandcamp_backend\": \"pybandcamp\", // how to handle youtube streams // \"youtube-dl\", \"pytube\", \"pafy\", \"invidious\" \"youtube_backend\": \"invidious\", // the url to the invidious instance to be used\" // by default uses a random instance \"invidious_host\": None, // get final stream locally or from where invidious is hosted // This partially allows bypassing geoblocked content, // but it is a global flag, not per entry. \"invidious_proxy\": False, // different forks of youtube-dl are supported // \"yt-dlp\", \"youtube-dl\", \"youtube-dlc\" \"ydl_backend\": \"yt-dlp\", // how to extract live streams from a youtube channel // \"pytube\", \"youtube_searcher\", \"redirect\", \"youtube-dl\" // uses youtube auto redirect https://www.youtube.com/{channel_name}/live \"youtube_live_backend\": \"redirect\" } }","title":"Advanced"},{"location":"OPM/","text":"OPM OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Plugin Packaging Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} ) Projects using OPM OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"Plugin Manager"},{"location":"OPM/#opm","text":"OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent of core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py","title":"OPM"},{"location":"OPM/#plugin-packaging","text":"Plugins need to define one entrypoint with their plugin type and plugin class # OPM recognized plugin types class PluginTypes(str, Enum): PHAL = \"ovos.plugin.phal\" ADMIN = \"ovos.plugin.phal.admin\" SKILL = \"ovos.plugin.skill\" VAD = \"ovos.plugin.VAD\" PHONEME = \"ovos.plugin.g2p\" AUDIO = 'mycroft.plugin.audioservice' STT = 'mycroft.plugin.stt' TTS = 'mycroft.plugin.tts' WAKEWORD = 'mycroft.plugin.wake_word' TRANSLATE = \"neon.plugin.lang.translate\" LANG_DETECT = \"neon.plugin.lang.detect\" UTTERANCE_TRANSFORMER = \"neon.plugin.text\" METADATA_TRANSFORMER = \"neon.plugin.metadata\" AUDIO_TRANSFORMER = \"neon.plugin.audio\" QUESTION_SOLVER = \"neon.plugin.solver\" COREFERENCE_SOLVER = \"intentbox.coreference\" KEYWORD_EXTRACTION = \"intentbox.keywords\" UTTERANCE_SEGMENTATION = \"intentbox.segmentation\" TOKENIZATION = \"intentbox.tokenization\" POSTAG = \"intentbox.postag\" plugins can also optionally provide metadata about language support and sample configs via the {plugin_type}.config entrypoint A typical setup.py for a plugin looks like this from setuptools import setup ### replace this data with your plugin specific info PLUGIN_TYPE = \"mycroft.plugin.stt\" # see Enum above PLUGIN_NAME = \"ovos-stt-plugin-name\" PLUGIN_PKG = PLUGIN_NAME.replace(\"-\", \"_\") PLUGIN_CLAZZ = \"MyPlugin\" PLUGIN_CONFIGS = \"MyPluginConfig\" ### PLUGIN_ENTRY_POINT = f'{PLUGIN_NAME} = {PLUGIN_PKG}:{PLUGIN_CLAZZ}' CONFIG_ENTRY_POINT = f'{PLUGIN_NAME}.config = {PLUGIN_PKG}:{PLUGIN_CONFIGS}' # add version, author, license, description.... setup( name=PLUGIN_NAME, version='0.1.0', packages=[PLUGIN_PKG], install_requires=[\"speechrecognition>=3.8.1\", \"ovos-plugin-manager>=0.0.1\"], keywords='mycroft ovos plugin', entry_points={PLUGIN_TYPE: PLUGIN_ENTRY_POINT, f'{PLUGIN_TYPE}.config': CONFIG_ENTRY_POINT} )","title":"Plugin Packaging"},{"location":"OPM/#projects-using-opm","text":"OPM plugins are know to be natively supported by the following projects (non-exhaustive list) ovos-core ovos-local-backend ovos-tts-server ovos-stt-http-server ovos-translate-server neon-core HiveMind voice satellite Additionally, some plugins (AudioService, WakeWord, TTS and STT) are also backwards compatible with mycroft-core","title":"Projects using OPM"},{"location":"PHAL/","text":"PHAL PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images Plugins Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-mk2 mycroft mark2 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-brightness-control-rpi brightness control ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) ovos-PHAL-plugin-dashboard dashboard control (ovos-shell) ovos-PHAL-plugin-notification-widgets system notifications (ovos-shell) ovos-PHAL-plugin-color-scheme-manager GUI color schemes (ovos-shell) ovos-PHAL-plugin-configuration-provider UI to edit mycroft.conf (ovos-shell) ovos-PHAL-plugin-analog-media-devices video/audio capture devices (OCP) Developers In mycroft-core the equivalent to PHAL plugins would usually be shipped as skills or hardcoded in OVOS sometimes it may be unclear if we should develop a skill or plugin, there isn't a one size fits all answer, in some circumstances it may make sense to create both a plugin and a companion skill Template PHAL plugins do not follow a strict template, they are usually event listeners that perform certain actions and integrate with other components from mycroft_bus_client import Message from ovos_plugin_manager.phal import PHALPlugin class MyPHALPluginValidator: @staticmethod def validate(config=None): \"\"\" this method is called before loading the plugin. If it returns False the plugin is not loaded. This allows a plugin to run platform checks\"\"\" return True class MyPHALPlugin(PHALPlugin): validator = MyPHALPluginValidator def __init__(self, bus=None, config=None): super().__init__(bus=bus, name=\"ovos-PHAL-plugin-NAME\", config=config) # register events for plugin self.bus.on(\"my.event\", self.handle_event) def handle_event(self, message): # TODO plugin stuff self.bus.emit(Message(\"my.event.response\")) def shutdown(self): # cleanly remove any event listeners and perform shutdown actions self.bus.remove(\"my.event\", self.handle_event) super().shutdown() You can find plugin packaging documentation here Admin PHAL AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions. Service to use AdminPHAL create a launcher /usr/libexec/mycroft-systemd-admin-phal import sdnotify from ovos_PHAL.admin import main n = sdnotify.SystemdNotifier() def notify_ready(): n.notify('READY=1') print('Startup of Admin service complete') def notify_stopping(): n.notify('STOPPING=1') print('Stopping Admin service') main(ready_hook=notify_ready, stopping_hook=notify_stopping) and system service /usr/lib/systemd/user/mycroft-admin-phal.service [Unit] Description=Admin PHAL PartOf=mycroft.service After=mycroft-messagebus.service [Service] Type=notify TimeoutStopSec=30 Restart=always User=root ExecStart=/usr/libexec/mycroft-systemd-admin-phal [Install] WantedBy=mycroft.service Plugins AdminPlugins are just like regular PHAL plugins that run with root privileges A plugin needs to identify itself as an admin plugin via it's entry point, PHAL will not load Admin plugins and AdminPHAL will not load regular plugins Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"Platform/Hardware Abstraction Layer"},{"location":"PHAL/#phal","text":"PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images","title":"PHAL"},{"location":"PHAL/#plugins","text":"Platform/Hardware specific integrations are loaded by PHAL, these plugins can handle all sorts of system activities Plugin Description ovos-PHAL-plugin-alsa volume control ovos-PHAL-plugin-system reboot / shutdown / factory reset ovos-PHAL-plugin-mk1 mycroft mark1 integration ovos-PHAL-plugin-mk2 mycroft mark2 integration ovos-PHAL-plugin-respeaker-2mic respeaker 2mic hat integration ovos-PHAL-plugin-respeaker-4mic respeaker 4mic hat integration ovos-PHAL-plugin-wifi-setup wifi setup (central plugin) ovos-PHAL-plugin-gui-network-client wifi setup (GUI interface) ovos-PHAL-plugin-balena-wifi wifi setup (hotspot) ovos-PHAL-plugin-network-manager wifi setup (network manager) ovos-PHAL-plugin-brightness-control-rpi brightness control ovos-PHAL-plugin-ipgeo automatic geolocation (IP address) ovos-PHAL-plugin-gpsd automatic geolocation (GPS) ovos-PHAL-plugin-dashboard dashboard control (ovos-shell) ovos-PHAL-plugin-notification-widgets system notifications (ovos-shell) ovos-PHAL-plugin-color-scheme-manager GUI color schemes (ovos-shell) ovos-PHAL-plugin-configuration-provider UI to edit mycroft.conf (ovos-shell) ovos-PHAL-plugin-analog-media-devices video/audio capture devices (OCP)","title":"Plugins"},{"location":"PHAL/#developers","text":"In mycroft-core the equivalent to PHAL plugins would usually be shipped as skills or hardcoded in OVOS sometimes it may be unclear if we should develop a skill or plugin, there isn't a one size fits all answer, in some circumstances it may make sense to create both a plugin and a companion skill","title":"Developers"},{"location":"PHAL/#template","text":"PHAL plugins do not follow a strict template, they are usually event listeners that perform certain actions and integrate with other components from mycroft_bus_client import Message from ovos_plugin_manager.phal import PHALPlugin class MyPHALPluginValidator: @staticmethod def validate(config=None): \"\"\" this method is called before loading the plugin. If it returns False the plugin is not loaded. This allows a plugin to run platform checks\"\"\" return True class MyPHALPlugin(PHALPlugin): validator = MyPHALPluginValidator def __init__(self, bus=None, config=None): super().__init__(bus=bus, name=\"ovos-PHAL-plugin-NAME\", config=config) # register events for plugin self.bus.on(\"my.event\", self.handle_event) def handle_event(self, message): # TODO plugin stuff self.bus.emit(Message(\"my.event.response\")) def shutdown(self): # cleanly remove any event listeners and perform shutdown actions self.bus.remove(\"my.event\", self.handle_event) super().shutdown() You can find plugin packaging documentation here","title":"Template"},{"location":"PHAL/#admin-phal","text":"AdminPHAL performs the exact same function as PHAL, but plugins it loads will have root privileges. This service is intended for handling any OS-level interactions requiring escalation of privileges. Be very careful when installing Admin plugins and scrutinize them closely NOTE: Because this service runs as root, plugins it loads are responsible for not writing configuration changes which would result in breaking config file permissions.","title":"Admin PHAL"},{"location":"PHAL/#service","text":"to use AdminPHAL create a launcher /usr/libexec/mycroft-systemd-admin-phal import sdnotify from ovos_PHAL.admin import main n = sdnotify.SystemdNotifier() def notify_ready(): n.notify('READY=1') print('Startup of Admin service complete') def notify_stopping(): n.notify('STOPPING=1') print('Stopping Admin service') main(ready_hook=notify_ready, stopping_hook=notify_stopping) and system service /usr/lib/systemd/user/mycroft-admin-phal.service [Unit] Description=Admin PHAL PartOf=mycroft.service After=mycroft-messagebus.service [Service] Type=notify TimeoutStopSec=30 Restart=always User=root ExecStart=/usr/libexec/mycroft-systemd-admin-phal [Install] WantedBy=mycroft.service","title":"Service"},{"location":"PHAL/#plugins_1","text":"AdminPlugins are just like regular PHAL plugins that run with root privileges A plugin needs to identify itself as an admin plugin via it's entry point, PHAL will not load Admin plugins and AdminPHAL will not load regular plugins Admin plugins will only load if their configuration contains \"enabled\": true . All admin plugins need to be explicitly enabled You can find plugin packaging documentation here","title":"Plugins"},{"location":"audio_plugins/","text":"Audio Plugins Audio plugins are responsible for handling playback of media, like music and podcasts If mycroft-gui is available these plugins will rarely be used unless ovos is explicitly configured to do so List of Audio plugins Plugin Description ovos-ocp-audio-plugin framework + compatibility layer ovos-audio-plugin-simple sox / aplay / paplay / mpg123 ovos-vlc-plugin vlc audio backend ovos-mplayer-plugin mplayer audio backend","title":"Audio Playback"},{"location":"audio_plugins/#audio-plugins","text":"Audio plugins are responsible for handling playback of media, like music and podcasts If mycroft-gui is available these plugins will rarely be used unless ovos is explicitly configured to do so","title":"Audio Plugins"},{"location":"audio_plugins/#list-of-audio-plugins","text":"Plugin Description ovos-ocp-audio-plugin framework + compatibility layer ovos-audio-plugin-simple sox / aplay / paplay / mpg123 ovos-vlc-plugin vlc audio backend ovos-mplayer-plugin mplayer audio backend","title":"List of Audio plugins"},{"location":"audio_service/","text":"Audio Service The audio service is responsible for loading TTS and Audio plugins All audio playback is handled by this service Native playback Usually playback is triggered by some originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client and ovos cli are considered native for legacy reasons the names for ovos cli and speech client are \"debug_cli\" and \"audio\" respectively TTS Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, \"module\": \"ovos-tts-plugin-mimic2\", \"fallback_module\": \"ovos-tts-plugin-mimic\" }, Audio You can enable additional Audio plugins and define the native sources described above under the \"Audio\" section of mycroft.conf ovos-core uses OCP natively for media playback, you can learn more about OCP here OCP will decide when to call the Audio service and what plugin to use, the main use case is for headless setups without a GUI NOTE: mycroft-core has a \"default-backend\" config option, in ovos-core this option has been deprecated and is always OCP. \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"], \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } },","title":"Audio"},{"location":"audio_service/#audio-service","text":"The audio service is responsible for loading TTS and Audio plugins All audio playback is handled by this service","title":"Audio Service"},{"location":"audio_service/#native-playback","text":"Usually playback is triggered by some originating bus message, eg \"recognizer_loop:utterance\" , this message contains metadata that is used to determine if playback should happen. message.context may contain a source and destination, playback is only triggered if a message destination is a native_source or if missing (considered a broadcast). This separation of native sources allows remote clients such as an android app to interact with OVOS without the actual device where ovos-core is running repeating all TTS and music playback out loud You can learn more about message targeting here By default, only utterances originating from the speech client and ovos cli are considered native for legacy reasons the names for ovos cli and speech client are \"debug_cli\" and \"audio\" respectively","title":"Native playback"},{"location":"audio_service/#tts","text":"Two TTS plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower quality offline voice as fallback to account for internet outages, this ensures your device can always give you feedback \"tts\": { \"pulse_duck\": false, \"module\": \"ovos-tts-plugin-mimic2\", \"fallback_module\": \"ovos-tts-plugin-mimic\" },","title":"TTS"},{"location":"audio_service/#audio","text":"You can enable additional Audio plugins and define the native sources described above under the \"Audio\" section of mycroft.conf ovos-core uses OCP natively for media playback, you can learn more about OCP here OCP will decide when to call the Audio service and what plugin to use, the main use case is for headless setups without a GUI NOTE: mycroft-core has a \"default-backend\" config option, in ovos-core this option has been deprecated and is always OCP. \"Audio\": { \"native_sources\": [\"debug_cli\", \"audio\"], \"backends\": { \"OCP\": { \"type\": \"ovos_common_play\", \"active\": true }, \"simple\": { \"type\": \"ovos_audio_simple\", \"active\": true }, \"vlc\": { \"type\": \"ovos_vlc\", \"active\": true } } },","title":"Audio"},{"location":"backend/","text":"Available Backend Services Supported Backends ovos-core supports multiple backends under a single unified interface Personal backend - self hosted Offline - support for setting your own api keys and query services directly Selene - https://api.mycroft.ai , will go down any day now and is in the process of being deprecated Neon - NeonMQ backend native integration is coming soon! Developers do not need to worry about backend details in their applications and skills Identity Information A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe STT Plugin a companion stt plugin is available to use a backend as remote STT provider It is recommended you use ovos-stt-server instead, this plugin is being deprecated together with selene edit your configuration to use ovos-stt-plugin-selene { \"stt\": { \"module\": \"ovos-stt-plugin-selene\" } } source code Offline Backend OVOS by default runs without a backend, in this case you will need to configure api keys manually This can be done with OCA or by editing mycroft.conf edit your configuration to use the offline backend { \"server\": { \"backend_type\": \"offline\" } } Personal Backend Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code Selene The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. With Mycroft's demise this backend may go down any day now, the last effort from mycroft developers was to prepare dinkum for this event. Support is in the process of being fully removed for ovos-core 0.0.8 Selene is AGPL licensed: - backend source code - frontend source code edit your configuration to use the selene backend { \"server\": { \"backend_type\": \"selene\", \"url\": \"https://api.mycroft.ai\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } }","title":"Available Backends"},{"location":"backend/#available-backend-services","text":"","title":"Available Backend Services"},{"location":"backend/#supported-backends","text":"ovos-core supports multiple backends under a single unified interface Personal backend - self hosted Offline - support for setting your own api keys and query services directly Selene - https://api.mycroft.ai , will go down any day now and is in the process of being deprecated Neon - NeonMQ backend native integration is coming soon! Developers do not need to worry about backend details in their applications and skills","title":"Supported Backends"},{"location":"backend/#identity-information","text":"A unique uuid and pairing information generated by registering with Home is stored in: ~/.config/mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS! This file uniquely identifies your device and should be kept safe","title":"Identity Information"},{"location":"backend/#stt-plugin","text":"a companion stt plugin is available to use a backend as remote STT provider It is recommended you use ovos-stt-server instead, this plugin is being deprecated together with selene edit your configuration to use ovos-stt-plugin-selene { \"stt\": { \"module\": \"ovos-stt-plugin-selene\" } } source code","title":"STT Plugin"},{"location":"backend/#offline-backend","text":"OVOS by default runs without a backend, in this case you will need to configure api keys manually This can be done with OCA or by editing mycroft.conf edit your configuration to use the offline backend { \"server\": { \"backend_type\": \"offline\" } }","title":"Offline Backend"},{"location":"backend/#personal-backend","text":"Personal backend is a reverse engineered alternative to selene that predates it It provides the same functionality for devices and packs some extra options It is not intended to serve different users or thousands of devices, there are no user accounts! This is currently the only way to run a vanilla mycroft-core device offline edit your configuration to use your own personal backend instance { \"server\": { \"backend_type\": \"personal\", \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } } source code","title":"Personal Backend"},{"location":"backend/#selene","text":"The official mycroft home backend is called selene, users need to create an account and pair devices with the mycroft servers. With Mycroft's demise this backend may go down any day now, the last effort from mycroft developers was to prepare dinkum for this event. Support is in the process of being fully removed for ovos-core 0.0.8 Selene is AGPL licensed: - backend source code - frontend source code edit your configuration to use the selene backend { \"server\": { \"backend_type\": \"selene\", \"url\": \"https://api.mycroft.ai\", \"version\": \"v1\", \"update\": true, \"metrics\": true, \"sync_skill_settings\": true } }","title":"Selene"},{"location":"backend_client/","text":"OVOS Backend Client Python client library for interaction with several supported backends under a single unified interface Personal backend - self hosted Selene - https://api.mycroft.ai (deprecated) Offline - support for setting your own api keys and query services directly Backend Overview API Offline Personal Selene Admin yes [1] yes no Device yes [2] yes yes Metrics yes [2] yes yes Dataset yes [2] yes yes OAuth yes [2] yes yes Wolfram yes [3] yes yes Geolocate yes yes yes STT yes [3] yes yes Weather yes [3] yes yes Email yes [3] yes yes [1] will update user level mycroft.conf [2] shared json database with personal backend for UI compat [3] needs additional configuration (eg. credentials) [4] uses offline_backend functionality Geolocation from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\") # {'city': 'Lisboa', # 'country': 'Portugal', # 'latitude': 38.7077507, # 'longitude': -9.1365919, # 'timezone': 'Europe/Lisbon'} Remote Settings To interact with skill settings on selene from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") # in mycroft-core please ensure a valid remote_id # in MycroftSkill class you can use # remote_id = self.settings_meta.skill_gid # s = RemoteSkillSettings(\"skill.author\", remote_id=\"@|whatever_msm_decided\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload() Selene Cloud by hijacking skill settings we allows storing arbitrary data in selene and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you dont trust selene! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\") OAuth Retrieving the tokens in a skill does not depend on the selected backend, the mechanism to register a token is backend specific First you need to authorize the application, this can be done with ovos-config-assistant if running offline or ovos-backend-manager if using personal backend If using selene there is no automated process to add a token, you need to contact support@mycroft.ai from ovos_backend_client.api import OAuthApi, BackendType # api = OAuthApi() # determine oauth backend from mycroft.conf api = OAuthApi(backend_type=BackendType.OFFLINE) # explicitly use ovos-backend-manager oauth token_json = api.get_oauth_token(\"spotify\") OpenWeatherMap Proxy from ovos_backend_client.api import OpenWeatherMapApi owm = OpenWeatherMapApi() data = owm.get_weather() # dict - see api docs from owm onecall api Wolfram Alpha proxy from ovos_backend_client.api import WolframAlphaApi wolf = WolframAlphaApi() answer = wolf.spoken(\"what is the speed of light\") # The speed of light has a value of about 300 million meters per second data = wolf.full_results(\"2+2\") # dict - see api docs from wolfram STT a companion stt plugin is available - ovos-stt-plugin-selene DEPRECATED - use ovos-stt-server instead Admin Api (personal backend only!) since local backend does not provide a web ui a admin api can be used to manage your devices from ovos_backend_client.api import AdminApi admin = AdminApi(\"secret_admin_key\") uuid = \"...\" # check identity2.json in the device you want to manage # manually pair a device identity_json = admin.pair(uuid) # set device info info = {\"opt_in\": True, \"name\": \"my_device\", \"device_location\": \"kitchen\", \"email\": \"notifications@me.com\", \"isolated_skills\": False, \"lang\": \"en-us\"} admin.set_device_info(uuid, info) # set device preferences prefs = {\"time_format\": \"full\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"lang\": \"en-us\", \"wake_word\": \"hey_mycroft\", \"ww_config\": {\"phonemes\": \"HH EY . M AY K R AO F T\", \"module\": \"ovos-ww-plugin-pocketsphinx\", \"threshold\": 1e-90}, \"tts_module\": \"ovos-tts-plugin-mimic\", \"tts_config\": {\"voice\": \"ap\"}} admin.set_device_prefs(uuid, prefs) # set location data loc = { \"city\": { \"code\": \"Lawrence\", \"name\": \"Lawrence\", \"state\": { \"code\": \"KS\", \"name\": \"Kansas\", \"country\": { \"code\": \"US\", \"name\": \"United States\" } } }, \"coordinate\": { \"latitude\": 38.971669, \"longitude\": -95.23525 }, \"timezone\": { \"code\": \"America/Chicago\", \"name\": \"Central Standard Time\", \"dstOffset\": 3600000, \"offset\": -21600000 } } admin.set_device_location(uuid, loc)","title":"Backend Client"},{"location":"backend_client/#ovos-backend-client","text":"Python client library for interaction with several supported backends under a single unified interface Personal backend - self hosted Selene - https://api.mycroft.ai (deprecated) Offline - support for setting your own api keys and query services directly","title":"OVOS Backend Client"},{"location":"backend_client/#backend-overview","text":"API Offline Personal Selene Admin yes [1] yes no Device yes [2] yes yes Metrics yes [2] yes yes Dataset yes [2] yes yes OAuth yes [2] yes yes Wolfram yes [3] yes yes Geolocate yes yes yes STT yes [3] yes yes Weather yes [3] yes yes Email yes [3] yes yes [1] will update user level mycroft.conf [2] shared json database with personal backend for UI compat [3] needs additional configuration (eg. credentials) [4] uses offline_backend functionality","title":"Backend Overview"},{"location":"backend_client/#geolocation","text":"from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\") # {'city': 'Lisboa', # 'country': 'Portugal', # 'latitude': 38.7077507, # 'longitude': -9.1365919, # 'timezone': 'Europe/Lisbon'}","title":"Geolocation"},{"location":"backend_client/#remote-settings","text":"To interact with skill settings on selene from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") # in mycroft-core please ensure a valid remote_id # in MycroftSkill class you can use # remote_id = self.settings_meta.skill_gid # s = RemoteSkillSettings(\"skill.author\", remote_id=\"@|whatever_msm_decided\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload()","title":"Remote Settings"},{"location":"backend_client/#selene-cloud","text":"by hijacking skill settings we allows storing arbitrary data in selene and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you dont trust selene! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\")","title":"Selene Cloud"},{"location":"backend_client/#oauth","text":"Retrieving the tokens in a skill does not depend on the selected backend, the mechanism to register a token is backend specific First you need to authorize the application, this can be done with ovos-config-assistant if running offline or ovos-backend-manager if using personal backend If using selene there is no automated process to add a token, you need to contact support@mycroft.ai from ovos_backend_client.api import OAuthApi, BackendType # api = OAuthApi() # determine oauth backend from mycroft.conf api = OAuthApi(backend_type=BackendType.OFFLINE) # explicitly use ovos-backend-manager oauth token_json = api.get_oauth_token(\"spotify\")","title":"OAuth"},{"location":"backend_client/#openweathermap-proxy","text":"from ovos_backend_client.api import OpenWeatherMapApi owm = OpenWeatherMapApi() data = owm.get_weather() # dict - see api docs from owm onecall api","title":"OpenWeatherMap Proxy"},{"location":"backend_client/#wolfram-alpha-proxy","text":"from ovos_backend_client.api import WolframAlphaApi wolf = WolframAlphaApi() answer = wolf.spoken(\"what is the speed of light\") # The speed of light has a value of about 300 million meters per second data = wolf.full_results(\"2+2\") # dict - see api docs from wolfram","title":"Wolfram Alpha proxy"},{"location":"backend_client/#stt","text":"a companion stt plugin is available - ovos-stt-plugin-selene DEPRECATED - use ovos-stt-server instead","title":"STT"},{"location":"backend_client/#admin-api-personal-backend-only","text":"since local backend does not provide a web ui a admin api can be used to manage your devices from ovos_backend_client.api import AdminApi admin = AdminApi(\"secret_admin_key\") uuid = \"...\" # check identity2.json in the device you want to manage # manually pair a device identity_json = admin.pair(uuid) # set device info info = {\"opt_in\": True, \"name\": \"my_device\", \"device_location\": \"kitchen\", \"email\": \"notifications@me.com\", \"isolated_skills\": False, \"lang\": \"en-us\"} admin.set_device_info(uuid, info) # set device preferences prefs = {\"time_format\": \"full\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"lang\": \"en-us\", \"wake_word\": \"hey_mycroft\", \"ww_config\": {\"phonemes\": \"HH EY . M AY K R AO F T\", \"module\": \"ovos-ww-plugin-pocketsphinx\", \"threshold\": 1e-90}, \"tts_module\": \"ovos-tts-plugin-mimic\", \"tts_config\": {\"voice\": \"ap\"}} admin.set_device_prefs(uuid, prefs) # set location data loc = { \"city\": { \"code\": \"Lawrence\", \"name\": \"Lawrence\", \"state\": { \"code\": \"KS\", \"name\": \"Kansas\", \"country\": { \"code\": \"US\", \"name\": \"United States\" } } }, \"coordinate\": { \"latitude\": 38.971669, \"longitude\": -95.23525 }, \"timezone\": { \"code\": \"America/Chicago\", \"name\": \"Central Standard Time\", \"dstOffset\": 3600000, \"offset\": -21600000 } } admin.set_device_location(uuid, loc)","title":"Admin Api (personal backend only!)"},{"location":"backend_manager/","text":"Backend Manager a simple UI for ovos-personal-backend , utility to manage all your devices If you are running ovos-core without a backend OCA provides a similar local interface Install pip install ovos-backend-manager or from source pip install git+https://github.com/OpenVoiceOS/ovos-backend-manager Usage It needs to run on the same machine as the backend, it directly interacts with the databases and configuration files ovos-backend-manager will be available in the command line after installing","title":"Backend Manager"},{"location":"backend_manager/#backend-manager","text":"a simple UI for ovos-personal-backend , utility to manage all your devices If you are running ovos-core without a backend OCA provides a similar local interface","title":"Backend Manager"},{"location":"backend_manager/#install","text":"pip install ovos-backend-manager or from source pip install git+https://github.com/OpenVoiceOS/ovos-backend-manager","title":"Install"},{"location":"backend_manager/#usage","text":"It needs to run on the same machine as the backend, it directly interacts with the databases and configuration files ovos-backend-manager will be available in the command line after installing","title":"Usage"},{"location":"bigscreen/","text":"Introduction: OpenVoiceOS GUI supports various Skills and PHAL plugins that share a voice application interface with Plasma Bigscreen. In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export an environment variable. Exporting the Environment Variable: In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export the environment variable QT_FILE_SELECTORS=mediacenter . This can be done by executing the following command in the terminal: export QT_FILE_SELECTORS=mediacenter This environment variable by default is enabled and added to the Plasma Bigscreen environment. To create your own media center environment store the variable in /etc/environment or /etc/profile.d/99-ovos-media-center.sh Exporting the environment variable QT_FILE_SELECTORS=mediacenter is a necessary step to enable key navigation on Plasma Bigscreen and Media Centers for the Open Voice OS project GUI. With this in place, the user can enjoy seamless key navigation while using the Skills and PHAL plugins on their Plasma Bigscreen and Media Centers.","title":"Bigscreen"},{"location":"bigscreen/#introduction","text":"OpenVoiceOS GUI supports various Skills and PHAL plugins that share a voice application interface with Plasma Bigscreen. In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export an environment variable.","title":"Introduction:"},{"location":"bigscreen/#exporting-the-environment-variable","text":"In order to enable key navigation on Plasma Bigscreen and Media Centers, the user needs to export the environment variable QT_FILE_SELECTORS=mediacenter . This can be done by executing the following command in the terminal: export QT_FILE_SELECTORS=mediacenter This environment variable by default is enabled and added to the Plasma Bigscreen environment. To create your own media center environment store the variable in /etc/environment or /etc/profile.d/99-ovos-media-center.sh Exporting the environment variable QT_FILE_SELECTORS=mediacenter is a necessary step to enable key navigation on Plasma Bigscreen and Media Centers for the Open Voice OS project GUI. With this in place, the user can enjoy seamless key navigation while using the Skills and PHAL plugins on their Plasma Bigscreen and Media Centers.","title":"Exporting the Environment Variable:"},{"location":"buildroot/","text":"Open Voice Operating System - Buildroot Edition Auto detection and configuration of HAT's The buildroot OpenVoiceOS editions is considered to be consumer friendly type of device, or as Mycroft A.I. would like to call, a retail version. However as we so not target a specific hardware platform and would like to support custom made systems we are implementing a smart way to detect and configure different type of Raspberry Pi HAT's. At boot the system scan the I2C bus for known and supported HAT's and if found configures the underlying linux sound system. At the moment this is still very much in development, however the below HAT's are or should soon be supported by this system; - ReSpeaker 2-mic HAT - ReSpeaker 4-mic Square HAT - ReSpeaker 4-mic linear / 6-mic HAT - USB devices such as the PS EYE-2 - SJ-201 Dev Kits - SJ-201 Mark2 retail device Snapcast Client & Server TODO - write docs Remote shared folder access (SMB - Windows) Your OpenVoiceOS device is accessible over the network from your Windows computer. This is still a work in process, but you can open a file explorer and navigate to you OpenVoiceOS device. At the moment the following directories within the user's home directory are shared over the network. - Documents - Music - Pictures These folders are also used by KDE Connect file transfer plugins and for instance the Camera skill (Hey Mycroft, take a selfie) and / or Homescreen Skill (Hey Mycroft, take a screenshot) Remote shared folder access (NFS - Linux) In the near future the above Windows network shares are also made available over NFS for Linux clients. This is still a Work In Progress / To Do item. Development. At this moment development is in very early stages and focussed on the Raspberry Pi 3B & 4. As soon as an initial first workable version is created, other hardware might be added. Source code: https://github.com/OpenVoiceOS/ovos-buildroot Build Environment Only use x86_64 based architecture/ hardware to build the image. The following example Build environment has been tested : Architecture: x86_64 Hardware: Intel Core i5 processor, 8GB RAM, 240GB SSD (you can build on less RAM (2GB) and slower storage but more RAM, faster storage = quicker image building) OS: Ubuntu 22.04 LTS desktop Installing System Build Dependencies The following system packages are required to build the image: gcc subversion qttools5-dev qttools5-dev-tools python git make g++ curl wget qtdeclarative5-dev The following firewall ports need to be allowed to the internet. In addition to the usual http/https ports (tcp 80, tcp 443) a couple of other ports need to be allowed to the internet : - tcp 9418 (git). - tcp 21 (ftp PASV) and random ports for DATA channel. This can be optional but better to have this allowed along with the corresponding random data channel ports. (knowledge of firewalls required) Getting the code. First, get the code on your system! The simplest method is via git. - cd ~/ - git clone --recurse-submodules https://github.com/OpenVoiceOS/OpenVoiceOS.git - cd OpenVoiceOS Patching Buildroot. (ONLY at the first clean checkout/clone) If this is the very first time you are going to build an image, you need to execute the following command once; - ./scripts/br-patches.sh This will patch the Buildroot packages. Building the image. Building the image(s) can be done by utilizing a proper Makefile; To see the available commands, just run: 'make help' As example to build the rpi4 version; - make clean - make rpi4_64-gui-config - make rpi4_64-gui Now grab a cup of coffee, go for a walk, sleep and repeat as the build process takes up a long time pulling everything from source and cross compiling everything for the device. Especially the qtwebengine package is taking a LONG time. (At the moment there is an outstanding issue which prevents the build to run completely to the end. The plasma-workspace package will error out, not finding the libGLESv4 properly linked within QT5GUI. When the build stopped because of this error, edit the following file; buildroot/output/host/aarch64-buildroot-linux-gnu/sysroot/usr/lib/cmake/Qt5Gui/Qt5GuiConfigExtras.cmake at the bottom of the file replace this line; _qt5gui_find_extra_libs(OPENGL \"GLESv2\" \"\" \"\") And replace it bit this line; _qt5gui_find_extra_libs(OPENGL \"${CMAKE_SYSROOT}/usr/lib/libGLESv2.so\" \"\" \"${CMAKE_SYSROOT}/usr/include/libdrm\") Then you can continue the build process by re-running the \"make rpi4_64-gui\" command. (DO NOT, run \"make clean\" and/or \"make rpi4_64-gui-config\" again, or you will start from scratch again !!!) When everything goes fine the xz compressed image will be available within the release directory. Booting image from sd card for the first time (setting up Wi-Fi and backend). 1.Ensure all required peripherals (mic, speakers, HDMI, usb mouse etc) are plugged in before powering on your RPI4 for the first time. 2. Skip this step if RPI4 is using an ethernet cable. Once powered on, the screen will present the Wifi setup screen ( a Wifi HotSpot is created). Connect to the Wifi HotSpot (ssid OVOS) from another device and follow the on-screen instructions to set up Wifi. 3.Once Wifi is setup a choice of Mycroft backend and Local backend is presented. Choose the Mycroft backend for now and follow the on-screen instructions, Local backend is not ready to use yet. After the pairing process has completed and skills downloaded it's time to test/ use it. Accessing the CLI. SSH to ip address of RPI4 default credentials 'mycroft/mycroft'","title":"Buildroot"},{"location":"buildroot/#open-voice-operating-system-buildroot-edition","text":"","title":" Open Voice Operating System - Buildroot Edition"},{"location":"buildroot/#auto-detection-and-configuration-of-hats","text":"The buildroot OpenVoiceOS editions is considered to be consumer friendly type of device, or as Mycroft A.I. would like to call, a retail version. However as we so not target a specific hardware platform and would like to support custom made systems we are implementing a smart way to detect and configure different type of Raspberry Pi HAT's. At boot the system scan the I2C bus for known and supported HAT's and if found configures the underlying linux sound system. At the moment this is still very much in development, however the below HAT's are or should soon be supported by this system; - ReSpeaker 2-mic HAT - ReSpeaker 4-mic Square HAT - ReSpeaker 4-mic linear / 6-mic HAT - USB devices such as the PS EYE-2 - SJ-201 Dev Kits - SJ-201 Mark2 retail device","title":"Auto detection and configuration of HAT's"},{"location":"buildroot/#snapcast-client-server","text":"TODO - write docs","title":"Snapcast Client &amp; Server"},{"location":"buildroot/#remote-shared-folder-access-smb-windows","text":"Your OpenVoiceOS device is accessible over the network from your Windows computer. This is still a work in process, but you can open a file explorer and navigate to you OpenVoiceOS device. At the moment the following directories within the user's home directory are shared over the network. - Documents - Music - Pictures These folders are also used by KDE Connect file transfer plugins and for instance the Camera skill (Hey Mycroft, take a selfie) and / or Homescreen Skill (Hey Mycroft, take a screenshot)","title":"Remote shared folder access (SMB - Windows)"},{"location":"buildroot/#remote-shared-folder-access-nfs-linux","text":"In the near future the above Windows network shares are also made available over NFS for Linux clients. This is still a Work In Progress / To Do item.","title":"Remote shared folder access (NFS - Linux)"},{"location":"buildroot/#development","text":"At this moment development is in very early stages and focussed on the Raspberry Pi 3B & 4. As soon as an initial first workable version is created, other hardware might be added. Source code: https://github.com/OpenVoiceOS/ovos-buildroot","title":"Development."},{"location":"buildroot/#build-environment","text":"Only use x86_64 based architecture/ hardware to build the image. The following example Build environment has been tested : Architecture: x86_64 Hardware: Intel Core i5 processor, 8GB RAM, 240GB SSD (you can build on less RAM (2GB) and slower storage but more RAM, faster storage = quicker image building) OS: Ubuntu 22.04 LTS desktop","title":"Build Environment"},{"location":"buildroot/#installing-system-build-dependencies","text":"The following system packages are required to build the image: gcc subversion qttools5-dev qttools5-dev-tools python git make g++ curl wget qtdeclarative5-dev","title":"Installing System Build Dependencies"},{"location":"buildroot/#the-following-firewall-ports-need-to-be-allowed-to-the-internet","text":"In addition to the usual http/https ports (tcp 80, tcp 443) a couple of other ports need to be allowed to the internet : - tcp 9418 (git). - tcp 21 (ftp PASV) and random ports for DATA channel. This can be optional but better to have this allowed along with the corresponding random data channel ports. (knowledge of firewalls required)","title":"The following firewall ports need to be allowed to the internet."},{"location":"buildroot/#getting-the-code","text":"First, get the code on your system! The simplest method is via git. - cd ~/ - git clone --recurse-submodules https://github.com/OpenVoiceOS/OpenVoiceOS.git - cd OpenVoiceOS","title":"Getting the code."},{"location":"buildroot/#patching-buildroot","text":"(ONLY at the first clean checkout/clone) If this is the very first time you are going to build an image, you need to execute the following command once; - ./scripts/br-patches.sh This will patch the Buildroot packages.","title":"Patching Buildroot."},{"location":"buildroot/#building-the-image","text":"Building the image(s) can be done by utilizing a proper Makefile; To see the available commands, just run: 'make help' As example to build the rpi4 version; - make clean - make rpi4_64-gui-config - make rpi4_64-gui Now grab a cup of coffee, go for a walk, sleep and repeat as the build process takes up a long time pulling everything from source and cross compiling everything for the device. Especially the qtwebengine package is taking a LONG time. (At the moment there is an outstanding issue which prevents the build to run completely to the end. The plasma-workspace package will error out, not finding the libGLESv4 properly linked within QT5GUI. When the build stopped because of this error, edit the following file; buildroot/output/host/aarch64-buildroot-linux-gnu/sysroot/usr/lib/cmake/Qt5Gui/Qt5GuiConfigExtras.cmake at the bottom of the file replace this line; _qt5gui_find_extra_libs(OPENGL \"GLESv2\" \"\" \"\") And replace it bit this line; _qt5gui_find_extra_libs(OPENGL \"${CMAKE_SYSROOT}/usr/lib/libGLESv2.so\" \"\" \"${CMAKE_SYSROOT}/usr/include/libdrm\") Then you can continue the build process by re-running the \"make rpi4_64-gui\" command. (DO NOT, run \"make clean\" and/or \"make rpi4_64-gui-config\" again, or you will start from scratch again !!!) When everything goes fine the xz compressed image will be available within the release directory.","title":"Building the image."},{"location":"buildroot/#booting-image-from-sd-card-for-the-first-time-setting-up-wi-fi-and-backend","text":"1.Ensure all required peripherals (mic, speakers, HDMI, usb mouse etc) are plugged in before powering on your RPI4 for the first time. 2. Skip this step if RPI4 is using an ethernet cable. Once powered on, the screen will present the Wifi setup screen ( a Wifi HotSpot is created). Connect to the Wifi HotSpot (ssid OVOS) from another device and follow the on-screen instructions to set up Wifi. 3.Once Wifi is setup a choice of Mycroft backend and Local backend is presented. Choose the Mycroft backend for now and follow the on-screen instructions, Local backend is not ready to use yet. After the pairing process has completed and skills downloaded it's time to test/ use it.","title":"Booting image from sd card for the first time (setting up Wi-Fi and backend)."},{"location":"buildroot/#accessing-the-cli","text":"SSH to ip address of RPI4 default credentials 'mycroft/mycroft'","title":"Accessing the CLI."},{"location":"bus_service/","text":"Bus Service The bus service provides a websocket where all internal events travel You can think of the bus service as OVOS's nervous system The mycroft-bus is considered an internal and private websocket, external clients should not connect directly to it. Exposing the bus Please do not expose the messagebus to the outside world! Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE If you need to connect to the bus from the outside world please check the companion project HiveMind Lots of guides for mycroft tell you to expose the websocket, please (re)read the info and links above, think 10 times before following steps blindly Message A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data . Targeting Theory ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for. the Message object provides the following methods: message.forward method, keeps previous context. message continues going to same destination message.reply method swaps \"source\" with \"destination\" message goes back to source The context destination parameter in the original message can be set to a list with any number of intended targets: bus.emit(Message('recognizer_loop:utterance', data, context={'destination': ['audio', 'kde'], 'source': \"remote_service\")) Sources ovos-core injects the context when it emits an utterance, this can be either typed in the ovos-cli-client or spoken via STT service STT will identify itself as audio ovos-cli-client will identify itself as debug_cli mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests Destinations Output capable services are the cli and the TTS The command line is a debug tool, it will ignore the destination TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" or \"debug_cli\" are the destination A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app) Internal routing intent service will .reply to the original utterance message all skill/intent service messages are .forward (from previous intent service .reply ) skills sending their own messages might not respect this warning converse / get_response support is limited, the context may be lost warning in the context of the multiple users skills might keep a shared state between clients, eg. a client may enable parrot mode for everyone warning scheduled events support is limited, the context is lost warning Configuration The messagebus has a dedicated section in mycroft.conf \"websocket\": { \"host\": \"0.0.0.0\", \"port\": 8181, \"route\": \"/core\", \"shared_connection\": true } Security in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem Additionally, it is recommended you change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed","title":"Bus"},{"location":"bus_service/#bus-service","text":"The bus service provides a websocket where all internal events travel You can think of the bus service as OVOS's nervous system The mycroft-bus is considered an internal and private websocket, external clients should not connect directly to it.","title":"Bus Service"},{"location":"bus_service/#exposing-the-bus","text":"Please do not expose the messagebus to the outside world! Anything connected to the bus can fully control OVOS, and OVOS usually has full control over the whole system! You can read more about the security issues over at Nhoya/MycroftAI-RCE If you need to connect to the bus from the outside world please check the companion project HiveMind Lots of guides for mycroft tell you to expose the websocket, please (re)read the info and links above, think 10 times before following steps blindly","title":"Exposing the bus"},{"location":"bus_service/#message","text":"A Message consists of a json payload, it contains a type , some data and a context . The context is considered to be metadata and might be changed at any time in transit, the context can contain anything depending on where the message came from, and often is completely empty. You can think of the message context as a sort of session data for a individual interaction, in general messages down the chain keep the context from the original message, most listeners (eg, skills) will only care about type and data .","title":"Message"},{"location":"bus_service/#targeting-theory","text":"ovos-core uses the message context to add metadata about the messages themselves, where do they come from and what are they intended for. the Message object provides the following methods: message.forward method, keeps previous context. message continues going to same destination message.reply method swaps \"source\" with \"destination\" message goes back to source The context destination parameter in the original message can be set to a list with any number of intended targets: bus.emit(Message('recognizer_loop:utterance', data, context={'destination': ['audio', 'kde'], 'source': \"remote_service\"))","title":"Targeting Theory"},{"location":"bus_service/#sources","text":"ovos-core injects the context when it emits an utterance, this can be either typed in the ovos-cli-client or spoken via STT service STT will identify itself as audio ovos-cli-client will identify itself as debug_cli mycroft.conf contains a native_sources section you can configure to change how the audio service processes external requests","title":"Sources"},{"location":"bus_service/#destinations","text":"Output capable services are the cli and the TTS The command line is a debug tool, it will ignore the destination TTS checks the message context if it's the intended target for the message and will only speak in the following conditions: Explicitly targeted i.e. the destination is \"audio\" destination is set to None destination is missing completely The idea is that for example when the android app is used to access OpenVoiceOS the device at home shouldn't start to speak. TTS will be executed when \"audio\" or \"debug_cli\" are the destination A missing destination or if the destination is set to None is interpreted as a multicast and should trigger all output capable processes (be it the audio service, a web-interface, the KDE plasmoid or maybe the android app)","title":"Destinations"},{"location":"bus_service/#internal-routing","text":"intent service will .reply to the original utterance message all skill/intent service messages are .forward (from previous intent service .reply ) skills sending their own messages might not respect this warning converse / get_response support is limited, the context may be lost warning in the context of the multiple users skills might keep a shared state between clients, eg. a client may enable parrot mode for everyone warning scheduled events support is limited, the context is lost warning","title":"Internal routing"},{"location":"bus_service/#configuration","text":"The messagebus has a dedicated section in mycroft.conf \"websocket\": { \"host\": \"0.0.0.0\", \"port\": 8181, \"route\": \"/core\", \"shared_connection\": true }","title":"Configuration"},{"location":"bus_service/#security","text":"in mycroft-core all skills share a bus connection, this allows malicious skills to manipulate it and affect other skills you can see a demonstration of this problem with BusBrickerSkill \"shared_connection\": false ensures each skill gets its own websocket connection and avoids this problem Additionally, it is recommended you change \"host\": \"127.0.0.1\" , this will ensure no outside world connections are allowed","title":"Security"},{"location":"community/","text":"Projects incubated under OpenVoiceOS The following projects are fully maintained by our community under the OpenVoiceOS github organization Thank you guys! Projects ovos-docker raspbian-ovos uptime monitor projen skill generator/retrofitter Skills ovos-skill-easter-eggs Plugins ovos-ww-plugin-openWakeWord ovos-microphone-plugin-sounddevice ovos-PHAL-plugin-homeassistant ovos-translate-plugin-deepl","title":"Community"},{"location":"community/#projects-incubated-under-openvoiceos","text":"The following projects are fully maintained by our community under the OpenVoiceOS github organization Thank you guys!","title":"Projects incubated under OpenVoiceOS"},{"location":"community/#projects","text":"ovos-docker raspbian-ovos uptime monitor projen skill generator/retrofitter","title":"Projects"},{"location":"community/#skills","text":"ovos-skill-easter-eggs","title":"Skills"},{"location":"community/#plugins","text":"ovos-ww-plugin-openWakeWord ovos-microphone-plugin-sounddevice ovos-PHAL-plugin-homeassistant ovos-translate-plugin-deepl","title":"Plugins"},{"location":"comparison/","text":"Images compared OpenVoiceOS ready to use images come in several flavours; The buildroot version, being the minimal consumer type of image and a headless raspbian image maintained by the community Some friend projects also publish images that include OpenVoiceOS in some form, those are also listed here Operating System OpenVoiceOS Friends Plugins OpenVoiceOS Friends Functionality OpenVoiceOS Friends Configuration OpenVoiceOS Friends Hardware OpenVoiceOS Friends Operating System OpenVoiceOS ovos-buildroot ovos-raspbian Operating System Base OS buildroot raspbian Last Updated - YYYY/MM/DD 2023-0?-?? 2023-0?-?? Customization Open Build System Yes Yes (pi-gen github workflow) Package manager N/A ( No buildtools available ) apt Software - Architecture Core ovos-core ovos-core GUI ovos-shell N/A Launcher systemd user session systemd user session Updating Update mechanism pip (bash scripts) pip (bash scripts) Screen - GUI GUI supported Show GUI if screen attached eglfs N/A Friends NeonAI Bigscreen Operating System Base OS debian manjaro Last Updated - YYYY/MM/DD 2023-03 (regular updates) Rolling Release Customization Open Build System Yes Manjaro ARM Tools Package manager apt pacman Software - Architecture Core neon-core ovos-core GUI ovos-shell Plasma Shell Launcher systemd system session systemd user session Updating Update mechanism PHAL plugin (pip/git) package manager (mycroft-bigscreen) Screen - GUI GUI supported Show GUI if screen attached eglfs X11/Wayland Plugins OpenVoiceOS ovos-buildroot ovos-raspbian Default Plugins VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad stt ovos-stt-plugin-server (ovos proxy) ovos-stt-plugin-server (ovos proxy) fallback_stt N/A N/A tts ovos-tts-plugin-mimic3-server (public servers) ovos-tts-plugin-mimic3-server (public servers) fallback_tts ovos-tts-plugin-mimic ovos-tts-plugin-mimic Recommended Plugins STT - On device ovos-stt-plugin-vosk ovos-stt-plugin-vosk STT - On premises ovos-stt-plugin-server (any plugin) ovos-stt-plugin-server (any plugin) STT - Cloud ovos-stt-plugin-server (ovos proxy) ovos-stt-plugin-server (ovos proxy) TTS - On device ovos-tts-plugin-mimic3 ovos-tts-plugin-mimic3 TTS - On premises ovos-tts-plugin-server (any plugin) ovos-tts-plugin-server (any plugin) TTS - Cloud ovos-tts-plugin-mimic3-server (public servers) ovos-tts-plugin-mimic3-server (public servers) VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-silero Friends NeonAI Bigscreen Default Plugins VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad stt neon-stt-plugin-google-cloud-streaming ovos-stt-plugin-server (ovos proxy) fallback_stt ovos-stt-plugin-vosk N/A tts neon-tts-plugin-coqui-remote ovos-tts-plugin-mimic2 fallback_tts neon-tts-plugin-coqui ovos-tts-plugin-mimic Recommended Plugins STT - On device ovos-stt-plugin-vosk ovos-stt-plugin-vosk STT - On premises neon-stt-plugin-nemo ovos-stt-plugin-server (any plugin) STT - Cloud neon-stt-plugin-google-cloud-streaming ovos-stt-plugin-server (ovos proxy) TTS - On device neon-tts-plugin-coqui ovos-tts-plugin-mimic3 TTS - On premises ovos-stt-plugin-server + neon-tts-plugin-coqui ovos-tts-plugin-server (any plugin) TTS - Cloud neon-tts-plugin-coqui-remote ovos-tts-plugin-mimic3-server (public servers) VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad Functionality MNOs - Mycroft/Neon/OVOS OpenVoiceOS ovos-buildroot ovos-raspbian Skill Frameworks Mycroft Audio Service (bus api) Yes Yes Adapt Intents Yes Yes Padatious Intents Yes Yes OCP Skills Yes Partial (audio only) MNOS Common Query Skills Yes Yes MNOs Fallback Skills Yes Yes MNOs Skills Yes Yes Mycroft Common Play Skills Partial (imperfect compat layer) Partial (imperfect compat layer) Media Frameworks OCP Yes Yes MPRIS Yes Yes (?) KDEConnect Yes No (?) Spotify Daemon Yes No Airplay Yes No Bluetooth Speaker WIP No IOT Frameworks HomeAssistant integration WIP HomeAssistant PHAL Plugin WIP HomeAssistant PHAL Plugin (manual install) Friends NeonAI Bigscreen Skill Frameworks Mycroft Audio Service (bus api) Yes Yes Adapt Intents Yes Yes Padatious Intents Yes Yes OCP Skills Yes Yes MNOS Common Query Skills Yes Yes MNOs Fallback Skills Yes Yes MNOs Skills Yes Yes Mycroft Common Play Skills Partial (imperfect compat layer) Partial (imperfect compat layer) Media Frameworks OCP Yes Yes MPRIS Yes (?) Yes KDEConnect No (?) Yes (?) Spotify Daemon No No (?) Airplay No No (?) Bluetooth Speaker No No (?) IOT Frameworks HomeAssistant integration WIP HomeAssistant PHAL Plugin WIP HomeAssistant PHAL Plugin Configuration OpenVoiceOS ovos-buildroot ovos-raspbian Configuration - Option Data privacy Yes Yes Offline mode Yes (setup skill) yes (need to change default plugins) Color theming Yes No Non-Pairing mode Yes (setup skill) Yes (preconfigured default) API Access w/o pairing Yes Yes On-Screen configuration Yes No Online configuration personal-backend personal-backend Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect from phone Yes No On device WiFi Setup Configure the connection directly on screen Yes No On screen keyboard Yes No Reconfigure network Easy way to change the network settings Yes (on screen) Yes (raspi-config) Friends NeonAI Bigscreen Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect from phone No No On device WiFi Setup Configure the connection directly on device Yes Yes On screen keyboard Yes Yes Reconfigure network Easy way to change the network settings Yes Yes Configuration - Option Data privacy Yes Yes Offline mode Yes Yes Color theming Yes No Non-Pairing mode Yes Yes API Access w/o pairing Yes Yes On-Device configuration Yes ? Online configuration WIP ? Hardware OpenVoiceOS ovos-buildroot ovos-raspbian Hardware - Compatibility Raspberry Pi 3B WIP Yes Raspberry Pi 3B+ WIP Yes Raspberry Pi 3A+ WIP Yes Raspberry Pi 4 Yes Yes X86_64 planned No Mark-1 WIP No possibly in future Mark-2 WIP (no leds) partial (no gui + manual install) Mark-2 (dev kit) Yes partial (no gui + manual install) Hardware - Peripherals ReSpeaker 2-mic Yes (?) manual install (?) ReSpeaker 4-mic squared Yes (?) manual install (?) ReSpeaker 4-mic linear Yes (?) manual install (?) ReSpeaker 6-mic Yes (?) manual install (?) USB Yes Yes SJ-201 Yes manual install (?) Google AIY v1 manual install (?) manual install (?) Google AIY v2 No perhaps in the future manual install (?) Friends NeonAI Bigscreen Hardware - Compatibility Raspberry Pi 3B No No Raspberry Pi 3B+ No No Raspberry Pi 3A+ No No Raspberry Pi 4 Yes Yes X86_64 No No Mark-1 No No Mark-2 Yes No Mark-2 (dev kit) Yes No Hardware - Peripherals ReSpeaker 2-mic manual install (?) manual install (?) ReSpeaker 4-mic squared manual install (?) manual install (?) ReSpeaker 4-mic linear manual install (?) manual install (?) ReSpeaker 6-mic manual install (?) manual install (?) USB Yes Yes SJ-201 Yes No Google AIY v1 manual install (?) manual install (?) Google AIY v2 manual install (?) manual install (?)","title":"Compared"},{"location":"comparison/#images-compared","text":"OpenVoiceOS ready to use images come in several flavours; The buildroot version, being the minimal consumer type of image and a headless raspbian image maintained by the community Some friend projects also publish images that include OpenVoiceOS in some form, those are also listed here Operating System OpenVoiceOS Friends Plugins OpenVoiceOS Friends Functionality OpenVoiceOS Friends Configuration OpenVoiceOS Friends Hardware OpenVoiceOS Friends","title":"Images compared"},{"location":"comparison/#operating-system","text":"","title":"Operating System"},{"location":"comparison/#openvoiceos","text":"ovos-buildroot ovos-raspbian Operating System Base OS buildroot raspbian Last Updated - YYYY/MM/DD 2023-0?-?? 2023-0?-?? Customization Open Build System Yes Yes (pi-gen github workflow) Package manager N/A ( No buildtools available ) apt Software - Architecture Core ovos-core ovos-core GUI ovos-shell N/A Launcher systemd user session systemd user session Updating Update mechanism pip (bash scripts) pip (bash scripts) Screen - GUI GUI supported Show GUI if screen attached eglfs N/A","title":"OpenVoiceOS"},{"location":"comparison/#friends","text":"NeonAI Bigscreen Operating System Base OS debian manjaro Last Updated - YYYY/MM/DD 2023-03 (regular updates) Rolling Release Customization Open Build System Yes Manjaro ARM Tools Package manager apt pacman Software - Architecture Core neon-core ovos-core GUI ovos-shell Plasma Shell Launcher systemd system session systemd user session Updating Update mechanism PHAL plugin (pip/git) package manager (mycroft-bigscreen) Screen - GUI GUI supported Show GUI if screen attached eglfs X11/Wayland","title":"Friends"},{"location":"comparison/#plugins","text":"","title":"Plugins"},{"location":"comparison/#openvoiceos_1","text":"ovos-buildroot ovos-raspbian Default Plugins VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad stt ovos-stt-plugin-server (ovos proxy) ovos-stt-plugin-server (ovos proxy) fallback_stt N/A N/A tts ovos-tts-plugin-mimic3-server (public servers) ovos-tts-plugin-mimic3-server (public servers) fallback_tts ovos-tts-plugin-mimic ovos-tts-plugin-mimic Recommended Plugins STT - On device ovos-stt-plugin-vosk ovos-stt-plugin-vosk STT - On premises ovos-stt-plugin-server (any plugin) ovos-stt-plugin-server (any plugin) STT - Cloud ovos-stt-plugin-server (ovos proxy) ovos-stt-plugin-server (ovos proxy) TTS - On device ovos-tts-plugin-mimic3 ovos-tts-plugin-mimic3 TTS - On premises ovos-tts-plugin-server (any plugin) ovos-tts-plugin-server (any plugin) TTS - Cloud ovos-tts-plugin-mimic3-server (public servers) ovos-tts-plugin-mimic3-server (public servers) VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-silero","title":"OpenVoiceOS"},{"location":"comparison/#friends_1","text":"NeonAI Bigscreen Default Plugins VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad stt neon-stt-plugin-google-cloud-streaming ovos-stt-plugin-server (ovos proxy) fallback_stt ovos-stt-plugin-vosk N/A tts neon-tts-plugin-coqui-remote ovos-tts-plugin-mimic2 fallback_tts neon-tts-plugin-coqui ovos-tts-plugin-mimic Recommended Plugins STT - On device ovos-stt-plugin-vosk ovos-stt-plugin-vosk STT - On premises neon-stt-plugin-nemo ovos-stt-plugin-server (any plugin) STT - Cloud neon-stt-plugin-google-cloud-streaming ovos-stt-plugin-server (ovos proxy) TTS - On device neon-tts-plugin-coqui ovos-tts-plugin-mimic3 TTS - On premises ovos-stt-plugin-server + neon-tts-plugin-coqui ovos-tts-plugin-server (any plugin) TTS - Cloud neon-tts-plugin-coqui-remote ovos-tts-plugin-mimic3-server (public servers) VAD ovos-vad-plugin-webrtcvad ovos-vad-plugin-webrtcvad","title":"Friends"},{"location":"comparison/#functionality","text":"MNOs - Mycroft/Neon/OVOS","title":"Functionality"},{"location":"comparison/#openvoiceos_2","text":"ovos-buildroot ovos-raspbian Skill Frameworks Mycroft Audio Service (bus api) Yes Yes Adapt Intents Yes Yes Padatious Intents Yes Yes OCP Skills Yes Partial (audio only) MNOS Common Query Skills Yes Yes MNOs Fallback Skills Yes Yes MNOs Skills Yes Yes Mycroft Common Play Skills Partial (imperfect compat layer) Partial (imperfect compat layer) Media Frameworks OCP Yes Yes MPRIS Yes Yes (?) KDEConnect Yes No (?) Spotify Daemon Yes No Airplay Yes No Bluetooth Speaker WIP No IOT Frameworks HomeAssistant integration WIP HomeAssistant PHAL Plugin WIP HomeAssistant PHAL Plugin (manual install)","title":"OpenVoiceOS"},{"location":"comparison/#friends_2","text":"NeonAI Bigscreen Skill Frameworks Mycroft Audio Service (bus api) Yes Yes Adapt Intents Yes Yes Padatious Intents Yes Yes OCP Skills Yes Yes MNOS Common Query Skills Yes Yes MNOs Fallback Skills Yes Yes MNOs Skills Yes Yes Mycroft Common Play Skills Partial (imperfect compat layer) Partial (imperfect compat layer) Media Frameworks OCP Yes Yes MPRIS Yes (?) Yes KDEConnect No (?) Yes (?) Spotify Daemon No No (?) Airplay No No (?) Bluetooth Speaker No No (?) IOT Frameworks HomeAssistant integration WIP HomeAssistant PHAL Plugin WIP HomeAssistant PHAL Plugin","title":"Friends"},{"location":"comparison/#configuration","text":"","title":"Configuration"},{"location":"comparison/#openvoiceos_3","text":"ovos-buildroot ovos-raspbian Configuration - Option Data privacy Yes Yes Offline mode Yes (setup skill) yes (need to change default plugins) Color theming Yes No Non-Pairing mode Yes (setup skill) Yes (preconfigured default) API Access w/o pairing Yes Yes On-Screen configuration Yes No Online configuration personal-backend personal-backend Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect from phone Yes No On device WiFi Setup Configure the connection directly on screen Yes No On screen keyboard Yes No Reconfigure network Easy way to change the network settings Yes (on screen) Yes (raspi-config)","title":"OpenVoiceOS"},{"location":"comparison/#friends_3","text":"NeonAI Bigscreen Network Setup - Options Mobile WiFi Setup Easy device \"hotspot\" to connect from phone No No On device WiFi Setup Configure the connection directly on device Yes Yes On screen keyboard Yes Yes Reconfigure network Easy way to change the network settings Yes Yes Configuration - Option Data privacy Yes Yes Offline mode Yes Yes Color theming Yes No Non-Pairing mode Yes Yes API Access w/o pairing Yes Yes On-Device configuration Yes ? Online configuration WIP ?","title":"Friends"},{"location":"comparison/#hardware","text":"","title":"Hardware"},{"location":"comparison/#openvoiceos_4","text":"ovos-buildroot ovos-raspbian Hardware - Compatibility Raspberry Pi 3B WIP Yes Raspberry Pi 3B+ WIP Yes Raspberry Pi 3A+ WIP Yes Raspberry Pi 4 Yes Yes X86_64 planned No Mark-1 WIP No possibly in future Mark-2 WIP (no leds) partial (no gui + manual install) Mark-2 (dev kit) Yes partial (no gui + manual install) Hardware - Peripherals ReSpeaker 2-mic Yes (?) manual install (?) ReSpeaker 4-mic squared Yes (?) manual install (?) ReSpeaker 4-mic linear Yes (?) manual install (?) ReSpeaker 6-mic Yes (?) manual install (?) USB Yes Yes SJ-201 Yes manual install (?) Google AIY v1 manual install (?) manual install (?) Google AIY v2 No perhaps in the future manual install (?)","title":"OpenVoiceOS"},{"location":"comparison/#friends_4","text":"NeonAI Bigscreen Hardware - Compatibility Raspberry Pi 3B No No Raspberry Pi 3B+ No No Raspberry Pi 3A+ No No Raspberry Pi 4 Yes Yes X86_64 No No Mark-1 No No Mark-2 Yes No Mark-2 (dev kit) Yes No Hardware - Peripherals ReSpeaker 2-mic manual install (?) manual install (?) ReSpeaker 4-mic squared manual install (?) manual install (?) ReSpeaker 4-mic linear manual install (?) manual install (?) ReSpeaker 6-mic manual install (?) manual install (?) USB Yes Yes SJ-201 Yes No Google AIY v1 manual install (?) manual install (?) Google AIY v2 manual install (?) manual install (?)","title":"Friends"},{"location":"compat/","text":"Compatibility FAQ Do OVOS images run on the mark2? Do OVOS skills work in mycroft-core? Do OPM plugins work in mycroft-core? Does PHAL work with mycroft-core? Known Incompatibilities Do OVOS images run on the mark2? The mark2 developer kit is one of the reference platforms we test and develop against, the ovos-buildroot images can be used in a mark 2 device Do OVOS skills work in mycroft-core? If you are a developer please subclass your skills from OVOSSkill provided in ovos-workshop package We implement all skill development tools under the ovos-workshop library, this allows most OVOS functionality to be used in mycroft-core without problems. This includes intent_layers , decorators and killable_events However some skills may decide to depend on features exclusive to ovos-core, or be missing bug fixes in mycroft-core, therefore we can not ensure 100% compatibility. eg, if a skill depends on the converse deactivated event it may misbehave under mycroft-core because that event is only sent in ovos-core When we introduce new functionality in ovos-core that if used in a skill would cause incompatibilities with mycroft we always make the methods private, as long as a skill does not access any property that starts with an underscore, eg. self._resources , it should work in mycroft-core See the table of Known Incompatibilities Do OPM plugins work in mycroft-core? yes! OPM provides new kinds of plugins not supported by mycroft-core, but STT, TTS, WakeWord and AudioService plugins will work in regular mycroft OPM base classes may contain improvements and new features, such as better caching and automatic viseme generation in TTS plugins, but we try very hard to ensure they remain compatible with mycroft-core dev branch Does PHAL work with mycroft-core? yes! PHAL is a standalone component, it only needs to connect to the mycroft messagebus. You can connect PHAL to vanilla mycroft-core and load any plugin. Depending on the plugin this may make sense or not Known Incompatibilities Here we present a list of know incompatibilities between ovos-core and mycroft-core feature consequence reason workarounds converse deactivated event skill won't know when its no longer active mycroft-core does not emit bus event access private skill property/method skill will not load syntax error port the feature to ovos_workshop using \"mycroft.gui.list.move\" GUI may be messed up mycroft-core does not implement full GUI protocol run ovos-gui, do not run mycroft enclosure","title":"Compatibility"},{"location":"compat/#compatibility-faq","text":"Do OVOS images run on the mark2? Do OVOS skills work in mycroft-core? Do OPM plugins work in mycroft-core? Does PHAL work with mycroft-core? Known Incompatibilities","title":"Compatibility FAQ"},{"location":"compat/#do-ovos-images-run-on-the-mark2","text":"The mark2 developer kit is one of the reference platforms we test and develop against, the ovos-buildroot images can be used in a mark 2 device","title":"Do OVOS images run on the mark2?"},{"location":"compat/#do-ovos-skills-work-in-mycroft-core","text":"If you are a developer please subclass your skills from OVOSSkill provided in ovos-workshop package We implement all skill development tools under the ovos-workshop library, this allows most OVOS functionality to be used in mycroft-core without problems. This includes intent_layers , decorators and killable_events However some skills may decide to depend on features exclusive to ovos-core, or be missing bug fixes in mycroft-core, therefore we can not ensure 100% compatibility. eg, if a skill depends on the converse deactivated event it may misbehave under mycroft-core because that event is only sent in ovos-core When we introduce new functionality in ovos-core that if used in a skill would cause incompatibilities with mycroft we always make the methods private, as long as a skill does not access any property that starts with an underscore, eg. self._resources , it should work in mycroft-core See the table of Known Incompatibilities","title":"Do OVOS skills work in mycroft-core?"},{"location":"compat/#do-opm-plugins-work-in-mycroft-core","text":"yes! OPM provides new kinds of plugins not supported by mycroft-core, but STT, TTS, WakeWord and AudioService plugins will work in regular mycroft OPM base classes may contain improvements and new features, such as better caching and automatic viseme generation in TTS plugins, but we try very hard to ensure they remain compatible with mycroft-core dev branch","title":"Do OPM plugins work in mycroft-core?"},{"location":"compat/#does-phal-work-with-mycroft-core","text":"yes! PHAL is a standalone component, it only needs to connect to the mycroft messagebus. You can connect PHAL to vanilla mycroft-core and load any plugin. Depending on the plugin this may make sense or not","title":"Does PHAL work with mycroft-core?"},{"location":"compat/#known-incompatibilities","text":"Here we present a list of know incompatibilities between ovos-core and mycroft-core feature consequence reason workarounds converse deactivated event skill won't know when its no longer active mycroft-core does not emit bus event access private skill property/method skill will not load syntax error port the feature to ovos_workshop using \"mycroft.gui.list.move\" GUI may be messed up mycroft-core does not implement full GUI protocol run ovos-gui, do not run mycroft enclosure","title":"Known Incompatibilities"},{"location":"config/","text":"Configuration Reading Configuration Configuring Configuration protected_keys disable_user_config disable_remote_config Meta Configuration ovos.conf Reading Configuration ovos_config.config.Configuration is a singleton that loads a single config object. The configuration files loaded are determined by ovos.conf as described below and can be in either json or yaml format. if Configuration() is called the following configs would be loaded in this order: {core-path} /configuration/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.mycroft/mycroft.conf (Deprecated) XDG_CONFIG_DIRS + /mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf XDG_CONFIG_HOME (default ~/.config) + /mycroft/mycroft.conf When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files. Configuring Configuration There are a couple of special configuration keys that change the way the configuration stack loads. Default config refers to the config specified at default_config_path in ovos.conf (#1 {core-path}/configuration/mycroft.conf in the stack above). System config refers to the config at /etc/{base_folder}/{config_filename} (#2 /etc/mycroft/mycroft.conf in the stack above). protected_keys A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations. disable_user_config If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths). disable_remote_config If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded. Meta Configuration while mycroft.conf configures the voice assistant, ovos.conf configures the library what this means is that ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this all XDG paths across OpenVoiceOS packages build their paths taking ovos.conf into consideration this feature is what allows downstream voice assistants such as neon-core to change their config files to neon.yaml Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: {core-path} /configuration/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.neon/neon.yaml (Deprecated) XDG_CONFIG_DIRS + /neon/neon.yaml /etc/xdg/neon/neon.yaml XDG_CONFIG_HOME (default ~/.config) + /neon/neon.yaml ovos.conf The ovos_config package determines which config files to load based on ovos.conf . get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf /etc/mycroft/ovos.conf (Deprecated) XDG_CONFIG_DIRS + /OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf XDG_CONFIG_HOME (default ~/.config) + /OpenVoiceOS/ovos.conf A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} } Note : default_config_path should always be an absolute path. This is generally detected automatically, but any manual override must specify an absolute path to a json or yaml config file. Non-Mycroft modules may specify alternate config paths. A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from mycroft with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } }","title":"Configuration"},{"location":"config/#configuration","text":"Reading Configuration Configuring Configuration protected_keys disable_user_config disable_remote_config Meta Configuration ovos.conf","title":"Configuration"},{"location":"config/#reading-configuration","text":"ovos_config.config.Configuration is a singleton that loads a single config object. The configuration files loaded are determined by ovos.conf as described below and can be in either json or yaml format. if Configuration() is called the following configs would be loaded in this order: {core-path} /configuration/mycroft.conf os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/mycroft/mycroft.conf os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.mycroft/mycroft.conf (Deprecated) XDG_CONFIG_DIRS + /mycroft/mycroft.conf /etc/xdg/mycroft/mycroft.conf XDG_CONFIG_HOME (default ~/.config) + /mycroft/mycroft.conf When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.","title":"Reading Configuration"},{"location":"config/#configuring-configuration","text":"There are a couple of special configuration keys that change the way the configuration stack loads. Default config refers to the config specified at default_config_path in ovos.conf (#1 {core-path}/configuration/mycroft.conf in the stack above). System config refers to the config at /etc/{base_folder}/{config_filename} (#2 /etc/mycroft/mycroft.conf in the stack above).","title":"Configuring Configuration"},{"location":"config/#protected_keys","text":"A \"protected_keys\" configuration section may be added to a Default or System Config file (default /etc/mycroft/mycroft.conf ). This configuration section specifies other configuration keys that may not be specified in remote or user configurations. Keys may specify nested parameters with . to exclude specific keys within nested dictionaries. An example config could be: { \"protected_keys\": { \"remote\": [ \"gui_websocket.host\", \"websocket.host\" ], \"user\": [ \"gui_websocket.host\" ] } } This example specifies that config['gui_websocket']['host'] may be specified in user configuration, but not remote. config['websocket']['host'] may not be specified in user or remote config, so it will only consider default and system configurations.","title":"protected_keys"},{"location":"config/#disable_user_config","text":"If this config parameter is set to True in Default or System configuration, no user configurations will be loaded (no XDG configuration paths).","title":"disable_user_config"},{"location":"config/#disable_remote_config","text":"If this config parameter is set to True in Default or System configuration, the remote configuration ( web_cache.json ) will not be loaded.","title":"disable_remote_config"},{"location":"config/#meta-configuration","text":"while mycroft.conf configures the voice assistant, ovos.conf configures the library what this means is that ovos.conf decides what files are loaded by the Configuration class described above, as an end user or skill developer you should never have to worry about this all XDG paths across OpenVoiceOS packages build their paths taking ovos.conf into consideration this feature is what allows downstream voice assistants such as neon-core to change their config files to neon.yaml Using the above example, if Configuration() is called from neon-core , the following configs would be loaded in this order: {core-path} /configuration/neon.yaml os.environ.get('MYCROFT_SYSTEM_CONFIG') or /etc/neon/neon.yaml os.environ.get('MYCROFT_WEB_CACHE') or XDG_CONFIG_PATH /neon/web_cache.json ~/.neon/neon.yaml (Deprecated) XDG_CONFIG_DIRS + /neon/neon.yaml /etc/xdg/neon/neon.yaml XDG_CONFIG_HOME (default ~/.config) + /neon/neon.yaml","title":"Meta Configuration"},{"location":"config/#ovosconf","text":"The ovos_config package determines which config files to load based on ovos.conf . get_ovos_config will return default values that load mycroft.conf unless otherwise configured. ovos.conf files are loaded in the following order, with later files taking priority over earlier ones in the list: /etc/OpenVoiceOS/ovos.conf /etc/mycroft/ovos.conf (Deprecated) XDG_CONFIG_DIRS + /OpenVoiceOS/ovos.conf /etc/xdg/OpenVoiceOS/ovos.conf XDG_CONFIG_HOME (default ~/.config) + /OpenVoiceOS/ovos.conf A simple ovos_config should have a structure like: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Absolute Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": {}, \"submodule_mappings\": {} } Note : default_config_path should always be an absolute path. This is generally detected automatically, but any manual override must specify an absolute path to a json or yaml config file. Non-Mycroft modules may specify alternate config paths. A call to get_ovos_config from neon_core or neon_messagebus will return a configuration like: { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } } If get_ovos_config was called from mycroft with the same configuration file as the last example, the returned configuration would be: { \"base_folder\": \"mycroft\", \"config_filename\": \"mycroft.conf\", \"default_config_path\": \"<Path to Installed Core>/configuration/mycroft.conf\", \"module_overrides\": { \"neon_core\": { \"base_folder\": \"neon\", \"config_filename\": \"neon.yaml\", \"default_config_path\": \"/etc/example/config/neon.yaml\" } }, \"submodule_mappings\": { \"neon_messagebus\": \"neon_core\", \"neon_speech\": \"neon_core\", \"neon_audio\": \"neon_core\", \"neon_gui\": \"neon_core\" } }","title":"ovos.conf"},{"location":"context/","text":"Conversational Context NOTE: Conversational context is currently only available with the Adapt Intent Parser, and is not yet available for Padatious How tall is John Cleese? \"John Cleese is 196 centimeters\" Where's he from? \"He's from England\" Context is added manually by the Skill creator using either the self.set_context() method or the @adds_context() decorator. Consider the following intent handlers: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): python = message.data.get('PythonPerson') self.speak('{} is {} cm tall'.format(python, length_dict[python])) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): python = message.data.get('PythonPerson') self.speak('{} is from {}'.format(python, from_dict[python])) To interact with the above handlers the user would need to say User: How tall is John Cleese? Mycroft: John Cleese is 196 centimeters User: Where is John Cleese from? Mycroft: He's from England To get a more natural response the functions can be changed to let OVOS know which PythonPerson we're talking about by using the self.set_context() method to give context: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('{} is {} cm tall'.format(python, length_dict[python])) self.set_context('PythonPerson', python) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('He is from {}'.format(from_dict[python])) self.set_context('PythonPerson', python) When either of the methods are called the PythonPerson keyword is added to OVOS's context, which means that if there is a match with Length but PythonPerson is missing OVOS will assume the last mention of that keyword. The interaction can now become the one described at the top of the page. User: How tall is John Cleese? OVOS detects the Length keyword and the PythonPerson keyword OVOS: 196 centimeters John Cleese is added to the current context User: Where's he from? OVOS detects the WhereFrom keyword but not any PythonPerson keyword. The Context Manager is activated and returns the latest entry of PythonPerson which is John Cleese OVOS: He's from England The context isn't limited by the keywords provided by the current Skill . For example @intent_handler(IntentBuilder().require(PythonPerson).require(WhereFrom)) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('He is from {}'.format(from_dict[python])) self.set_context('PythonPerson', python) self.set_context('Location', from_dict[python]) Enables conversations with other Skills as well. User: Where is John Cleese from? Mycroft: He's from England User: What's the weather like over there? Mycroft: Raining and 14 degrees... Using context to enable Intents To make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. Context can be used to create \"bubbles\" of available intent handlers. User: Hey Mycroft, bring me some Tea Mycroft: Of course, would you like Milk with that? User: No Mycroft: How about some Honey? User: All right then Mycroft: Here you go, here's your Tea with Honey from mycroft.skills.context import adds_context, removes_context class TeaSkill(MycroftSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) @intent_handler(IntentBuilder('YesMilkIntent').require(\"YesKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_yes_milk_intent(self, message): self.milk = True self.speak('What about Honey?', expect_response=True) @intent_handler(IntentBuilder('NoHoneyIntent').require(\"NoKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_no_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with a dash of Milk') else: self.speak('Heres your Tea, straight up') @intent_handler(IntentBuilder('YesHoneyIntent').require(\"YesKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_yes_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with Milk and Honey') else: self.speak('Heres your Tea with Honey') When starting up only the TeaIntent will be available. When that has been triggered and MilkContext is added the MilkYesIntent and MilkNoIntent are available since the MilkContext is set. when a yes or no is received the MilkContext is removed and can't be accessed. In it's place the HoneyContext is added making the YesHoneyIntent and NoHoneyIntent available. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree .","title":"Context"},{"location":"context/#conversational-context","text":"NOTE: Conversational context is currently only available with the Adapt Intent Parser, and is not yet available for Padatious How tall is John Cleese? \"John Cleese is 196 centimeters\" Where's he from? \"He's from England\" Context is added manually by the Skill creator using either the self.set_context() method or the @adds_context() decorator. Consider the following intent handlers: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): python = message.data.get('PythonPerson') self.speak('{} is {} cm tall'.format(python, length_dict[python])) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): python = message.data.get('PythonPerson') self.speak('{} is from {}'.format(python, from_dict[python])) To interact with the above handlers the user would need to say User: How tall is John Cleese? Mycroft: John Cleese is 196 centimeters User: Where is John Cleese from? Mycroft: He's from England To get a more natural response the functions can be changed to let OVOS know which PythonPerson we're talking about by using the self.set_context() method to give context: @intent_handler(IntentBuilder().require('PythonPerson').require('Length')) def handle_length(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('{} is {} cm tall'.format(python, length_dict[python])) self.set_context('PythonPerson', python) @intent_handler(IntentBuilder().require('PythonPerson').require('WhereFrom')) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('He is from {}'.format(from_dict[python])) self.set_context('PythonPerson', python) When either of the methods are called the PythonPerson keyword is added to OVOS's context, which means that if there is a match with Length but PythonPerson is missing OVOS will assume the last mention of that keyword. The interaction can now become the one described at the top of the page. User: How tall is John Cleese? OVOS detects the Length keyword and the PythonPerson keyword OVOS: 196 centimeters John Cleese is added to the current context User: Where's he from? OVOS detects the WhereFrom keyword but not any PythonPerson keyword. The Context Manager is activated and returns the latest entry of PythonPerson which is John Cleese OVOS: He's from England The context isn't limited by the keywords provided by the current Skill . For example @intent_handler(IntentBuilder().require(PythonPerson).require(WhereFrom)) def handle_from(self, message): # PythonPerson can be any of the Monty Python members python = message.data.get('PythonPerson') self.speak('He is from {}'.format(from_dict[python])) self.set_context('PythonPerson', python) self.set_context('Location', from_dict[python]) Enables conversations with other Skills as well. User: Where is John Cleese from? Mycroft: He's from England User: What's the weather like over there? Mycroft: Raining and 14 degrees...","title":"Conversational Context"},{"location":"context/#using-context-to-enable-intents","text":"To make sure certain Intents can't be triggered unless some previous stage in a conversation has occurred. Context can be used to create \"bubbles\" of available intent handlers. User: Hey Mycroft, bring me some Tea Mycroft: Of course, would you like Milk with that? User: No Mycroft: How about some Honey? User: All right then Mycroft: Here you go, here's your Tea with Honey from mycroft.skills.context import adds_context, removes_context class TeaSkill(MycroftSkill): @intent_handler(IntentBuilder('TeaIntent').require(\"TeaKeyword\")) @adds_context('MilkContext') def handle_tea_intent(self, message): self.milk = False self.speak('Of course, would you like Milk with that?', expect_response=True) @intent_handler(IntentBuilder('NoMilkIntent').require(\"NoKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_no_milk_intent(self, message): self.speak('all right, any Honey?', expect_response=True) @intent_handler(IntentBuilder('YesMilkIntent').require(\"YesKeyword\"). require('MilkContext').build()) @removes_context('MilkContext') @adds_context('HoneyContext') def handle_yes_milk_intent(self, message): self.milk = True self.speak('What about Honey?', expect_response=True) @intent_handler(IntentBuilder('NoHoneyIntent').require(\"NoKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_no_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with a dash of Milk') else: self.speak('Heres your Tea, straight up') @intent_handler(IntentBuilder('YesHoneyIntent').require(\"YesKeyword\"). require('HoneyContext').build()) @removes_context('HoneyContext') def handle_yes_honey_intent(self, message): if self.milk: self.speak('Heres your Tea with Milk and Honey') else: self.speak('Heres your Tea with Honey') When starting up only the TeaIntent will be available. When that has been triggered and MilkContext is added the MilkYesIntent and MilkNoIntent are available since the MilkContext is set. when a yes or no is received the MilkContext is removed and can't be accessed. In it's place the HoneyContext is added making the YesHoneyIntent and NoHoneyIntent available. You can find an example Tea Skill using conversational context on Github . As you can see, Conversational Context lends itself well to implementing a dialog tree or conversation tree .","title":"Using context to enable Intents"},{"location":"converse/","text":"Converse Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False. Basic usage Let's use a version of the Ice Cream Skill we've been building up and add a converse method to catch any brief statements of thanks that might directly follow an order. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming-right-up', {'flavor': selection}) def converse(self, message): if self.voc_match(message.data['utterances'][0], 'Thankyou'): self.speak_dialog(\"you-are-welcome\") return True def create_skill(): return IceCreamSkill() In this example: A User might request an ice cream which is handled by handle_request_icecream() The Skill would be added to the system Active Skill list for up to 5 minutes. Any utterance received by OVOS would trigger this Skills converse system whilst it is considered active. If the User followed up with a pleasantry such as \"Hey Mycroft, thanks\" - the converse method would match this vocab against the Thankyou.voc file in the Skill and speak the contents of the you-are-welcome.dialog file. The method would return True and the utterance would be consumed meaning the intent parsing service would never be triggered. Any utterance that did not match would be silently ignored and allowed to continue on to other converse methods and finally to the intent parsing service. Active Skill List A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service Making a Skill Active There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.make_active() to manually add our Skill to the front of the Active Skills List.","title":"Converse"},{"location":"converse/#converse","text":"Each Skill may define a converse() method. This method will be called anytime the Skill has been recently active and a new utterance is processed. The converse method expects a single argument which is a standard Mycroft Message object. This is the same object an intent handler receives. Converse methods must return a Boolean value. True if an utterance was handled, otherwise False.","title":"Converse"},{"location":"converse/#basic-usage","text":"Let's use a version of the Ice Cream Skill we've been building up and add a converse method to catch any brief statements of thanks that might directly follow an order. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming-right-up', {'flavor': selection}) def converse(self, message): if self.voc_match(message.data['utterances'][0], 'Thankyou'): self.speak_dialog(\"you-are-welcome\") return True def create_skill(): return IceCreamSkill() In this example: A User might request an ice cream which is handled by handle_request_icecream() The Skill would be added to the system Active Skill list for up to 5 minutes. Any utterance received by OVOS would trigger this Skills converse system whilst it is considered active. If the User followed up with a pleasantry such as \"Hey Mycroft, thanks\" - the converse method would match this vocab against the Thankyou.voc file in the Skill and speak the contents of the you-are-welcome.dialog file. The method would return True and the utterance would be consumed meaning the intent parsing service would never be triggered. Any utterance that did not match would be silently ignored and allowed to continue on to other converse methods and finally to the intent parsing service.","title":"Basic usage"},{"location":"converse/#active-skill-list","text":"A Skill is considered active if it has been called in the last 5 minutes. Skills are called in order of when they were last active. For example, if a user spoke the following commands: Hey Mycroft, set a timer for 10 minutes Hey Mycroft, what's the weather Then the utterance \"what's the weather\" would first be sent to the Timer Skill's converse() method, then to the intent service for normal handling where the Weather Skill would be called. As the Weather Skill was called it has now been added to the front of the Active Skills List. Hence, the next utterance received will be directed to: WeatherSkill.converse() TimerSkill.converse() Normal intent parsing service","title":"Active Skill List"},{"location":"converse/#making-a-skill-active","text":"There are occasions where a Skill has not been triggered by the User, but it should still be considered \"Active\". In the case of our Ice Cream Skill - we might have a function that will execute when the customers order is ready. At this point, we also want to be responsive to the customers thanks, so we call self.make_active() to manually add our Skill to the front of the Active Skills List.","title":"Making a Skill Active"},{"location":"core/","text":"ovos-core OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OVOS-core is a backwards-compatible descendant of Mycroft-core , the central component of Mycroft. It contains extensions and features not present upstream. All Mycroft Skills and Plugins should work normally with OVOS-core. OVOS-core is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library! Getting Started ovos-core is very modular, depending on where you are running ovos-core you may want to run only a subset of the services by default ovos-core only installs the minimum components common to all services, for the purposes of this document we will assume you want a full install if you want to finetune the components please replace [all] in commands below with the subset of desired extras, eg [skills,bus] Installing ovos-core ovos-core can be installed from pypi or from source if install fails you may need to install some system dependencies, how to do this will depend on your distro sudo apt install build-essential python3-dev swig libssl-dev libfann-dev portaudio19-dev libpulse-dev Note : MycroftAI's dev_setup.sh does not exist in OVOS-core. from source We suggest you do this in a virtualenv: pip install git+https://github.com/OpenVoiceOS/ovos-core[all] from pypi pip install ovos-core[all] Running ovos-core Developer launcher script start-mycroft.sh is available to perform common tasks. Assuming you installed ovos-core in your home directory, run: cd ~/ovos-core ./start-mycroft.sh debug The \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface. Later you can bring up the CLI using ./start-mycroft.sh cli . The background services can be stopped as a group with: ./stop-mycroft.sh Automatically on boot We recommend you create system services to manage ovos instead of depending on the launcher script above A good explanation can be found here https://github.com/j1nx/mycroft-systemd A reference implementation can be found in ovos-buildroot","title":"Introduction"},{"location":"core/#ovos-core","text":"OpenVoiceOS is an open source platform for smart speakers and other voice-centric devices. OVOS-core is a backwards-compatible descendant of Mycroft-core , the central component of Mycroft. It contains extensions and features not present upstream. All Mycroft Skills and Plugins should work normally with OVOS-core. OVOS-core is fully modular. Furthermore, common components have been repackaged as plugins. That means it isn't just a great assistant on its own, but also a pretty small library!","title":"ovos-core"},{"location":"core/#getting-started","text":"ovos-core is very modular, depending on where you are running ovos-core you may want to run only a subset of the services by default ovos-core only installs the minimum components common to all services, for the purposes of this document we will assume you want a full install if you want to finetune the components please replace [all] in commands below with the subset of desired extras, eg [skills,bus]","title":"Getting Started"},{"location":"core/#installing-ovos-core","text":"ovos-core can be installed from pypi or from source if install fails you may need to install some system dependencies, how to do this will depend on your distro sudo apt install build-essential python3-dev swig libssl-dev libfann-dev portaudio19-dev libpulse-dev Note : MycroftAI's dev_setup.sh does not exist in OVOS-core.","title":"Installing ovos-core"},{"location":"core/#from-source","text":"We suggest you do this in a virtualenv: pip install git+https://github.com/OpenVoiceOS/ovos-core[all]","title":"from source"},{"location":"core/#from-pypi","text":"pip install ovos-core[all]","title":"from pypi"},{"location":"core/#running-ovos-core","text":"","title":"Running ovos-core"},{"location":"core/#developer-launcher-script","text":"start-mycroft.sh is available to perform common tasks. Assuming you installed ovos-core in your home directory, run: cd ~/ovos-core ./start-mycroft.sh debug The \"debug\" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface. Later you can bring up the CLI using ./start-mycroft.sh cli . The background services can be stopped as a group with: ./stop-mycroft.sh","title":"Developer launcher script"},{"location":"core/#automatically-on-boot","text":"We recommend you create system services to manage ovos instead of depending on the launcher script above A good explanation can be found here https://github.com/j1nx/mycroft-systemd A reference implementation can be found in ovos-buildroot","title":"Automatically on boot"},{"location":"dev/","text":"Developer FAQ How do I know what is currently happening in the GUI? How do I stop an intent mid execution? How do I send files over the bus? How do I use OAuth in a skill? How do I manage remote skill settings? How do I share data between devices? How do I use Geolocation backend services? How do I use Weather backend services? How do I use WolframAlpha backend services? How do I know what is currently happening in the GUI? from ovos_utils.gui import GUITracker from ovos_workshop.skills import OVOSSkill from mycroft import intent_handler class MyGUIEventTracker(GUITracker): # GUI event handlers # skill can/should subclass this def on_idle(self, namespace): print(\"IDLE\", namespace) timestamp = self.idle_ts def on_active(self, namespace): # NOTE: page has not been loaded yet # event will fire right after this one print(\"ACTIVE\", namespace) # check namespace values, they should all be set before this event values = self.gui_values[namespace] def on_new_page(self, page, namespace, index): print(\"NEW PAGE\", namespace, index, namespace) # check all loaded pages for n in self.gui_pages: # list of named tuples nspace = n.name # namespace / skill_id pages = n.pages # ordered list of page uris def on_gui_value(self, namespace, key, value): # WARNING this will pollute logs quite a lot, and you will get # duplicates, better to check values on a different event, # demonstrated in on_active print(\"VALUE\", namespace, key, value) class MySkill(OVOSSkill): def initialize(self): self.tracker = MyGUIEventTracker(bus=self.bus) @intent_handler(\"gui.status.intent\") def handle_status_intent(self, message): print(\"device has screen:\", self.tracker.can_display()) print(\"mycroft-gui installed:\", self.tracker.is_gui_installed()) print(\"gui connected:\", self.tracker.is_gui_connected()) # TODO - speak or something @intent_handler(\"list.idle.screens.intent\") def handle_idle_screens_intent(self, message): # check registered idle screens print(\"Registered idle screens:\") for name in self.tracker.idle_screens: skill_id = self.tracker.idle_screens[name] print(\" - \", name, \":\", skill_id) # TODO - speak or something How do I stop an intent mid execution? Sometimes you want to abort a running intent immediately, the stop method may not be enough in some circumstances we provide a killable_intent decorator in ovos_workshop that can be used to abort a running intent immediately a common use case is for GUI interfaces where the same action may be done by voice or clicking buttons, in this case you may need to abort a running get_response loop from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import killable_intent from mycroft import intent_handler from time import sleep class Test(OVOSSkill): \"\"\" send \"mycroft.skills.abort_question\" and confirm only get_response is aborted send \"mycroft.skills.abort_execution\" and confirm the full intent is aborted, except intent3 send \"my.own.abort.msg\" and confirm intent3 is aborted say \"stop\" and confirm all intents are aborted \"\"\" def __init__(self): super(Test, self).__init__(\"KillableSkill\") self.my_special_var = \"default\" def handle_intent_aborted(self): self.speak(\"I am dead\") # handle any cleanup the skill might need, since intent was killed # at an arbitrary place of code execution some variables etc. might # end up in unexpected states self.my_special_var = \"default\" @killable_intent(callback=handle_intent_aborted) @intent_handler(\"test.intent\") def handle_test_abort_intent(self, message): self.my_special_var = \"changed\" while True: sleep(1) self.speak(\"still here\") @intent_handler(\"test2.intent\") @killable_intent(callback=handle_intent_aborted) def handle_test_get_response_intent(self, message): self.my_special_var = \"CHANGED\" ans = self.get_response(\"question\", num_retries=99999) self.log.debug(\"get_response returned: \" + str(ans)) if ans is None: self.speak(\"question aborted\") @killable_intent(msg=\"my.own.abort.msg\", callback=handle_intent_aborted) @intent_handler(\"test3.intent\") def handle_test_msg_intent(self, message): if self.my_special_var != \"default\": self.speak(\"someone forgot to cleanup\") while True: sleep(1) self.speak(\"you can't abort me\") How do I send files over the bus? Sometimes you may want to send files or binary data over the messagebus, ovos_utils provides some tools to make this easy Sending a file from ovos_utils.messagebus import send_binary_file_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.file\", self.receive_file) def receive_file(self, message): print(\"Receiving file\") path = message.data[\"path\"] # file path, extract filename if needed binary_data = decode_binary_message(message) # TODO process data somehow def send_file(self, my_file_path): send_binary_file_message(my_file_path) Sending binary data directly from ovos_utils.messagebus import send_binary_data_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.data\", self.receive_binary) def send_data(self, binary_data): send_binary_data_message(binary_data) def receive_binary(self, message): print(\"Receiving binary data\") binary_data = decode_binary_message(message) # TODO process data somehow How do I manage remote skill settings? To interact with skill settings via DeviceApi from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") # in mycroft-core please ensure a valid remote_id # in MycroftSkill class you can use # remote_id = self.settings_meta.skill_gid # s = RemoteSkillSettings(\"skill.author\", remote_id=\"@|whatever_msm_decided\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload() How do I share data between devices? by hijacking skill settings we allow storing arbitrary data via DeviceApi and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you don't trust the backend! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\") How do I use Geolocation backend services? from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\")","title":"F.A.Q."},{"location":"dev/#developer-faq","text":"How do I know what is currently happening in the GUI? How do I stop an intent mid execution? How do I send files over the bus? How do I use OAuth in a skill? How do I manage remote skill settings? How do I share data between devices? How do I use Geolocation backend services? How do I use Weather backend services? How do I use WolframAlpha backend services?","title":"Developer FAQ"},{"location":"dev/#how-do-i-know-what-is-currently-happening-in-the-gui","text":"from ovos_utils.gui import GUITracker from ovos_workshop.skills import OVOSSkill from mycroft import intent_handler class MyGUIEventTracker(GUITracker): # GUI event handlers # skill can/should subclass this def on_idle(self, namespace): print(\"IDLE\", namespace) timestamp = self.idle_ts def on_active(self, namespace): # NOTE: page has not been loaded yet # event will fire right after this one print(\"ACTIVE\", namespace) # check namespace values, they should all be set before this event values = self.gui_values[namespace] def on_new_page(self, page, namespace, index): print(\"NEW PAGE\", namespace, index, namespace) # check all loaded pages for n in self.gui_pages: # list of named tuples nspace = n.name # namespace / skill_id pages = n.pages # ordered list of page uris def on_gui_value(self, namespace, key, value): # WARNING this will pollute logs quite a lot, and you will get # duplicates, better to check values on a different event, # demonstrated in on_active print(\"VALUE\", namespace, key, value) class MySkill(OVOSSkill): def initialize(self): self.tracker = MyGUIEventTracker(bus=self.bus) @intent_handler(\"gui.status.intent\") def handle_status_intent(self, message): print(\"device has screen:\", self.tracker.can_display()) print(\"mycroft-gui installed:\", self.tracker.is_gui_installed()) print(\"gui connected:\", self.tracker.is_gui_connected()) # TODO - speak or something @intent_handler(\"list.idle.screens.intent\") def handle_idle_screens_intent(self, message): # check registered idle screens print(\"Registered idle screens:\") for name in self.tracker.idle_screens: skill_id = self.tracker.idle_screens[name] print(\" - \", name, \":\", skill_id) # TODO - speak or something","title":"How do I know what is currently happening in the GUI?"},{"location":"dev/#how-do-i-stop-an-intent-mid-execution","text":"Sometimes you want to abort a running intent immediately, the stop method may not be enough in some circumstances we provide a killable_intent decorator in ovos_workshop that can be used to abort a running intent immediately a common use case is for GUI interfaces where the same action may be done by voice or clicking buttons, in this case you may need to abort a running get_response loop from ovos_workshop.skills import OVOSSkill from ovos_workshop.decorators import killable_intent from mycroft import intent_handler from time import sleep class Test(OVOSSkill): \"\"\" send \"mycroft.skills.abort_question\" and confirm only get_response is aborted send \"mycroft.skills.abort_execution\" and confirm the full intent is aborted, except intent3 send \"my.own.abort.msg\" and confirm intent3 is aborted say \"stop\" and confirm all intents are aborted \"\"\" def __init__(self): super(Test, self).__init__(\"KillableSkill\") self.my_special_var = \"default\" def handle_intent_aborted(self): self.speak(\"I am dead\") # handle any cleanup the skill might need, since intent was killed # at an arbitrary place of code execution some variables etc. might # end up in unexpected states self.my_special_var = \"default\" @killable_intent(callback=handle_intent_aborted) @intent_handler(\"test.intent\") def handle_test_abort_intent(self, message): self.my_special_var = \"changed\" while True: sleep(1) self.speak(\"still here\") @intent_handler(\"test2.intent\") @killable_intent(callback=handle_intent_aborted) def handle_test_get_response_intent(self, message): self.my_special_var = \"CHANGED\" ans = self.get_response(\"question\", num_retries=99999) self.log.debug(\"get_response returned: \" + str(ans)) if ans is None: self.speak(\"question aborted\") @killable_intent(msg=\"my.own.abort.msg\", callback=handle_intent_aborted) @intent_handler(\"test3.intent\") def handle_test_msg_intent(self, message): if self.my_special_var != \"default\": self.speak(\"someone forgot to cleanup\") while True: sleep(1) self.speak(\"you can't abort me\")","title":"How do I stop an intent mid execution?"},{"location":"dev/#how-do-i-send-files-over-the-bus","text":"Sometimes you may want to send files or binary data over the messagebus, ovos_utils provides some tools to make this easy Sending a file from ovos_utils.messagebus import send_binary_file_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.file\", self.receive_file) def receive_file(self, message): print(\"Receiving file\") path = message.data[\"path\"] # file path, extract filename if needed binary_data = decode_binary_message(message) # TODO process data somehow def send_file(self, my_file_path): send_binary_file_message(my_file_path) Sending binary data directly from ovos_utils.messagebus import send_binary_data_message, decode_binary_message from ovos_workshop.skills import OVOSSkill class MySkill(OVOSSkill): def initialize(self): self.add_event(\"mycroft.binary.data\", self.receive_binary) def send_data(self, binary_data): send_binary_data_message(binary_data) def receive_binary(self, message): print(\"Receiving binary data\") binary_data = decode_binary_message(message) # TODO process data somehow","title":"How do I send files over the bus?"},{"location":"dev/#how-do-i-manage-remote-skill-settings","text":"To interact with skill settings via DeviceApi from ovos_backend_client.settings import RemoteSkillSettings # in ovos-core skill_id is deterministic and safe s = RemoteSkillSettings(\"skill.author\") # in mycroft-core please ensure a valid remote_id # in MycroftSkill class you can use # remote_id = self.settings_meta.skill_gid # s = RemoteSkillSettings(\"skill.author\", remote_id=\"@|whatever_msm_decided\") s.download() s.settings[\"existing_value\"] = True s.settings[\"new_value\"] = \"will NOT show up in UI\" s.upload() # auto generate new settings meta for all new values before uploading s.settings[\"new_value\"] = \"will show up in UI\" s.generate_meta() # now \"new_value\" is in meta s.upload()","title":"How do I manage remote skill settings?"},{"location":"dev/#how-do-i-share-data-between-devices","text":"by hijacking skill settings we allow storing arbitrary data via DeviceApi and use it across devices and skills from ovos_backend_client.cloud import SeleneCloud cloud = SeleneCloud() cloud.add_entry(\"test\", {\"secret\": \"NOT ENCRYPTED MAN\"}) data = cloud.get_entry(\"test\") an encrypted version is also supported if you don't trust the backend! from ovos_backend_client.cloud import SecretSeleneCloud k = \"D8fmXEP5VqzVw2HE\" # you need this to read back the data cloud = SecretSeleneCloud(k) cloud.add_entry(\"test\", {\"secret\": \"secret data, selene cant read this\"}) data = cloud.get_entry(\"test\")","title":"How do I share data between devices?"},{"location":"dev/#how-do-i-use-geolocation-backend-services","text":"from ovos_backend_client.api import GeolocationApi geo = GeolocationApi() data = geo.get_geolocation(\"Lisbon Portugal\")","title":"How do I use Geolocation backend services?"},{"location":"dinkum/","text":"Dinkum What is Dinkum Mycroft Mark2 shipped with a new version of mycroft called \"dinkum\", this is a total overhaul of mycroft-core and incompatible mycroft-core is now referred to as \"Classic Core\" by MycroftAI MycroftAI now provides what they call sandbox images, to add to the confusion those only work in the mark 2 and \"Classic Core\" means the mark-ii/latest branch of mycroft-core, this is a derivative version of the branch that was used in the dev kits ( mark-ii/qa ) and is also backwards incompatible, changes in this branch were not done via PRs and had no review or community input Mark2 useful links: mark 2 docs dinkum source code sandbox images mark2/latest (core for sandbox images) mark2/qa (core for dev kit image) Dinkum vs ovos-core you can find mycroft's guide to porting skills to dinkum here https://mycroft-ai.gitbook.io/mark-ii/differences-to-classic-core/porting-classic-core-skills mark2/qa brought some changes to mycroft-core, not all of them backwards compatible and some that were contentious within the community. VAD - VAD has been added to the mark-ii, but it is hardcoded to silero, this feature has been adopted via OPM, it is an important part of ovos-core listening modes introduced in version 0.0.5 self.resources - resource file loading was overhauled, this feature has been improved ( ovos-core/pull/130 + ovos-core/pull/131 + ovos-core/pull/135 + ovos-core/pull/170 ) and ported to OVOS and is also available in OVOSkill class ( OVOS-workshop/pull/30 ) for usage in classic core audio hal - audio playback was rewritten from scratch, audio plugin support has been removed, OVOS will not adopt this new approach but keep improving the previous one skill states - converse method introduced skill states, this changed some core assumptions behind converse method and active skills, OVOS will not adopt skill states, see community discussion here mycroft-core/pull/2901 + mycroft-core/pull/2906 pure regex intents - pure regex intents have been introduced, we strongly recommend you use padatious instead if this is desired, regex makes language support really hard, let the intent engines do their jobs adapt fork - a fork of adapt is used in the mark2, it introduces the exactly and excludes methods. excludes will be added upstream in adapt/pull/156 . Any skill using these new methods will be incompatible with most core versions activities - an activity is just a set of bus messages to indicate something started and ended, it is a reimplementation of an already existing feature, in ovos we use the native events from the self.add_event skill method dinkum contains all changes above and also brought further changes to the table sessions - in dinkum session handling is done by skills, it completely ignores the message.context mechanism and existing session_id, in ovos we believe session should come in the message and handled by the clients (eg, a chat user or a hivemind client....), in ovos we are expanding the original session concept ovos-core/pull/160 dbus-hal - a dbus service specific to the mk2 has been introduced, in ovos we have a generic PHAL service and companion plugins to interface with mk2 hardware instead, this component is mark2 specific and should be ignored in the ovos ecosystem Any skills using these new \"features\" will not work outside the mark2 FAQ Do OVOS skills run in dinkum? No, not even classic core skills run in dinkum. We have no plans to support this Do Dinkum skills run in ovos? No, dinkum is designed in a very incompatible way, the mycroft module is not always mycroft-core and the MycroftSkill class is not always a MycroftSkill, we have no intention of transparently loading dinkum skills in ovos-core We have a small proof of concept tool to convert a dinkum skill into an ovos/classic core compatible skill, see https://github.com/OpenVoiceOS/undinkumfier Does OCP work in dinkum? No, Audio plugin support has been removed, you can run OCP standalone but will be missing the compatibility layers and can't load OCP skills anyway It could be made to work but this is not in the roadmap, PRs will be accepted and reviewed Does PHAL work in dinkum? It should! We don't explicitly target or test it with dinkum, but it is a fairly standalone component Does OPM work in dinkum? STT , TTS and WW plugins should work, We don't explicitly target or test compatibility, PRs will be accepted and reviewed","title":"Dinkum"},{"location":"dinkum/#dinkum","text":"","title":"Dinkum"},{"location":"dinkum/#what-is-dinkum","text":"Mycroft Mark2 shipped with a new version of mycroft called \"dinkum\", this is a total overhaul of mycroft-core and incompatible mycroft-core is now referred to as \"Classic Core\" by MycroftAI MycroftAI now provides what they call sandbox images, to add to the confusion those only work in the mark 2 and \"Classic Core\" means the mark-ii/latest branch of mycroft-core, this is a derivative version of the branch that was used in the dev kits ( mark-ii/qa ) and is also backwards incompatible, changes in this branch were not done via PRs and had no review or community input Mark2 useful links: mark 2 docs dinkum source code sandbox images mark2/latest (core for sandbox images) mark2/qa (core for dev kit image)","title":"What is Dinkum"},{"location":"dinkum/#dinkum-vs-ovos-core","text":"you can find mycroft's guide to porting skills to dinkum here https://mycroft-ai.gitbook.io/mark-ii/differences-to-classic-core/porting-classic-core-skills mark2/qa brought some changes to mycroft-core, not all of them backwards compatible and some that were contentious within the community. VAD - VAD has been added to the mark-ii, but it is hardcoded to silero, this feature has been adopted via OPM, it is an important part of ovos-core listening modes introduced in version 0.0.5 self.resources - resource file loading was overhauled, this feature has been improved ( ovos-core/pull/130 + ovos-core/pull/131 + ovos-core/pull/135 + ovos-core/pull/170 ) and ported to OVOS and is also available in OVOSkill class ( OVOS-workshop/pull/30 ) for usage in classic core audio hal - audio playback was rewritten from scratch, audio plugin support has been removed, OVOS will not adopt this new approach but keep improving the previous one skill states - converse method introduced skill states, this changed some core assumptions behind converse method and active skills, OVOS will not adopt skill states, see community discussion here mycroft-core/pull/2901 + mycroft-core/pull/2906 pure regex intents - pure regex intents have been introduced, we strongly recommend you use padatious instead if this is desired, regex makes language support really hard, let the intent engines do their jobs adapt fork - a fork of adapt is used in the mark2, it introduces the exactly and excludes methods. excludes will be added upstream in adapt/pull/156 . Any skill using these new methods will be incompatible with most core versions activities - an activity is just a set of bus messages to indicate something started and ended, it is a reimplementation of an already existing feature, in ovos we use the native events from the self.add_event skill method dinkum contains all changes above and also brought further changes to the table sessions - in dinkum session handling is done by skills, it completely ignores the message.context mechanism and existing session_id, in ovos we believe session should come in the message and handled by the clients (eg, a chat user or a hivemind client....), in ovos we are expanding the original session concept ovos-core/pull/160 dbus-hal - a dbus service specific to the mk2 has been introduced, in ovos we have a generic PHAL service and companion plugins to interface with mk2 hardware instead, this component is mark2 specific and should be ignored in the ovos ecosystem Any skills using these new \"features\" will not work outside the mark2","title":"Dinkum vs ovos-core"},{"location":"dinkum/#faq","text":"","title":"FAQ"},{"location":"dinkum/#do-ovos-skills-run-in-dinkum","text":"No, not even classic core skills run in dinkum. We have no plans to support this","title":"Do OVOS skills run in dinkum?"},{"location":"dinkum/#do-dinkum-skills-run-in-ovos","text":"No, dinkum is designed in a very incompatible way, the mycroft module is not always mycroft-core and the MycroftSkill class is not always a MycroftSkill, we have no intention of transparently loading dinkum skills in ovos-core We have a small proof of concept tool to convert a dinkum skill into an ovos/classic core compatible skill, see https://github.com/OpenVoiceOS/undinkumfier","title":"Do Dinkum skills run in ovos?"},{"location":"dinkum/#does-ocp-work-in-dinkum","text":"No, Audio plugin support has been removed, you can run OCP standalone but will be missing the compatibility layers and can't load OCP skills anyway It could be made to work but this is not in the roadmap, PRs will be accepted and reviewed","title":"Does OCP work in dinkum?"},{"location":"dinkum/#does-phal-work-in-dinkum","text":"It should! We don't explicitly target or test it with dinkum, but it is a fairly standalone component","title":"Does PHAL work in dinkum?"},{"location":"dinkum/#does-opm-work-in-dinkum","text":"STT , TTS and WW plugins should work, We don't explicitly target or test compatibility, PRs will be accepted and reviewed","title":"Does OPM work in dinkum?"},{"location":"faq/","text":"Frequently Asked Questions What is OVOS? How did OVOS start? Who is behind OVOS? What is the relationship between OVOS and Mycroft? How does OVOS make money? Where is your website? Does OVOS have any default skills? Does OVOS work offline? Does OVOS depend on any servers? How many voices does OVOS support? Can I change the wake word? Can OVOS run without a wake word? How fast can OVOS respond? What is OPM? What is PHAL? What is OCP? What is OVOS-shell? How do I run OVOS behind a proxy? What is OVOS? OVOS aims to be a full operating system that is free and open source. The Open Voice Operating System consists of OVOS packages (programs specifically released by the OVOS Project) as well as free software released by third parties such as skills and plugins. OVOS makes it possible to voice enable technology without software that would trample your freedom. Historically OVOS has been used to refer to several things, the team, the github organization and the reference buildroot implementation How did OVOS start? OVOS started as MycroftOS, you can find the original mycroft forums thread here . Over time more mycroft community members joined the project, and it was renamed to OpenVoiceOS to avoid trademark issues. Initially OVOS was focused on bundling mycroft-core and on creating only companion software, but due to contributions not being accepted upstream we now maintain an enhanced reference fork of mycroft-core with extra functionality, while keeping all companion software mycroft-core (dev branch) compatible You can think of OVOS as the unsanctioned \"Mycroft Community Edition\" Who is behind OVOS? Everyone in the OVOS team is a long term mycroft community member and has experience working with the mycroft code base Meet the team: Peter Steenbergen - mycroft community developer since 2018, founder of MycroftOS project Casimiro Ferreira - mycroft community developer since 2017, co-founder of HelloChatterbox Aditya Mehra - mycroft community developer since 2016, mycroft-gui lead developer Daniel McKnight - community developer since 2017, NeonGecko lead developer Parker Seaman - mycroft enthusiast since 2018 Chance - mycroft community developer since 2019, ex-maintainer of lingua_franca currently taking a break, he will be back! What is the relationship between OVOS and Mycroft? Both projects are fully independent, initially OVOS was focused on wrapping mycroft-core with a minimal OS, but as both projects matured, ovos-core was created to include extra functionality and make OVOS development faster and more efficient. OVOS has been committed to keeping our components compatible with Mycroft and many of our changes are submitted to Mycroft to include in their projects at their discretion. How does OVOS make money? We don't, OVOS is a volunteer project with no source of income or business model However, we want to acknowledge Blue Systems and NeonGeckoCom , a lot of the work in OVOS is done on paid company time from these projects Where is your website? website - openvoiceos.com chat - matrix forums - github discussions Does OVOS have any default skills? We provide essential skills and those are bundled in all our reference images. ovos-core does not manage your skills, unlike mycroft it won't install or update anything by itself. if you installed ovos-core manually you also need to install skills manually Does OVOS work offline? By default ovos-core does not require a backend internet server to operate. Some skills can be accessed (via command line) entirely offline. The default speech-to-text (STT) engine currently requires an internet connection, though some self-hosted, offline options are available. Individual skills and plugins may require internet, and most of the time you will want to use those. Does OVOS depend on any servers? no! you can integrate ovos-core with selene or personal backend but that is fully optional we provide some microservices for some of our skills, but you can also use your own api keys How many voices does OVOS support? hundreds! nearly everything in OVOS is modular and configurable, that includes Text To Speech. Voices depend on language and the plugins you have installed, you can find a non-exhaustive list of plugins in the ovos plugins awesome list Can I change the wake word? yes, ovos-core supports several wake word plugins . Additionally, OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word Can OVOS run without a wake word? mostly yes, depending on exactly what you mean by this question OVOS can run without any wake word configured, in this case you will only be able to interact via CLI or button press, best for privacy, not so great for a smart speaker ovos-core also provides a couple experimental settings, if you enable continuous listening then VAD will be used to detect speech and no wake word is needed, just speak to mycroft and it should answer! However, this setting is experimental for a reason, you may find that mycroft answers your TV or even tries to answer itself if your hardware does not have AEC Another experimental setting is hybrid mode, with hybrid mode you can ask follow-up questions, up to 45 seconds after the last mycroft interaction, if you do not interact with mycroft it will go back to waiting for a wake word How fast can OVOS respond? By default, to answer a request: Detects the wake word Records 3 - 10 seconds of audio Transcribes the audio and returns the text transcription , either locally or remotely, depending on the speech-to-text (STT) engine in use Parses the text to understand the intent Sends the text to the intent handler with the highest confidence Allows the Skill to perform some action and provide the text to be spoken Synthesizes audio from the given text, either locally or remotely, depending on the text-to-speech (TTS) engine in use Plays the synthesized spoken audio. Through this process there are a number of factors that can affect the perceived speed of responses: System resources - more processing power and memory never hurts! Network latency - depending on configured plugins, network latency and connection speed can play a significant role in slowing down response times. Streaming STT - we have been experimenting with the use of streaming services. This transcribes audio as it's received rather than waiting for the entire utterance to be finished and sending the resulting audio file to a server to be processed in its entirety. It is possible to switch to a streaming STT service. See STT Plugins for a list of options available. Dialog structure - a long sentence will always take more time to synthesize than a short one. Skill developers can help provide quicker response times by considering the structure of their dialog and breaking that dialog up. TTS Caching - synthesized audio is cached meaning common recently generated phrases don't need to be generated, they can be returned immediately. What is OPM? OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent from core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Some projects using OPM are ovos-core , hivemind-voice-sat , ovos-personal-backend , ovos-stt-server and ovos-tts-server What is PHAL? PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images What is OCP? OCP stands for OpenVoiceOS Common Play, it is a full fledged media player OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from mycroft-core in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill mycroft-core CommonPlay skill framework is disabled when OCP loads OCP skills have a dedicated MycroftSkill class and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP What is OVOS-shell? The gui service in ovos-core will expose a websocket to the GUI client following the protocol outlined here The GUI library which implements the protocol lives in the mycroft-gui repository, The repository also hosts a development client for skill developers wanting to develop on the desktop. OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images, other distributions may offer alternative implementations such as plasma-bigscreen * or mycroft mark2 OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider Further reading: plasma-bigscreen GUI clients How do I run OVOS behind a proxy? Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy . If you plan to use OVOS behind a proxy, then you will need to do an additional configuration step. NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic OVOS will be using. We use https traffic on port 443 , primarily for accessing ReST-based APIs. Using OVOS behind a proxy without authentication If you are using OVOS behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://proxy_hostname.com:proxy_port $ export https_port=http://proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\" Using OVOS behind an authenticated proxy If you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://user:password@proxy_hostname.com:proxy_port $ export https_port=http://user:password@proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"F.A.Q."},{"location":"faq/#frequently-asked-questions","text":"What is OVOS? How did OVOS start? Who is behind OVOS? What is the relationship between OVOS and Mycroft? How does OVOS make money? Where is your website? Does OVOS have any default skills? Does OVOS work offline? Does OVOS depend on any servers? How many voices does OVOS support? Can I change the wake word? Can OVOS run without a wake word? How fast can OVOS respond? What is OPM? What is PHAL? What is OCP? What is OVOS-shell? How do I run OVOS behind a proxy?","title":"Frequently Asked Questions"},{"location":"faq/#what-is-ovos","text":"OVOS aims to be a full operating system that is free and open source. The Open Voice Operating System consists of OVOS packages (programs specifically released by the OVOS Project) as well as free software released by third parties such as skills and plugins. OVOS makes it possible to voice enable technology without software that would trample your freedom. Historically OVOS has been used to refer to several things, the team, the github organization and the reference buildroot implementation","title":"What is OVOS?"},{"location":"faq/#how-did-ovos-start","text":"OVOS started as MycroftOS, you can find the original mycroft forums thread here . Over time more mycroft community members joined the project, and it was renamed to OpenVoiceOS to avoid trademark issues. Initially OVOS was focused on bundling mycroft-core and on creating only companion software, but due to contributions not being accepted upstream we now maintain an enhanced reference fork of mycroft-core with extra functionality, while keeping all companion software mycroft-core (dev branch) compatible You can think of OVOS as the unsanctioned \"Mycroft Community Edition\"","title":"How did OVOS start?"},{"location":"faq/#who-is-behind-ovos","text":"Everyone in the OVOS team is a long term mycroft community member and has experience working with the mycroft code base Meet the team: Peter Steenbergen - mycroft community developer since 2018, founder of MycroftOS project Casimiro Ferreira - mycroft community developer since 2017, co-founder of HelloChatterbox Aditya Mehra - mycroft community developer since 2016, mycroft-gui lead developer Daniel McKnight - community developer since 2017, NeonGecko lead developer Parker Seaman - mycroft enthusiast since 2018 Chance - mycroft community developer since 2019, ex-maintainer of lingua_franca currently taking a break, he will be back!","title":"Who is behind OVOS?"},{"location":"faq/#what-is-the-relationship-between-ovos-and-mycroft","text":"Both projects are fully independent, initially OVOS was focused on wrapping mycroft-core with a minimal OS, but as both projects matured, ovos-core was created to include extra functionality and make OVOS development faster and more efficient. OVOS has been committed to keeping our components compatible with Mycroft and many of our changes are submitted to Mycroft to include in their projects at their discretion.","title":"What is the relationship between OVOS and Mycroft?"},{"location":"faq/#how-does-ovos-make-money","text":"We don't, OVOS is a volunteer project with no source of income or business model However, we want to acknowledge Blue Systems and NeonGeckoCom , a lot of the work in OVOS is done on paid company time from these projects","title":"How does OVOS make money?"},{"location":"faq/#where-is-your-website","text":"website - openvoiceos.com chat - matrix forums - github discussions","title":"Where is your website?"},{"location":"faq/#does-ovos-have-any-default-skills","text":"We provide essential skills and those are bundled in all our reference images. ovos-core does not manage your skills, unlike mycroft it won't install or update anything by itself. if you installed ovos-core manually you also need to install skills manually","title":"Does OVOS have any default skills?"},{"location":"faq/#does-ovos-work-offline","text":"By default ovos-core does not require a backend internet server to operate. Some skills can be accessed (via command line) entirely offline. The default speech-to-text (STT) engine currently requires an internet connection, though some self-hosted, offline options are available. Individual skills and plugins may require internet, and most of the time you will want to use those.","title":"Does OVOS work offline?"},{"location":"faq/#does-ovos-depend-on-any-servers","text":"no! you can integrate ovos-core with selene or personal backend but that is fully optional we provide some microservices for some of our skills, but you can also use your own api keys","title":"Does OVOS depend on any servers?"},{"location":"faq/#how-many-voices-does-ovos-support","text":"hundreds! nearly everything in OVOS is modular and configurable, that includes Text To Speech. Voices depend on language and the plugins you have installed, you can find a non-exhaustive list of plugins in the ovos plugins awesome list","title":"How many voices does OVOS support?"},{"location":"faq/#can-i-change-the-wake-word","text":"yes, ovos-core supports several wake word plugins . Additionally, OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word","title":"Can I change the wake word?"},{"location":"faq/#can-ovos-run-without-a-wake-word","text":"mostly yes, depending on exactly what you mean by this question OVOS can run without any wake word configured, in this case you will only be able to interact via CLI or button press, best for privacy, not so great for a smart speaker ovos-core also provides a couple experimental settings, if you enable continuous listening then VAD will be used to detect speech and no wake word is needed, just speak to mycroft and it should answer! However, this setting is experimental for a reason, you may find that mycroft answers your TV or even tries to answer itself if your hardware does not have AEC Another experimental setting is hybrid mode, with hybrid mode you can ask follow-up questions, up to 45 seconds after the last mycroft interaction, if you do not interact with mycroft it will go back to waiting for a wake word","title":"Can OVOS run without a wake word?"},{"location":"faq/#how-fast-can-ovos-respond","text":"By default, to answer a request: Detects the wake word Records 3 - 10 seconds of audio Transcribes the audio and returns the text transcription , either locally or remotely, depending on the speech-to-text (STT) engine in use Parses the text to understand the intent Sends the text to the intent handler with the highest confidence Allows the Skill to perform some action and provide the text to be spoken Synthesizes audio from the given text, either locally or remotely, depending on the text-to-speech (TTS) engine in use Plays the synthesized spoken audio. Through this process there are a number of factors that can affect the perceived speed of responses: System resources - more processing power and memory never hurts! Network latency - depending on configured plugins, network latency and connection speed can play a significant role in slowing down response times. Streaming STT - we have been experimenting with the use of streaming services. This transcribes audio as it's received rather than waiting for the entire utterance to be finished and sending the resulting audio file to a server to be processed in its entirety. It is possible to switch to a streaming STT service. See STT Plugins for a list of options available. Dialog structure - a long sentence will always take more time to synthesize than a short one. Skill developers can help provide quicker response times by considering the structure of their dialog and breaking that dialog up. TTS Caching - synthesized audio is cached meaning common recently generated phrases don't need to be generated, they can be returned immediately.","title":"How fast can OVOS respond?"},{"location":"faq/#what-is-opm","text":"OPM is the OVOS Plugin Manager , this base package provides arbitrary plugins to the ovos ecosystem OPM plugins import their base classes from OPM making them portable and independent from core, plugins can be used in your standalone projects By using OPM you can ensure a standard interface to plugins and easily make them configurable in your project, plugin code and example configurations are mapped to a string via python entrypoints in setup.py Some projects using OPM are ovos-core , hivemind-voice-sat , ovos-personal-backend , ovos-stt-server and ovos-tts-server","title":"What is OPM?"},{"location":"faq/#what-is-phal","text":"PHAL is our Platform/Hardware Abstraction Layer, it completely replaces the concept of hardcoded \"enclosure\" from mycroft-core Any number of plugins providing functionality can be loaded and validated at runtime, plugins can be system integrations to handle things like reboot and shutdown, or hardware drivers such as mycroft mark2 plugin PHAL plugins can perform actions such as hardware detection before loading, eg, the mark2 plugin will not load if it does not detect the sj201 hat. This makes plugins safe to install and bundle by default in our base images","title":"What is PHAL?"},{"location":"faq/#what-is-ocp","text":"OCP stands for OpenVoiceOS Common Play, it is a full fledged media player OCP is a OVOSAbstractApplication , this means it is a standalone but native OVOS application with full voice integration OCP differs from mycroft-core in several aspects: Can run standalone, only needs a bus connection OCP provides its own intents as if it was a skill OCP provides its own GUI as if it was a skill mycroft-core CommonPlay skill framework is disabled when OCP loads OCP skills have a dedicated MycroftSkill class and decorators in ovos-workshop OCP skills act as media providers, they do not (usually) handle playback mycroft-core CommonPlay skills have an imperfect compatibility layer and are given lower priority over OCP skills OCP handles several kinds of playback, including video OCP has a sub-intent parser for matching requested media types AudioService becomes a subsystem for OCP OCP also has AudioService plugin component introducing a compatibility layer for skills using \"old style audioservice api\" OCP integrates with MPRIS, it can be controlled from external apps, e.g. KdeConnect in your phone OCP manages external MPRIS enabled players, you can voice control 3rd party apps without writing a skill for it via OCP","title":"What is OCP?"},{"location":"faq/#what-is-ovos-shell","text":"The gui service in ovos-core will expose a websocket to the GUI client following the protocol outlined here The GUI library which implements the protocol lives in the mycroft-gui repository, The repository also hosts a development client for skill developers wanting to develop on the desktop. OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images, other distributions may offer alternative implementations such as plasma-bigscreen * or mycroft mark2 OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider Further reading: plasma-bigscreen GUI clients","title":"What is OVOS-shell?"},{"location":"faq/#how-do-i-run-ovos-behind-a-proxy","text":"Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy . If you plan to use OVOS behind a proxy, then you will need to do an additional configuration step. NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic OVOS will be using. We use https traffic on port 443 , primarily for accessing ReST-based APIs.","title":"How do I run OVOS behind a proxy?"},{"location":"faq/#using-ovos-behind-a-proxy-without-authentication","text":"If you are using OVOS behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://proxy_hostname.com:proxy_port $ export https_port=http://proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"Using OVOS behind a proxy without authentication"},{"location":"faq/#using-ovos-behind-an-authenticated-proxy","text":"If you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI). $ export http_proxy=http://user:password@proxy_hostname.com:proxy_port $ export https_port=http://user:password@proxy_hostname.com:proxy_port $ export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1\"","title":"Using OVOS behind an authenticated proxy"},{"location":"friends/","text":"OVOS Friends OpenVoiceOS is part of a larger ecosystem of FOSS voice technology, we work closely with the following projects HiveMind HiveMind is a community-developed superset or extension of OpenVoiceOS With HiveMind, you can extend one (or more, but usually just one!) instance of Mycroft to as many devices as you want, including devices that can't ordinarily run Mycroft! HiveMind's developers have successfully connected to Mycroft from a PinePhone, a 2009 MacBook, and a Raspberry Pi 0, among other devices. Mycroft itself usually runs on our desktop computers or our home servers, but you can use any Mycroft-branded device, or OpenVoiceOS, as your central unit. You find the website here and the source code here Plasma Bigscreen Plasma Bigscreen integrates and uses OpenVoiceOS as voice framework stack to serve voice queries and voice applications (skills with a homescreen), one can easily enable mycroft / ovos integration in the bigscreen launcher by installing ovos core and required services and enabling the integration switch in the bigscreen KCM You find the website here and the source code here NeonGecko Neon was one of the first projects ever to adopt ovos-core as a library to build their own voice assistant, Neon works closely together with OVOS and both projects are mostly compatible You find the website here and the source code here Mycroft Mycroft AI started it all, it was one of the first ever FOSS voice assistants and is the project OVOS descends from. Most applications made for mycroft will work in OVOS and vice-versa You find the website here and the source code here Secret Sauce AI Secret Sauce AI is a coordinated community of tech minded AI enthusiasts working together on projects to identify blockers and improve the basic open source tools and pipeline components in the AI (voice) assistant pipeline (wakeword, ASR, NLU, NLG, TTS). The focus is mostly geared toward deployment on edge devices and self-hosted solutions. This is not a voice assistant project in and of itself, rather Secret Sauce AI helps AI (voice) assistant projects come together as individuals and solve basic problems faced by the entire community. You find the website here and the source code here","title":"Friends"},{"location":"friends/#ovos-friends","text":"OpenVoiceOS is part of a larger ecosystem of FOSS voice technology, we work closely with the following projects","title":"OVOS Friends"},{"location":"friends/#hivemind","text":"HiveMind is a community-developed superset or extension of OpenVoiceOS With HiveMind, you can extend one (or more, but usually just one!) instance of Mycroft to as many devices as you want, including devices that can't ordinarily run Mycroft! HiveMind's developers have successfully connected to Mycroft from a PinePhone, a 2009 MacBook, and a Raspberry Pi 0, among other devices. Mycroft itself usually runs on our desktop computers or our home servers, but you can use any Mycroft-branded device, or OpenVoiceOS, as your central unit. You find the website here and the source code here","title":" HiveMind"},{"location":"friends/#plasma-bigscreen","text":"Plasma Bigscreen integrates and uses OpenVoiceOS as voice framework stack to serve voice queries and voice applications (skills with a homescreen), one can easily enable mycroft / ovos integration in the bigscreen launcher by installing ovos core and required services and enabling the integration switch in the bigscreen KCM You find the website here and the source code here","title":" Plasma Bigscreen"},{"location":"friends/#neongecko","text":"Neon was one of the first projects ever to adopt ovos-core as a library to build their own voice assistant, Neon works closely together with OVOS and both projects are mostly compatible You find the website here and the source code here","title":" NeonGecko"},{"location":"friends/#mycroft","text":"Mycroft AI started it all, it was one of the first ever FOSS voice assistants and is the project OVOS descends from. Most applications made for mycroft will work in OVOS and vice-versa You find the website here and the source code here","title":" Mycroft"},{"location":"friends/#secret-sauce-ai","text":"Secret Sauce AI is a coordinated community of tech minded AI enthusiasts working together on projects to identify blockers and improve the basic open source tools and pipeline components in the AI (voice) assistant pipeline (wakeword, ASR, NLU, NLG, TTS). The focus is mostly geared toward deployment on edge devices and self-hosted solutions. This is not a voice assistant project in and of itself, rather Secret Sauce AI helps AI (voice) assistant projects come together as individuals and solve basic problems faced by the entire community. You find the website here and the source code here","title":" Secret Sauce AI"},{"location":"g2p_plugins/","text":"Grapheme to Phoneme Plugins Grapheme to Phoneme is the process of converting text into a set of \"sound units\" called phonemes These plugins are used to auto generate mouth movements / visemes in the TTS stage, they can also be used to help configuring wake words or to facilitate training of TTS systems These plugins can provide phonemes either in ARPA or IPA alphabets, an automatic conversion will happen behind the scenes when needed Mouth movements are generated via a mapping of ARPA to VISEMES, Visemes are predefined mouth positions, timing per phonemes will default to 0.4 seconds if the plugin does not report a duration Mapping based on Jeffers phoneme to viseme map, seen in table 1 , partially based on the \"12 mouth shapes visuals seen here List of G2P plugins Plugin Type neon-g2p-cmudict-plugin ARPA neon-g2p-phoneme-guesser-plugin ARPA neon-g2p-mimic-plugin ARPA neon-g2p-mimic2-plugin ARPA neon-g2p-espeak-plugin IPA neon-g2p-gruut-plugin IPA Standalone Usage All G2P plugins can be used as follows utterance = \"hello world\" word = \"hello\" lang=\"en-us\" plug = G2pPlugin() # convert a word into a list of phonemes phones = plug.get_ipa(word, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a'] phones = plug.get_arpa(word, lang) assert phones == ['HH', 'AH', 'L', 'OW'] # convert a utterance into a list of phonemes phones = plug.utterance2arpa(utterance, lang) assert phones == ['HH', 'AH', 'L', 'OW', '.', 'W', 'ER', 'L', 'D'] phones = plug.utterance2ipa(utterance, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a', '.', 'w', '\u025d', 'l', 'd'] # convert a utterance into a list of viseme, duration pairs visemes = plug.utterance2visemes(utterance, lang) assert visemes == [('0', 0.0775), ('0', 0.155), ('3', 0.2325), ('2', 0.31), ('2', 0.434), ('2', 0.558), ('3', 0.682), ('3', 0.806)] Plugin Template from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyARPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_arpa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): words = utterance.split() phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes] If your plugin uses IPA instead of ARPA simply replace get_arpa with get_ipa from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyIPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_ipa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): # auto converted to arpa if ipa is implemented phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes]","title":"Grapheme to Phoneme"},{"location":"g2p_plugins/#grapheme-to-phoneme-plugins","text":"Grapheme to Phoneme is the process of converting text into a set of \"sound units\" called phonemes These plugins are used to auto generate mouth movements / visemes in the TTS stage, they can also be used to help configuring wake words or to facilitate training of TTS systems These plugins can provide phonemes either in ARPA or IPA alphabets, an automatic conversion will happen behind the scenes when needed Mouth movements are generated via a mapping of ARPA to VISEMES, Visemes are predefined mouth positions, timing per phonemes will default to 0.4 seconds if the plugin does not report a duration Mapping based on Jeffers phoneme to viseme map, seen in table 1 , partially based on the \"12 mouth shapes visuals seen here","title":"Grapheme to Phoneme Plugins"},{"location":"g2p_plugins/#list-of-g2p-plugins","text":"Plugin Type neon-g2p-cmudict-plugin ARPA neon-g2p-phoneme-guesser-plugin ARPA neon-g2p-mimic-plugin ARPA neon-g2p-mimic2-plugin ARPA neon-g2p-espeak-plugin IPA neon-g2p-gruut-plugin IPA","title":"List of G2P plugins"},{"location":"g2p_plugins/#standalone-usage","text":"All G2P plugins can be used as follows utterance = \"hello world\" word = \"hello\" lang=\"en-us\" plug = G2pPlugin() # convert a word into a list of phonemes phones = plug.get_ipa(word, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a'] phones = plug.get_arpa(word, lang) assert phones == ['HH', 'AH', 'L', 'OW'] # convert a utterance into a list of phonemes phones = plug.utterance2arpa(utterance, lang) assert phones == ['HH', 'AH', 'L', 'OW', '.', 'W', 'ER', 'L', 'D'] phones = plug.utterance2ipa(utterance, lang) assert phones == ['h', '\u028c', 'l', 'o\u028a', '.', 'w', '\u025d', 'l', 'd'] # convert a utterance into a list of viseme, duration pairs visemes = plug.utterance2visemes(utterance, lang) assert visemes == [('0', 0.0775), ('0', 0.155), ('3', 0.2325), ('2', 0.31), ('2', 0.434), ('2', 0.558), ('3', 0.682), ('3', 0.806)]","title":"Standalone Usage"},{"location":"g2p_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyARPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_arpa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): words = utterance.split() phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes] If your plugin uses IPA instead of ARPA simply replace get_arpa with get_ipa from ovos_plugin_manager.templates.g2p import Grapheme2PhonemePlugin from ovos_utils.lang.visimes import VISIMES # base plugin class class MyIPAG2PPlugin(Grapheme2PhonemePlugin): def __init__(self, config=None): self.config = config or {} def get_ipa(self, word, lang, ignore_oov=False): phones = [] # TODO implement return phones def get_durations(self, utterance, lang=\"en\", default_dur=0.4): # auto converted to arpa if ipa is implemented phones = [self.get_arpa(w, lang) for w in utterance.split()] dur = default_dur # TODO this is plugin specific return [(pho, dur) for pho in phones] def utterance2visemes(self, utterance, lang=\"en\", default_dur=0.4): phonemes = self.get_durations(utterance, lang, default_dur) return [(VISIMES.get(pho[0].lower(), '4'), float(pho[1])) for pho in phonemes]","title":"Plugin Template"},{"location":"glossary/","text":"Glossary The OpenVoiceOS Project - All the repositories under OpenVoiceOS organization The OpenVoiceOS Team - The team behind OVOS ovos-core - the central repository where the voice assistant \"brain\" is developed OVOS - all of the above! utterance - command, question, or query from a user (eg 'turn on the kitchen lights') intent - when an utterance is classified for its action and entities (eg 'turn on the kitchen lights' -> skill: home assistant, action: turn on/off, entity: kitchen lights) STT - speech to text, also known as ASR, automated speech recognition, the process of converting audio into words wakeword - a specific word or phrase trained used to activate the STT (eg 'hey mycroft') TTS - text to speech, the process of generating the audio with the responses","title":"Glossary"},{"location":"glossary/#glossary","text":"The OpenVoiceOS Project - All the repositories under OpenVoiceOS organization The OpenVoiceOS Team - The team behind OVOS ovos-core - the central repository where the voice assistant \"brain\" is developed OVOS - all of the above! utterance - command, question, or query from a user (eg 'turn on the kitchen lights') intent - when an utterance is classified for its action and entities (eg 'turn on the kitchen lights' -> skill: home assistant, action: turn on/off, entity: kitchen lights) STT - speech to text, also known as ASR, automated speech recognition, the process of converting audio into words wakeword - a specific word or phrase trained used to activate the STT (eg 'hey mycroft') TTS - text to speech, the process of generating the audio with the responses","title":"Glossary"},{"location":"gui_extensions/","text":"GUI Extensions OVOS Core supports a GUI Extension framework which allows the GUI service to incorporate additional behaviour for a specific platform. GUI Extensions currently supported: Smartspeaker Extension This extension is responsible for managing the smartspeaker GUI interface behaviour, it supports homescreens and homescreen management. Enabling the smartspeaker GUI extension: \"gui\": { \"extension\": \"smartspeaker\", \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" } Bigscreen Extension This extension is responsible for managing the plasma bigscreen GUI interface behaviour, it supports window management and window behaviour control on specific window managers like Kwin. Enabling the Bigscreen GUI extension: \"gui\": { \"extension\": \"bigscreen\" } Mobile Extension This extension is responsible for managing the mobile GUI interface behaviour, it supports homescreens and additionally adds support for global page back navigation. Enabling the Mobile GUI extension: \"gui\": { \"extension\": \"mobile\", \"idle_display_skill\": \"skill-android-homescreen.openvoiceos\", } Generic Extension This extension provides a generic GUI interface and does not add any additional behaviour, it optionally supports homescreens if the platform or user manually enables it. This extension is enabled by default when no other extension is specified. \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }","title":"Extensions"},{"location":"gui_extensions/#gui-extensions","text":"OVOS Core supports a GUI Extension framework which allows the GUI service to incorporate additional behaviour for a specific platform. GUI Extensions currently supported:","title":"GUI Extensions"},{"location":"gui_extensions/#smartspeaker-extension","text":"This extension is responsible for managing the smartspeaker GUI interface behaviour, it supports homescreens and homescreen management. Enabling the smartspeaker GUI extension: \"gui\": { \"extension\": \"smartspeaker\", \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\" }","title":"Smartspeaker Extension"},{"location":"gui_extensions/#bigscreen-extension","text":"This extension is responsible for managing the plasma bigscreen GUI interface behaviour, it supports window management and window behaviour control on specific window managers like Kwin. Enabling the Bigscreen GUI extension: \"gui\": { \"extension\": \"bigscreen\" }","title":"Bigscreen Extension"},{"location":"gui_extensions/#mobile-extension","text":"This extension is responsible for managing the mobile GUI interface behaviour, it supports homescreens and additionally adds support for global page back navigation. Enabling the Mobile GUI extension: \"gui\": { \"extension\": \"mobile\", \"idle_display_skill\": \"skill-android-homescreen.openvoiceos\", }","title":"Mobile Extension"},{"location":"gui_extensions/#generic-extension","text":"This extension provides a generic GUI interface and does not add any additional behaviour, it optionally supports homescreens if the platform or user manually enables it. This extension is enabled by default when no other extension is specified. \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }","title":"Generic Extension"},{"location":"gui_intro/","text":"Introduction OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. The display interaction technology is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the Mycroft GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences. Framework Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. Introduction to QML QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements. A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami","title":"Introduction"},{"location":"gui_intro/#introduction","text":"OVOS devices with displays provide skill developers the opportunity to create skills that can be empowered by both voice and screen interaction. The display interaction technology is based on the QML user interface markup language that gives you complete freedom to create in-depth innovative interactions without boundaries or provide you with simple templates within the Mycroft GUI framework that allow minimalistic display of text and images based on your skill development specifics and preferences.","title":"Introduction"},{"location":"gui_intro/#framework","text":"Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt.","title":"Framework"},{"location":"gui_intro/#introduction-to-qml","text":"QML user interface markup language is a declarative language built on top of Qt's existing strengths designed to describe the user interface of a program: both what it looks like, and how it behaves. QML provides modules that consist of sophisticated set of graphical and behavioral building elements. A collection of resources to familiarize you with QML and Kirigami Framework. Introduction to QML Introduction to Kirigami","title":"Introduction to QML"},{"location":"gui_protocol/","text":"Protocol The gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined here The transport protocol works between gui service and the gui clients, mycroft does not directly use the protocol but instead communicates with the gui service via the standard mycroft bus OVOS images are powered by ovos-shell , the client side implementation of the gui protocol The GUI library which implements the protocol lives in the mycroft-gui repository.","title":"Protocol"},{"location":"gui_protocol/#protocol","text":"The gui service in ovos-core will expose a websocket to the GUI clients following the protocol outlined here The transport protocol works between gui service and the gui clients, mycroft does not directly use the protocol but instead communicates with the gui service via the standard mycroft bus OVOS images are powered by ovos-shell , the client side implementation of the gui protocol The GUI library which implements the protocol lives in the mycroft-gui repository.","title":"Protocol"},{"location":"gui_service/","text":"GUI Service OVOS uses the standard mycroft-gui framework, you can find the official documentation here The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core. You can find indepth documentation in the dedicated GUI section of these docs Configuration The gui service has a few sections in mycroft.conf \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }, \"gui_websocket\": { \"host\": \"0.0.0.0\", \"base_port\": 18181, \"route\": \"/gui\", \"ssl\": false },","title":"GUI"},{"location":"gui_service/#gui-service","text":"OVOS uses the standard mycroft-gui framework, you can find the official documentation here The GUI service provides a websocket for gui clients to connect to, it is responsible for implementing the gui protocol under ovos-core. You can find indepth documentation in the dedicated GUI section of these docs","title":"GUI Service"},{"location":"gui_service/#configuration","text":"The gui service has a few sections in mycroft.conf \"gui\": { \"idle_display_skill\": \"skill-ovos-homescreen.openvoiceos\", \"extension\": \"generic\", \"generic\": { \"homescreen_supported\": false } }, \"gui_websocket\": { \"host\": \"0.0.0.0\", \"base_port\": 18181, \"route\": \"/gui\", \"ssl\": false },","title":"Configuration"},{"location":"guidelines/","text":"Voice User Interface Design Guidelines Through these guidelines you will learn how to use principles of Voice User Interface Design to build more effective skills. These tools will help define and validate the features of the skill before diving deep into development. This guide will cover some methods to use that can help plan, prototype and test your skill during the early design stages. The full original guide can be found over at the mycroft documentation Interactions Intents Let's start with an example. A user in Melbourne, Australia might want to know about the weather. To ask for this information, they might say: \"Hey Mycroft, what's today's weather like?\" \"Hey Mycroft, what's the weather like in Melbourne?\" \"Hey Mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. It is up us as Skill creators to teach OVOS the variety of ways that a user might express the same intent. This is a key part of the design process. It is the key difference between a Skill that kind of works if you know what to say, and a Skill that feels intuitive and natural to talk to. This is handled by an intent parser whose job it is to learn from your Skill what intents it can handle, and extract from the user's speech and key information that might be useful for your Skill. In this case it might include the specified date and location. Statements and Prompts You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. For example a weather forecast like this would be considered a statement: Today\u2019s forecast is sunny with a high of 60 and a low of 45. Statements For a lot of skills the conversation might end with a simple statement from OVOS, and no further action is necessary. Try to imagine what the user is trying to accomplish, if a simple statement gets the job done there is no reason to keep the conversation rolling, and in fact a follow-up might annoy the user with unnecessary interaction. Prompts It may be tempting to always give users specific instructions like traditional automated phones systems (Interactive Voice Response). Many phone systems are notorious for being too verbose and difficult to follow. With OVOS we\u2019re trying to break that mold and make the interaction natural. If you follow the phone system method you may be giving the user the exact phrasing to say, but you\u2019re also taking up valuable time and training them to think the system is very narrow in capability. In the event that the user does give a response that your skill can not handle, create follow-up prompts that steer the user back on track. Remember, there are no errors in a cooperative conversation. Avoid Speaker Mycroft How many people are playing? For example, you can say 2 players. Better Speaker Mycroft How many players? User My Brother and Myself Mycroft I\u2019m sorry, what was the number of players? User Two In the first example the user is told explicitly what they can say, but the prompt is unnatural, we don\u2019t typically suggest responses to a conversation partner in real life. These long-winded prompts can become tiresome if they are used repeatedly throughout the skill. Remember the phone system example, typically poorly designed automated phone systems inundate the user with many options and additional instructions at every step of the interaction. In the second example we see a better prompt, although the user gives a response that is easy for a human to understand it is more difficult to design a skill to understand. Instead, the skill designer can apply a re-prompt strategy that steers the user back on track and doesn't require them to start the interaction over. In this case the re-prompt changes the wording slightly to make it clear that the user needs to say a number. The next time the user interacts with this Skill, they will likely say a number in their first interaction. Determining whether to respond to the user with a statement or a prompt can be a bit tricky. When the user is somewhat vague it is tempting to assume the user\u2019s intent in order to speed along the interaction. Sometimes this is the right approach and sometimes it is not. If you are very confident in the user\u2019s input then it is alright to assume, and respond with a statement. For example in the Wikipedia Skill the Wikipedia API is used to confidently select the best response. Wikipedia Skill Speaker User Tell me about Abraham Lincoln Mycroft Abraham Lincoln was an American statesman and lawyer who served as the 16th president of the UnitedStates from 1861 to 1865. Lincoln led the nation through its greatest moral, constitutional, and political crisis in the American Civil War . In contrast, let\u2019s look at an example of where a follow-up prompt is a better approach. You\u2019re working on a skill that gives important information about birds, such as wingspan, laden and unladen airspeed, etc.. Avoid Speaker **** User what is the airspeed velocity of an unladen swallow? Mycroft The unladen airspeed of the european swallow is 20.1 miles per hour. Better Speaker User What is the airspeed velocity of an unladen Swallow? Mycroft What do you mean, African or European Swallow? User European Swallow. Mycroft The unladen airspeed of the European Swallow is 20.1 miles per hour. In the first example Mycroft assumes the user is referring to the European Swallow, however there is nothing to give confidence to that assumption. In this case it\u2019s dangerous to assume the User meant the European Swallow since it is just as likely they meant the African Swallow. When there is no confidence in one answer versus another, OVOS should follow up with a prompt as demonstrated in the second example. Another approach to solving this problem is to offer the most likely answer and inform the user that there are other answers available with a statement. Confirmations Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit. Implicit Confirmation This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. For example Mycroft\u2019s Timer Skill uses implicit confirmation when creating new timers. Timer Skill Speaker User Hey Mycroft set a timer for 5 minutes Mycroft Alright, I\u2019m starting a 5 minute timer Avoid Speaker User Hey Mycroft, add bananas to my shopping list Mycroft done Better Speaker User Hey Mycroft, add bananas to my shopping list Mycroft Adding bananas Explicit Confirmation This type of confirmation requires an input from the user to verify everything is correct. Only use explicit confirmation when needed as it can be tedious for the user to always confirm their input. This strategy is best when there are multiple steps, and it would be difficult to fix a misstep without starting over, or when deleting or modifying an existing multistep input. Avoid Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft Ok, I\u2019ve set an alarm for 9:00 am tomorrow, is that correct? User Yes Mycroft 9:00 am alarm set Better Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft I've set an alarm for tomorrow at 9:00 am, 16 hours from now The first example requires the user to confirm the alarm. Since the interaction is quite short it isn\u2019t necessary to use explicit confirmation. Avoid Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I've created a meeting for Tomorrow Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Done Better Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Are you sure you want to delete your meeting tomorrow at 11:00 am? User Yes Both examples use explicit and implicit confirmation correctly until the end of the interaction when the user wants to cancel the event. Unlike setting a simple timer or an alarm creating this calendar event requires multiple steps and would require reasonable effort to recreate. There may also be significant consequences if the request was misinterpreted and the meeting was cancelled unintentionally. Conversations Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. For example, many personal navigation devices (GPS units) require users to input the street number, street name, city, and state separately, like this: Speaker Navigation Device What is the street number User 123 Navigation Device What is the street name User Main Street Navigation Device What is the city User Springfield Navigation Device Oregon This was a limitation of the technology, both the STT and intent parser were not capable of handling an entire address. With OVOS you do not need to separate the address into multiple inputs. It\u2019s also easy to think like a traditional GUI and translate what are typically separate fields into prompts for the user, but that will only slow down the interaction. The best solution is to allow the user to input information both ways, all at once, or in pieces. For example the Alarm skill allows the user to set a complicated alarm all in one go, or in steps. In Steps: Speaker User Hey Mycroft, set a recurring alarm Mycroft For what day? User weekdays Mycroft For what time? User 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays All at once: Speaker User Hey Mycroft, set a recurring alarm for weekdays at 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays Context Conversational context is a given when speaking to another person, but not always when interacting with a computer based system. Many skills are designed with only a quick back and forth interaction in mind, and can operate well without conversational context, but conversational context can be a powerful addition to a skill and can make the interaction much more life like. Here are some examples of how to use context in conversation. Speaker User Who is Shaquille O'Neal Mycroft Shaquille O'Neal is an American former professional basketball player who is a sports analyst on the television program Inside the NBA on TNT. He is considered one of the greatest players in National Basketball Association history User How tall is he? Mycroft seven foot 1 inches tall User when did he retire Mycroft 2011 Error Handling Inevitably, the user will say something that your skill can\u2019t handle. It\u2019s best not to think of these as errors on the part of the user, remember there aren\u2019t really errors in conversations. Conversations are naturally cooperative with both parties seeking to continue to a successful outcome. Keeping that in mind you should do your best to avoid Error Handling that is a dead-end. Avoid Speaker User Timer Mycroft I'm sorry I can't help you with that Better Speaker Use r Timer Mycroft A timer for how long? User 5 minutes In the first example OVOS does not give the user any options to finish the job they set out to do. Try to avoid situations where the user has to start the conversation over.","title":"Design Guidelines"},{"location":"guidelines/#voice-user-interface-design-guidelines","text":"Through these guidelines you will learn how to use principles of Voice User Interface Design to build more effective skills. These tools will help define and validate the features of the skill before diving deep into development. This guide will cover some methods to use that can help plan, prototype and test your skill during the early design stages. The full original guide can be found over at the mycroft documentation","title":"Voice User Interface Design Guidelines"},{"location":"guidelines/#interactions","text":"","title":"Interactions"},{"location":"guidelines/#intents","text":"Let's start with an example. A user in Melbourne, Australia might want to know about the weather. To ask for this information, they might say: \"Hey Mycroft, what's today's weather like?\" \"Hey Mycroft, what's the weather like in Melbourne?\" \"Hey Mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. It is up us as Skill creators to teach OVOS the variety of ways that a user might express the same intent. This is a key part of the design process. It is the key difference between a Skill that kind of works if you know what to say, and a Skill that feels intuitive and natural to talk to. This is handled by an intent parser whose job it is to learn from your Skill what intents it can handle, and extract from the user's speech and key information that might be useful for your Skill. In this case it might include the specified date and location.","title":"Intents"},{"location":"guidelines/#statements-and-prompts","text":"You can think of Prompts as questions and Statements as providing information to the user that does not need a follow-up response. For example a weather forecast like this would be considered a statement: Today\u2019s forecast is sunny with a high of 60 and a low of 45.","title":"Statements and Prompts"},{"location":"guidelines/#statements","text":"For a lot of skills the conversation might end with a simple statement from OVOS, and no further action is necessary. Try to imagine what the user is trying to accomplish, if a simple statement gets the job done there is no reason to keep the conversation rolling, and in fact a follow-up might annoy the user with unnecessary interaction.","title":"Statements"},{"location":"guidelines/#prompts","text":"It may be tempting to always give users specific instructions like traditional automated phones systems (Interactive Voice Response). Many phone systems are notorious for being too verbose and difficult to follow. With OVOS we\u2019re trying to break that mold and make the interaction natural. If you follow the phone system method you may be giving the user the exact phrasing to say, but you\u2019re also taking up valuable time and training them to think the system is very narrow in capability. In the event that the user does give a response that your skill can not handle, create follow-up prompts that steer the user back on track. Remember, there are no errors in a cooperative conversation. Avoid Speaker Mycroft How many people are playing? For example, you can say 2 players. Better Speaker Mycroft How many players? User My Brother and Myself Mycroft I\u2019m sorry, what was the number of players? User Two In the first example the user is told explicitly what they can say, but the prompt is unnatural, we don\u2019t typically suggest responses to a conversation partner in real life. These long-winded prompts can become tiresome if they are used repeatedly throughout the skill. Remember the phone system example, typically poorly designed automated phone systems inundate the user with many options and additional instructions at every step of the interaction. In the second example we see a better prompt, although the user gives a response that is easy for a human to understand it is more difficult to design a skill to understand. Instead, the skill designer can apply a re-prompt strategy that steers the user back on track and doesn't require them to start the interaction over. In this case the re-prompt changes the wording slightly to make it clear that the user needs to say a number. The next time the user interacts with this Skill, they will likely say a number in their first interaction. Determining whether to respond to the user with a statement or a prompt can be a bit tricky. When the user is somewhat vague it is tempting to assume the user\u2019s intent in order to speed along the interaction. Sometimes this is the right approach and sometimes it is not. If you are very confident in the user\u2019s input then it is alright to assume, and respond with a statement. For example in the Wikipedia Skill the Wikipedia API is used to confidently select the best response. Wikipedia Skill Speaker User Tell me about Abraham Lincoln Mycroft Abraham Lincoln was an American statesman and lawyer who served as the 16th president of the UnitedStates from 1861 to 1865. Lincoln led the nation through its greatest moral, constitutional, and political crisis in the American Civil War . In contrast, let\u2019s look at an example of where a follow-up prompt is a better approach. You\u2019re working on a skill that gives important information about birds, such as wingspan, laden and unladen airspeed, etc.. Avoid Speaker **** User what is the airspeed velocity of an unladen swallow? Mycroft The unladen airspeed of the european swallow is 20.1 miles per hour. Better Speaker User What is the airspeed velocity of an unladen Swallow? Mycroft What do you mean, African or European Swallow? User European Swallow. Mycroft The unladen airspeed of the European Swallow is 20.1 miles per hour. In the first example Mycroft assumes the user is referring to the European Swallow, however there is nothing to give confidence to that assumption. In this case it\u2019s dangerous to assume the User meant the European Swallow since it is just as likely they meant the African Swallow. When there is no confidence in one answer versus another, OVOS should follow up with a prompt as demonstrated in the second example. Another approach to solving this problem is to offer the most likely answer and inform the user that there are other answers available with a statement.","title":"Prompts"},{"location":"guidelines/#confirmations","text":"Confirmation approaches can also be defined by Statements or Prompts , but when we talk about them in the context of confirmations we call them Implicit and Explicit.","title":"Confirmations"},{"location":"guidelines/#implicit-confirmation","text":"This type of confirmation is also a statement. The idea is to parrot the information back to the user to confirm that it was correct, but not require additional input from the user. The implicit confirmation can be used in a majority of situations. For example Mycroft\u2019s Timer Skill uses implicit confirmation when creating new timers. Timer Skill Speaker User Hey Mycroft set a timer for 5 minutes Mycroft Alright, I\u2019m starting a 5 minute timer Avoid Speaker User Hey Mycroft, add bananas to my shopping list Mycroft done Better Speaker User Hey Mycroft, add bananas to my shopping list Mycroft Adding bananas","title":"Implicit Confirmation"},{"location":"guidelines/#explicit-confirmation","text":"This type of confirmation requires an input from the user to verify everything is correct. Only use explicit confirmation when needed as it can be tedious for the user to always confirm their input. This strategy is best when there are multiple steps, and it would be difficult to fix a misstep without starting over, or when deleting or modifying an existing multistep input. Avoid Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft Ok, I\u2019ve set an alarm for 9:00 am tomorrow, is that correct? User Yes Mycroft 9:00 am alarm set Better Speaker User Hey Mycroft, set an alarm for 9:00 am tomorrow Mycroft I've set an alarm for tomorrow at 9:00 am, 16 hours from now The first example requires the user to confirm the alarm. Since the interaction is quite short it isn\u2019t necessary to use explicit confirmation. Avoid Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I've created a meeting for Tomorrow Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Done Better Speaker User Hey Mycroft, set a calendar event for a meeting with Chris Stevens Mycroft When is the event? User Tomorrow at 11:00 am Mycroft should I send an invite to Chris Stevens? User Yes Mycroft Alright, I\u2019ve created a meeting for Tomorrow at 11:00 am with Chris Stevens User Cancel my 11:00 am meeting tomorrow Mycroft Are you sure you want to delete your meeting tomorrow at 11:00 am? User Yes Both examples use explicit and implicit confirmation correctly until the end of the interaction when the user wants to cancel the event. Unlike setting a simple timer or an alarm creating this calendar event requires multiple steps and would require reasonable effort to recreate. There may also be significant consequences if the request was misinterpreted and the meeting was cancelled unintentionally.","title":"Explicit Confirmation"},{"location":"guidelines/#conversations","text":"Any time the user needs to input a lot of information or the user needs to sort through a variety of options a conversation will be needed. Users may be used to systems that require them to separate input into different chunks. For example, many personal navigation devices (GPS units) require users to input the street number, street name, city, and state separately, like this: Speaker Navigation Device What is the street number User 123 Navigation Device What is the street name User Main Street Navigation Device What is the city User Springfield Navigation Device Oregon This was a limitation of the technology, both the STT and intent parser were not capable of handling an entire address. With OVOS you do not need to separate the address into multiple inputs. It\u2019s also easy to think like a traditional GUI and translate what are typically separate fields into prompts for the user, but that will only slow down the interaction. The best solution is to allow the user to input information both ways, all at once, or in pieces. For example the Alarm skill allows the user to set a complicated alarm all in one go, or in steps. In Steps: Speaker User Hey Mycroft, set a recurring alarm Mycroft For what day? User weekdays Mycroft For what time? User 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays All at once: Speaker User Hey Mycroft, set a recurring alarm for weekdays at 7 am Mycroft I've set a recurring Alarm for 7 am on weekdays","title":"Conversations"},{"location":"guidelines/#context","text":"Conversational context is a given when speaking to another person, but not always when interacting with a computer based system. Many skills are designed with only a quick back and forth interaction in mind, and can operate well without conversational context, but conversational context can be a powerful addition to a skill and can make the interaction much more life like. Here are some examples of how to use context in conversation. Speaker User Who is Shaquille O'Neal Mycroft Shaquille O'Neal is an American former professional basketball player who is a sports analyst on the television program Inside the NBA on TNT. He is considered one of the greatest players in National Basketball Association history User How tall is he? Mycroft seven foot 1 inches tall User when did he retire Mycroft 2011","title":"Context"},{"location":"guidelines/#error-handling","text":"Inevitably, the user will say something that your skill can\u2019t handle. It\u2019s best not to think of these as errors on the part of the user, remember there aren\u2019t really errors in conversations. Conversations are naturally cooperative with both parties seeking to continue to a successful outcome. Keeping that in mind you should do your best to avoid Error Handling that is a dead-end. Avoid Speaker User Timer Mycroft I'm sorry I can't help you with that Better Speaker Use r Timer Mycroft A timer for how long? User 5 minutes In the first example OVOS does not give the user any options to finish the job they set out to do. Try to avoid situations where the user has to start the conversation over.","title":"Error Handling"},{"location":"homescreen/","text":"OpenVoiceOS Home Screen The home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections. Features Night Mode Feature The Night Mode feature lets you quickly switch your home screen into a dark standby clock, reducing the amount of light emitted by your device. This is especially useful if you are using your device in a dark room or at night. You can enable the night mode feature by tapping on the left edge pill button on the home screen. Quick Actions Dashboard The Quick Actions Dashboard provides you with a card-based interface to quickly access and add your most used action. The Quick Actions dashboard comes with a variety of pre-defined actions like the ability to quickly add a new alarm, start a new timer or add a new note. You can also add your own custom actions to the dashboard by tapping on the plus button in the top right corner of the dashboard. The Quick Actions dashboard is accessible by tapping on the right edge pill button on the home screen. Application Launcher OpenVoiceOS comes with support for dedicated voice applications. Voice Applications can be dedicated skills or PHAL plugins, providing their own dedicated user interface. The application launcher will show you a list of all available voice applications. You can access the application launcher by tapping on the center pill button on the bottom of the home screen. Wallpapers The home screen supports custom wallpapers and comes with a bunch of wallpapers to choose from. You can easily change your custom wallpaper by swiping from right to left on the home screen. Widgets Notifications Widget The notifications widget provides you with a quick overview of all your notifications. The notifications bell icon will be displayed in the top left corner of the home screen. You can access the notifications overview by tapping on the bell icon when it is displayed. Timer Widget The timer widget is displayed in top left corner after the notifications bell icon. It will show up when you have an active timer running. Clicking on the timer widget will open the timers overview. Alarm Widget The alarm widget is displayed in top left corner after the timer widget. It will show up when you have an active alarm set. Clicking on the alarm widget will open the alarms overview. Media Player Widget The media player widget is displayed in the bottom of the home screen, It replaces the examples widget when a media player is active. The media player widget will show you the currently playing media and provide you with a quick way to pause, resume or skip the current media. You can also quickly access the media player by tapping the quick display media player button on the right side of the media player widget. Configuration Settings The homescreen has several customizations available. This is sample settings.json file with all of the options explained { \"__mycroft_skill_firstrun\": false, \"weather_skill\": \"skill-weather.openvoiceos\", \"datetime_skill\": \"skill-date-time.mycroftai\", \"examples_skill\": \"ovos-skills-info.openvoiceos\", \"wallpaper\": \"default.jpg\", \"persistent_menu_hint\": false, \"examples_enabled\": true, \"randomize_examples\": true, \"examples_prefix\": true } __mycroft_skill_firstrun: This is automatically set on first load of skill weather_skill: DEPRECATED and has no effect - PR pending datetime_skill: Allows you to use a custom skill to display the date and time. Defaults to skill-ovos-date-time.openvoiceos examples_skill: Allows use of a custom skill for the displayed examples. Defaults to ovos_skills_manager.utils.get_skills_example() function wallpaper: Allows a custom wallpaper to be displayed. Use a complete url without any tilde ~ persistent_menu_hint: When true, displayes a hint of the pull-down menu at the top of the page examples_enabled: When false, the examples at the bottom of the screen will be hidden randomize_examples: When false, the rotation of the examples will follow the way they are loaded examples_prefix: When false, the prefix 'Ask Me' will NOT be displayed with the examples","title":"Homescreen"},{"location":"homescreen/#openvoiceos-home-screen","text":"The home screen is the central place for all your tasks. It is the first thing you will see after completing the onboarding process. It supports a variety of pre-defined widgets which provide you with a quick overview of information you need to know like the current date, time and weather. The home screen contains various features and integrations which you can learn more about in the following sections.","title":"OpenVoiceOS Home Screen"},{"location":"homescreen/#features","text":"","title":"Features"},{"location":"homescreen/#night-mode-feature","text":"The Night Mode feature lets you quickly switch your home screen into a dark standby clock, reducing the amount of light emitted by your device. This is especially useful if you are using your device in a dark room or at night. You can enable the night mode feature by tapping on the left edge pill button on the home screen.","title":"Night Mode Feature"},{"location":"homescreen/#quick-actions-dashboard","text":"The Quick Actions Dashboard provides you with a card-based interface to quickly access and add your most used action. The Quick Actions dashboard comes with a variety of pre-defined actions like the ability to quickly add a new alarm, start a new timer or add a new note. You can also add your own custom actions to the dashboard by tapping on the plus button in the top right corner of the dashboard. The Quick Actions dashboard is accessible by tapping on the right edge pill button on the home screen.","title":"Quick Actions Dashboard"},{"location":"homescreen/#application-launcher","text":"OpenVoiceOS comes with support for dedicated voice applications. Voice Applications can be dedicated skills or PHAL plugins, providing their own dedicated user interface. The application launcher will show you a list of all available voice applications. You can access the application launcher by tapping on the center pill button on the bottom of the home screen.","title":"Application Launcher"},{"location":"homescreen/#wallpapers","text":"The home screen supports custom wallpapers and comes with a bunch of wallpapers to choose from. You can easily change your custom wallpaper by swiping from right to left on the home screen.","title":"Wallpapers"},{"location":"homescreen/#widgets","text":"","title":"Widgets"},{"location":"homescreen/#notifications-widget","text":"The notifications widget provides you with a quick overview of all your notifications. The notifications bell icon will be displayed in the top left corner of the home screen. You can access the notifications overview by tapping on the bell icon when it is displayed.","title":"Notifications Widget"},{"location":"homescreen/#timer-widget","text":"The timer widget is displayed in top left corner after the notifications bell icon. It will show up when you have an active timer running. Clicking on the timer widget will open the timers overview.","title":"Timer Widget"},{"location":"homescreen/#alarm-widget","text":"The alarm widget is displayed in top left corner after the timer widget. It will show up when you have an active alarm set. Clicking on the alarm widget will open the alarms overview.","title":"Alarm Widget"},{"location":"homescreen/#media-player-widget","text":"The media player widget is displayed in the bottom of the home screen, It replaces the examples widget when a media player is active. The media player widget will show you the currently playing media and provide you with a quick way to pause, resume or skip the current media. You can also quickly access the media player by tapping the quick display media player button on the right side of the media player widget.","title":"Media Player Widget"},{"location":"homescreen/#configuration","text":"","title":"Configuration"},{"location":"homescreen/#settings","text":"The homescreen has several customizations available. This is sample settings.json file with all of the options explained { \"__mycroft_skill_firstrun\": false, \"weather_skill\": \"skill-weather.openvoiceos\", \"datetime_skill\": \"skill-date-time.mycroftai\", \"examples_skill\": \"ovos-skills-info.openvoiceos\", \"wallpaper\": \"default.jpg\", \"persistent_menu_hint\": false, \"examples_enabled\": true, \"randomize_examples\": true, \"examples_prefix\": true } __mycroft_skill_firstrun: This is automatically set on first load of skill weather_skill: DEPRECATED and has no effect - PR pending datetime_skill: Allows you to use a custom skill to display the date and time. Defaults to skill-ovos-date-time.openvoiceos examples_skill: Allows use of a custom skill for the displayed examples. Defaults to ovos_skills_manager.utils.get_skills_example() function wallpaper: Allows a custom wallpaper to be displayed. Use a complete url without any tilde ~ persistent_menu_hint: When true, displayes a hint of the pull-down menu at the top of the page examples_enabled: When false, the examples at the bottom of the screen will be hidden randomize_examples: When false, the rotation of the examples will follow the way they are loaded examples_prefix: When false, the prefix 'Ask Me' will NOT be displayed with the examples","title":"Settings"},{"location":"intents/","text":"Intents A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. Keyword / Rule based these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here We will now look at each in more detail, including how to use them in a Skill. Keyword Intents Keyword based intent parsers determine user intent based on a list of keywords or entities contained within a user's utterance. Defining keywords and entities Vocab (.voc) Files Vocab files define keywords that the intent parser will look for in a Users utterance to determine their intent. These files can be located in either the vocab/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to list synonyms or terms that have the same meaning in the context of this Skill. OVOS will match any of these keywords with the Intent. Consider a simple Potato.voc . Within this file we might include: potato potatoes spud If the User speaks either : potato or potatoes or spud OVOS will match this to any Keyword Intents that are using the Potato keyword. Regular Expression (.rx) Files Regular expressions (or regex) allow us to capture entities based on the structure of an utterance. We strongly recommend you avoid using regex, it is very hard to make portable across languages, hard to translate and the reported confidence of the intents is not great. We suggest using example based intents instead if you find yourself needing regex These files can be located in either the regex/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to provide different ways that an entity may be referenced. OVOS will execute these lines in the order they appear and return the first result as an entity to the Intent Handler. Let's consider a type.rx file to extract the type of potato we are interested in. Within this file we might include: .* about (?P<Type>.*) potatoes .* (make|like) (?P<Type>.*) potato What is this regex doing? .* matches zero, one or more of any single character. (?P<Type>.*) is known as a Named Capturing Group. The variable name is defined between the \"<>\", and what is captured is defined after this name. In this case we use .* to capture anything. Learn more about Regular Expressions . So our first line would match an utterance such as: Tell me about sweet potatoes Whilst the second line will match either: Do you like deep fried potato or How do I make mashed potato From these three utterances, what will the extracted Type be:\\ 1. sweet \\ 2. deep fried \\ 3. mashed This Type will be available to use in your Skill's Intent Handler on the message object. We can access this using: message.data.get('Type') Using Keyword Intents in a Skill Now that we have a Vocab and Regular Expression defined, let's look at how to use these in a simple Skill. For the following example we will use the two files we outlined above: Potato.voc Type.rx We will also add some new .voc files: Like.voc - containing a single line \"like\" You.voc - containing a single line \"you\" I.voc - containing a single line \"I\" Creating the Intent Handler To construct a Keyword Intent, we use the intent_handler() _decorator_ and pass in the IntentBuilder helper class. Learn more about decorators in Python . Both of these must be imported before we can use them: from adapt.intent import IntentBuilder from mycroft import intent_handler The IntentBuilder is then passed the name of the Intent as a string, followed by one or more parameters that correspond with one of our .voc or .rx files. @intent_handler(IntentBuilder('IntentName') .require('Potato') .require('Like') .optionally('Type') .one_of('You', 'I')) In this example: the Potato and Like keywords are required. It must be present for the intent to match. the Type entity is optional. A stronger match will be made if this is found, but it is not required. we require at least one of the You or I keywords. What are some utterances that would match this intent? Do you like potato? Do you like fried potato? Will I like mashed potato? Do you think I would like potato? What are some utterances that would not match the intent? How do I make mashed potato? The required Like keyword is not found. Is it like a potato? Neither the You nor I keyword is found. Including it in a Skill Now we can create our Potato Skill: from adapt.intent import IntentBuilder from mycroft import intent_handler class PotatoSkill(MycroftSkill): @intent_handler(IntentBuilder('WhatIsPotato').require('What') .require('Potato')) def handle_what_is(self, message): self.speak_dialog('potato.description') @intent_handler(IntentBuilder('DoYouLikePotato').require('Potato') .require('Like').optionally('Type').one_of('You', 'I')) def handle_do_you_like(self, message): potato_type = message.data.get('Type') if potato_type is not None: self.speak_dialog('like.potato.type', {'type': potato_type}) else: self.speak_dialog('like.potato.generic') def create_skill(): return PotatoSkill() You can download this entire Potato Skill from Github , or see another Keyword Intent handler example in the Hello World Skill Common Problems More vocab! One of the most common mistakes when getting started with Skills is that the vocab file doesn't include all the keywords or terms that a User might use to trigger the intent. It is important to map out your Skill and test the interactions with others to see how they might ask questions differently. I have added new phrases in the .voc file, but Mycroft isn't recognizing them Compound words like \"don't\", \"won't\", \"shouldn't\" etc. are normalized by OVOS - so they become \"do not\", \"will not\", \"should not\". You should use the normalized words in your .voc files. Similarly, definite articles like the word \"the\" are removed in the normalization process, so avoid using them in your .voc or .rx files as well. Tab != 4 Spaces, sometimes your text editor or IDE automatically replaces tabs with spaces or vice versa. This may lead to an indentation error. So make sure there's no extra tabs and that your editor doesn't replace your spaces! Wrong order of files directories is a very common mistake. You have to make a language sub-folder inside the dialog, vocab or locale folders such as skill-dir/locale/en-us/somefile.dialog . So make sure that your .voc files and .dialog files inside a language subfolder. I am unable to match against the utterance string The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler(IntentBuilder('Example').require('Example').require('Intent')) def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it') Example based Intents Example based parsers have a number of key benefits over other intent parsing technologies. Intents are easy to create You can easily extract entities and then use these in Skills. For example, \"Find the nearest gas station\" -> { \"place\":\"gas station\"} Disambiguation between intents is easier Harder to create a bad intent that throws the intent parser off Creating Intents Most example based intent parsers use a series of example sentences to train a machine learning model to identify an intent. Regex can also be used behind the scenes for example to extract entities The examples are stored in a Skill's vocab/lang or local/lang directory, in files ending in the file extension .intent . For example, if you were to create a tomato Skill to respond to questions about a tomato , you would create the file vocab/en-us/what.is.a.tomato.intent This file would contain examples of questions asking what a tomato is. what would you say a tomato is what is a tomato describe a tomato what defines a tomato These sample phrases do not require punctuation like a question mark. We can also leave out contractions such as \"what's\", as this will be automatically expanded to \"what is\" by OVOS before the utterance is parsed. Each file should contain at least 4 examples for good modeling. The above example allows us to map many phrases to a single intent, however often we need to extract specific data from an utterance. This might be a date, location, category, or some other entity . Defining entities Let's now find out OVOS's opinion on different types of tomatoes. To do this we will create a new intent file: vocab/en-us/do.you.like.intent with examples of questions about mycroft's opinion about tomatoes: are you fond of tomatoes do you like tomatoes what are your thoughts on tomatoes are you fond of {type} tomatoes do you like {type} tomatoes what are your thoughts on {type} tomatoes Note the {type} in the above examples. These are wild-cards where matching content is forwarded to the skill's intent handler. Specific Entities In the above example, {type} will match anything. While this makes the intent flexible, it will also match if we say something like Do you like eating tomatoes?. It would think the type of tomato is \"eating\" which doesn't make much sense. Instead, we can specify what type of things the {type} of tomato should be. We do this by defining the type entity file here: vocab/en-us/type.entity which might contain something like: red reddish green greenish yellow yellowish ripe unripe pale This must be registered in the Skill before use - most commonly in the initialize() method: from mycroft import MycroftSkill, intent_handler class TomatoSkill(MycroftSkill): def initialize(self): self.register_entity_file('type.entity') Now, we can say things like \"do you like greenish tomatoes?\" and it will tag type as: \"greenish\" . However, if we say \"do you like eating tomatoes?\" - the phrase will not match as \"eating\" is not included in our type.entity file. Number matching Let's say you are writing an Intent to call a phone number. You can make it only match specific formats of numbers by writing out possible arrangements using # where a number would go. For example, with the following intent: Call {number}. Call the phone number {number}. the number.entity could be written as: +### (###) ###-#### +## (###) ###-#### +# (###) ###-#### (###) ###-#### ###-#### ###-###-#### ###.###.#### ### ### #### ########## Entities with unknown tokens Let's say you wanted to create an intent to match places: Directions to {place}. Navigate me to {place}. Open maps to {place}. Show me how to get to {place}. How do I get to {place}? This alone will work, but it will still get a high confidence with a phrase like \"How do I get to the boss in my game?\". We can try creating a .entity file with things like: New York City #### Georgia Street San Francisco The problem is, now anything that is not specifically a mix of New York City, San Francisco, or something on Georgia Street won't match. Instead, we can specify an unknown word with :0. This would be written as: :0 :0 City #### :0 Street :0 :0 Now, while this will still match quite a lot, it will match things like \"Directions to Baldwin City\" more than \"How do I get to the boss in my game?\" NOTE: Currently, the number of :0 words is not fully taken into consideration so the above might match quite liberally, but this will change in the future. Parentheses Expansion Sometimes you might find yourself writing a lot of variations of the same thing. For example, to write a skill that orders food, you might write the following intent: Order some {food}. Order some {food} from {place}. Grab some {food}. Grab some {food} from {place}. Rather than writing out all combinations of possibilities, you can embed them into one or more lines by writing each possible option inside parentheses with | in between each part. For example, that same intent above could be written as: (Order | Grab) some {food} (Order | Grab) some {food} from {place} or even on a single-line: (Order | Grab) some {food} (from {place} | ) Nested parentheses are supported to create even more complex combinations, such as the following: (Look (at | for) | Find) {object}. Which would expand to: Look at {object} Look for {object} Find {object} There is no performance benefit to using parentheses expansion. When used appropriately, this syntax can be much clearer to read. However, more complex structures should be broken down into multiple lines to aid readability and reduce false utterances being included in the model. Overuse can even result in the model training timing out, rendering the Skill unusable. Using it in a Skill The intent_handler() decorator can be used to create an examples based intent handler by passing in the filename of the .intent file as a string. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated and you can now replace any instance of this with the simpler @intent_handler decorator. From our first example above, we created a file vocab/en-us/what.is.a.tomato.intent . To register an intent using this file we can use: @intent_handler('what.is.a.tomato.intent') This decorator must be imported before it is used: from mycroft import intent_handler Learn more about decorators in Python . Now we can create our Tomato Skill: from mycroft import MycroftSkill, intent_handler class TomatoSkill(MycroftSkill): def initialize(self): self.register_entity_file('type.entity') @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): self.speak_dialog('tomato.description') @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') def stop(self): pass def create_skill(): return TomatoSkill() See a Padatious intent handler example in the Hello World Skill Common Problems I am unable to match against the utterance string The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler('example.intent') def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"Intents"},{"location":"intents/#intents","text":"A user can accomplish the same task by expressing their intent in multiple ways. The role of the intent parser is to extract from the user's speech key data elements that specify their intent in more detail. This data can then be passed to other services, such as Skills to help the user accomplish their intended task. Example : Julie wants to know about today's weather in her current location, which is Melbourne, Australia. \"hey mycroft, what's today's weather like?\" \"hey mycroft, what's the weather like in Melbourne?\" \"hey mycroft, weather\" Even though these are three different expressions, for most of us they probably have roughly the same meaning. In each case we would assume the user expects OVOS to respond with today's weather for their current location. The role of an intent parser is to determine what this intent is. In the example above, we might extract data elements like: weather - we know that Julie wants to know about the weather, but she has not been specific about the type of weather, such as wind , precipitation , snowfall or the risk of fire danger from bushfires. Melbourne, Australia rarely experiences snowfall, but falls under bushfire risk every summer. location - Julie has stipulated her location as Melbourne, but she does not state that she means Melbourne, Australia. How do we distinguish this from Melbourne, Florida, United States? date - Julie has been specific about the timeframe she wants weather data for - today. But how do we know what today means in Julie's timezone. Melbourne, Australia is between 14-18 hours ahead of the United States. We don't want to give Julie yesterday's weather, particularly as Melbourne is renowned for having changeable weather. OVOS has two separate Intent parsing engines each with their own strengths. Each of these can be used in most situations, however they will process the utterance in different ways. Example based intents are trained on whole phrases. These intents are generally more accurate however require you to include sample phrases that cover the breadth of ways that a User may ask about something. Keyword / Rule based these intents look for specific required keywords. They are more flexible, but since these are essentially rule based this can result in a lot of false matches. A badly designed intent may totally throw the intent parser off guard. The main advantage of keyword based intents is the integration with conversational context , they facilitate continuous dialogs OVOS is moving towards a plugin system for intent engines, currently only the default MycroftAI intent parsers are supported Padatious is a light-weight neural network that is trained on whole phrases. You can find the official documentation here Adapt is a keyword based parser. You can find the official documentation here We will now look at each in more detail, including how to use them in a Skill.","title":"Intents"},{"location":"intents/#keyword-intents","text":"Keyword based intent parsers determine user intent based on a list of keywords or entities contained within a user's utterance.","title":"Keyword Intents"},{"location":"intents/#defining-keywords-and-entities","text":"","title":"Defining keywords and entities"},{"location":"intents/#vocab-voc-files","text":"Vocab files define keywords that the intent parser will look for in a Users utterance to determine their intent. These files can be located in either the vocab/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to list synonyms or terms that have the same meaning in the context of this Skill. OVOS will match any of these keywords with the Intent. Consider a simple Potato.voc . Within this file we might include: potato potatoes spud If the User speaks either : potato or potatoes or spud OVOS will match this to any Keyword Intents that are using the Potato keyword.","title":"Vocab (.voc) Files"},{"location":"intents/#regular-expression-rx-files","text":"Regular expressions (or regex) allow us to capture entities based on the structure of an utterance. We strongly recommend you avoid using regex, it is very hard to make portable across languages, hard to translate and the reported confidence of the intents is not great. We suggest using example based intents instead if you find yourself needing regex These files can be located in either the regex/lang-code/ or locale/lang-code/ directories of a Skill. They can have one or more lines to provide different ways that an entity may be referenced. OVOS will execute these lines in the order they appear and return the first result as an entity to the Intent Handler. Let's consider a type.rx file to extract the type of potato we are interested in. Within this file we might include: .* about (?P<Type>.*) potatoes .* (make|like) (?P<Type>.*) potato What is this regex doing? .* matches zero, one or more of any single character. (?P<Type>.*) is known as a Named Capturing Group. The variable name is defined between the \"<>\", and what is captured is defined after this name. In this case we use .* to capture anything. Learn more about Regular Expressions . So our first line would match an utterance such as: Tell me about sweet potatoes Whilst the second line will match either: Do you like deep fried potato or How do I make mashed potato From these three utterances, what will the extracted Type be:\\ 1. sweet \\ 2. deep fried \\ 3. mashed This Type will be available to use in your Skill's Intent Handler on the message object. We can access this using: message.data.get('Type')","title":"Regular Expression (.rx) Files"},{"location":"intents/#using-keyword-intents-in-a-skill","text":"Now that we have a Vocab and Regular Expression defined, let's look at how to use these in a simple Skill. For the following example we will use the two files we outlined above: Potato.voc Type.rx We will also add some new .voc files: Like.voc - containing a single line \"like\" You.voc - containing a single line \"you\" I.voc - containing a single line \"I\"","title":"Using Keyword Intents in a Skill"},{"location":"intents/#creating-the-intent-handler","text":"To construct a Keyword Intent, we use the intent_handler() _decorator_ and pass in the IntentBuilder helper class. Learn more about decorators in Python . Both of these must be imported before we can use them: from adapt.intent import IntentBuilder from mycroft import intent_handler The IntentBuilder is then passed the name of the Intent as a string, followed by one or more parameters that correspond with one of our .voc or .rx files. @intent_handler(IntentBuilder('IntentName') .require('Potato') .require('Like') .optionally('Type') .one_of('You', 'I')) In this example: the Potato and Like keywords are required. It must be present for the intent to match. the Type entity is optional. A stronger match will be made if this is found, but it is not required. we require at least one of the You or I keywords. What are some utterances that would match this intent? Do you like potato? Do you like fried potato? Will I like mashed potato? Do you think I would like potato? What are some utterances that would not match the intent? How do I make mashed potato? The required Like keyword is not found. Is it like a potato? Neither the You nor I keyword is found.","title":"Creating the Intent Handler"},{"location":"intents/#including-it-in-a-skill","text":"Now we can create our Potato Skill: from adapt.intent import IntentBuilder from mycroft import intent_handler class PotatoSkill(MycroftSkill): @intent_handler(IntentBuilder('WhatIsPotato').require('What') .require('Potato')) def handle_what_is(self, message): self.speak_dialog('potato.description') @intent_handler(IntentBuilder('DoYouLikePotato').require('Potato') .require('Like').optionally('Type').one_of('You', 'I')) def handle_do_you_like(self, message): potato_type = message.data.get('Type') if potato_type is not None: self.speak_dialog('like.potato.type', {'type': potato_type}) else: self.speak_dialog('like.potato.generic') def create_skill(): return PotatoSkill() You can download this entire Potato Skill from Github , or see another Keyword Intent handler example in the Hello World Skill","title":"Including it in a Skill"},{"location":"intents/#common-problems","text":"","title":"Common Problems"},{"location":"intents/#more-vocab","text":"One of the most common mistakes when getting started with Skills is that the vocab file doesn't include all the keywords or terms that a User might use to trigger the intent. It is important to map out your Skill and test the interactions with others to see how they might ask questions differently.","title":"More vocab!"},{"location":"intents/#i-have-added-new-phrases-in-the-voc-file-but-mycroft-isnt-recognizing-them","text":"Compound words like \"don't\", \"won't\", \"shouldn't\" etc. are normalized by OVOS - so they become \"do not\", \"will not\", \"should not\". You should use the normalized words in your .voc files. Similarly, definite articles like the word \"the\" are removed in the normalization process, so avoid using them in your .voc or .rx files as well. Tab != 4 Spaces, sometimes your text editor or IDE automatically replaces tabs with spaces or vice versa. This may lead to an indentation error. So make sure there's no extra tabs and that your editor doesn't replace your spaces! Wrong order of files directories is a very common mistake. You have to make a language sub-folder inside the dialog, vocab or locale folders such as skill-dir/locale/en-us/somefile.dialog . So make sure that your .voc files and .dialog files inside a language subfolder.","title":"I have added new phrases in the .voc file, but Mycroft isn't recognizing them"},{"location":"intents/#i-am-unable-to-match-against-the-utterance-string","text":"The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler(IntentBuilder('Example').require('Example').require('Intent')) def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"I am unable to match against the utterance string"},{"location":"intents/#example-based-intents","text":"Example based parsers have a number of key benefits over other intent parsing technologies. Intents are easy to create You can easily extract entities and then use these in Skills. For example, \"Find the nearest gas station\" -> { \"place\":\"gas station\"} Disambiguation between intents is easier Harder to create a bad intent that throws the intent parser off","title":"Example based Intents"},{"location":"intents/#creating-intents","text":"Most example based intent parsers use a series of example sentences to train a machine learning model to identify an intent. Regex can also be used behind the scenes for example to extract entities The examples are stored in a Skill's vocab/lang or local/lang directory, in files ending in the file extension .intent . For example, if you were to create a tomato Skill to respond to questions about a tomato , you would create the file vocab/en-us/what.is.a.tomato.intent This file would contain examples of questions asking what a tomato is. what would you say a tomato is what is a tomato describe a tomato what defines a tomato These sample phrases do not require punctuation like a question mark. We can also leave out contractions such as \"what's\", as this will be automatically expanded to \"what is\" by OVOS before the utterance is parsed. Each file should contain at least 4 examples for good modeling. The above example allows us to map many phrases to a single intent, however often we need to extract specific data from an utterance. This might be a date, location, category, or some other entity .","title":"Creating Intents"},{"location":"intents/#defining-entities","text":"Let's now find out OVOS's opinion on different types of tomatoes. To do this we will create a new intent file: vocab/en-us/do.you.like.intent with examples of questions about mycroft's opinion about tomatoes: are you fond of tomatoes do you like tomatoes what are your thoughts on tomatoes are you fond of {type} tomatoes do you like {type} tomatoes what are your thoughts on {type} tomatoes Note the {type} in the above examples. These are wild-cards where matching content is forwarded to the skill's intent handler.","title":"Defining entities"},{"location":"intents/#specific-entities","text":"In the above example, {type} will match anything. While this makes the intent flexible, it will also match if we say something like Do you like eating tomatoes?. It would think the type of tomato is \"eating\" which doesn't make much sense. Instead, we can specify what type of things the {type} of tomato should be. We do this by defining the type entity file here: vocab/en-us/type.entity which might contain something like: red reddish green greenish yellow yellowish ripe unripe pale This must be registered in the Skill before use - most commonly in the initialize() method: from mycroft import MycroftSkill, intent_handler class TomatoSkill(MycroftSkill): def initialize(self): self.register_entity_file('type.entity') Now, we can say things like \"do you like greenish tomatoes?\" and it will tag type as: \"greenish\" . However, if we say \"do you like eating tomatoes?\" - the phrase will not match as \"eating\" is not included in our type.entity file.","title":"Specific Entities"},{"location":"intents/#number-matching","text":"Let's say you are writing an Intent to call a phone number. You can make it only match specific formats of numbers by writing out possible arrangements using # where a number would go. For example, with the following intent: Call {number}. Call the phone number {number}. the number.entity could be written as: +### (###) ###-#### +## (###) ###-#### +# (###) ###-#### (###) ###-#### ###-#### ###-###-#### ###.###.#### ### ### #### ##########","title":"Number matching"},{"location":"intents/#entities-with-unknown-tokens","text":"Let's say you wanted to create an intent to match places: Directions to {place}. Navigate me to {place}. Open maps to {place}. Show me how to get to {place}. How do I get to {place}? This alone will work, but it will still get a high confidence with a phrase like \"How do I get to the boss in my game?\". We can try creating a .entity file with things like: New York City #### Georgia Street San Francisco The problem is, now anything that is not specifically a mix of New York City, San Francisco, or something on Georgia Street won't match. Instead, we can specify an unknown word with :0. This would be written as: :0 :0 City #### :0 Street :0 :0 Now, while this will still match quite a lot, it will match things like \"Directions to Baldwin City\" more than \"How do I get to the boss in my game?\" NOTE: Currently, the number of :0 words is not fully taken into consideration so the above might match quite liberally, but this will change in the future.","title":"Entities with unknown tokens"},{"location":"intents/#parentheses-expansion","text":"Sometimes you might find yourself writing a lot of variations of the same thing. For example, to write a skill that orders food, you might write the following intent: Order some {food}. Order some {food} from {place}. Grab some {food}. Grab some {food} from {place}. Rather than writing out all combinations of possibilities, you can embed them into one or more lines by writing each possible option inside parentheses with | in between each part. For example, that same intent above could be written as: (Order | Grab) some {food} (Order | Grab) some {food} from {place} or even on a single-line: (Order | Grab) some {food} (from {place} | ) Nested parentheses are supported to create even more complex combinations, such as the following: (Look (at | for) | Find) {object}. Which would expand to: Look at {object} Look for {object} Find {object} There is no performance benefit to using parentheses expansion. When used appropriately, this syntax can be much clearer to read. However, more complex structures should be broken down into multiple lines to aid readability and reduce false utterances being included in the model. Overuse can even result in the model training timing out, rendering the Skill unusable.","title":"Parentheses Expansion"},{"location":"intents/#using-it-in-a-skill","text":"The intent_handler() decorator can be used to create an examples based intent handler by passing in the filename of the .intent file as a string. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated and you can now replace any instance of this with the simpler @intent_handler decorator. From our first example above, we created a file vocab/en-us/what.is.a.tomato.intent . To register an intent using this file we can use: @intent_handler('what.is.a.tomato.intent') This decorator must be imported before it is used: from mycroft import intent_handler Learn more about decorators in Python . Now we can create our Tomato Skill: from mycroft import MycroftSkill, intent_handler class TomatoSkill(MycroftSkill): def initialize(self): self.register_entity_file('type.entity') @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): self.speak_dialog('tomato.description') @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') def stop(self): pass def create_skill(): return TomatoSkill() See a Padatious intent handler example in the Hello World Skill","title":"Using it in a Skill"},{"location":"intents/#common-problems_1","text":"","title":"Common Problems"},{"location":"intents/#i-am-unable-to-match-against-the-utterance-string_1","text":"The utterance string received from the speech-to-text engine is received all lowercase. As such any string matching you are trying to do should also be converted to lowercase. For example: @intent_handler('example.intent') def handle_example(self, message): utterance = message.data.get('utterance') if 'Proper Noun'.lower() in utterance: self.speak('Found it')","title":"I am unable to match against the utterance string"},{"location":"jurebes/","text":"jurebes J.U.R.E.B.E.S: Joint Universal Rule-based Engine and Bagging Ensemble-based System This acronym reflects a combined approach of using rule-based techniques along with a bagging ensemble-based approach for intent parsing in the JUREBES engine, written in Python with the use of NLTK and scikit-learn libraries. Usage from jurebes import JurebesIntentContainer hello = [\"hello human\", \"hello there\", \"hey\", \"hello\", \"hi\"] name = [\"my name is {name}\", \"call me {name}\", \"I am {name}\", \"the name is {name}\", \"{name} is my name\", \"{name} is my name\"] joke = [\"tell me a joke\", \"say a joke\", \"tell joke\"] engine = JurebesIntentContainer() engine.add_entity(\"name\", [\"jarbas\", \"bob\", \"Jo\u00e3o Casimiro Ferreira\"]) engine.add_intent(\"hello\", hello) engine.add_intent(\"name\", name) engine.add_intent(\"joke\", joke) engine.train() test_set = {\"name\": [\"I am groot\", \"my name is jarbas\", \"jarbas is the name\", \"they call me Ana Ferreira\"], \"hello\": [\"hello beautiful\", \"hello bob\", \"hello world\"], \"joke\": [\"say joke\", \"make me laugh\", \"do you know any joke\"]} for intent, sents in test_set.items(): for sent in sents: # optionally pass a threshold to return None instead of low confidence matches print(sent, engine.calc_intent(sent, threshold=0.5)) # I am groot IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'groot'}) # my name is jarbas IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'jarbas'}) # jarbas is the name IntentMatch(intent_name='name', confidence=0.9201351734080562, entities={'name': 'jarbas'}) # call me Ana Ferreira IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'ferreira'}) # hello beautiful IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # hello bob IntentMatch(intent_name='hello', confidence=0.5400801051648911, entities={'name': 'bob'}) # hello world IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # say joke IntentMatch(intent_name='joke', confidence=0.9785338275012387, entities={}) # make me laugh IntentMatch(intent_name='name', confidence=0.725778770677012, entities={}) # do you know any joke IntentMatch(intent_name='joke', confidence=0.917960967116358, entities={}) Advanced Usage you can select the classifiers or enable fuzzy matching and influence predictions, jurebes is stateful from jurebes import JurebesIntentContainer # single clf clf = SVC(probability=True) # any scikit-learn clf # multiple classifiers will use soft voting to select prediction # clf = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] / default if not in args tagger = OVOSNgramTagger(default_tag=\"O\") # classic nltk / default if not in args #tagger = SVC(probability=True) # any scikit-learn clf #tagger = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] # pre defined pipelines from ovos-classifiers clf_pipeline = \"tfidf\" # default if not in args tagger_pipeline = \"words\" # default if not in args engine = JurebesIntentContainer(clf, tagger, clf_pipeline, tagger_pipeline) (...) # register intents # fuzzy matching engine.enable_fuzzy() sent = \"they call me Ana Ferreira\" print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8716633619210677, entities={'name': 'ana ferreira'}) engine.disable_fuzzy() print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8282293617609358, entities={'name': 'ferreira'}) # temporarily disable a intent engine.detach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='hello', confidence=0.06113697262028985, entities={'name': 'ferreira'}) engine.reatach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8548664325189478, entities={'name': 'ferreira'}) # force correct predictions engine.exclude_keywords(\"name\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=0.5125373111690074, entities={}) engine.exclude_keywords(\"hello\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=1.0, entities={}) # inject context engine.set_context(\"joke\", \"joke_type\", \"chuck_norris\") # if a value is passed it will populate entities print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9707841337857908, entities={'joke_type': 'chuck_norris'}) # require context engine.require_context(\"joke\", \"joke_type\") engine.unset_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='hello', confidence=0.060199275248566525, entities={}) engine.unrequire_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9462089582801377, entities={}) # exclude intent matches based on context engine.exclude_context(\"hello\", \"said_hello\") print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='hello', confidence=1, entities={}) engine.set_context(\"hello\", \"said_hello\") # now wont predict hello intent print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='joke', confidence=0.06986199472674888, entities={})","title":"Jurebes"},{"location":"jurebes/#jurebes","text":"J.U.R.E.B.E.S: Joint Universal Rule-based Engine and Bagging Ensemble-based System This acronym reflects a combined approach of using rule-based techniques along with a bagging ensemble-based approach for intent parsing in the JUREBES engine, written in Python with the use of NLTK and scikit-learn libraries.","title":"jurebes"},{"location":"jurebes/#usage","text":"from jurebes import JurebesIntentContainer hello = [\"hello human\", \"hello there\", \"hey\", \"hello\", \"hi\"] name = [\"my name is {name}\", \"call me {name}\", \"I am {name}\", \"the name is {name}\", \"{name} is my name\", \"{name} is my name\"] joke = [\"tell me a joke\", \"say a joke\", \"tell joke\"] engine = JurebesIntentContainer() engine.add_entity(\"name\", [\"jarbas\", \"bob\", \"Jo\u00e3o Casimiro Ferreira\"]) engine.add_intent(\"hello\", hello) engine.add_intent(\"name\", name) engine.add_intent(\"joke\", joke) engine.train() test_set = {\"name\": [\"I am groot\", \"my name is jarbas\", \"jarbas is the name\", \"they call me Ana Ferreira\"], \"hello\": [\"hello beautiful\", \"hello bob\", \"hello world\"], \"joke\": [\"say joke\", \"make me laugh\", \"do you know any joke\"]} for intent, sents in test_set.items(): for sent in sents: # optionally pass a threshold to return None instead of low confidence matches print(sent, engine.calc_intent(sent, threshold=0.5)) # I am groot IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'groot'}) # my name is jarbas IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'jarbas'}) # jarbas is the name IntentMatch(intent_name='name', confidence=0.9201351734080562, entities={'name': 'jarbas'}) # call me Ana Ferreira IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'ferreira'}) # hello beautiful IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # hello bob IntentMatch(intent_name='hello', confidence=0.5400801051648911, entities={'name': 'bob'}) # hello world IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={}) # say joke IntentMatch(intent_name='joke', confidence=0.9785338275012387, entities={}) # make me laugh IntentMatch(intent_name='name', confidence=0.725778770677012, entities={}) # do you know any joke IntentMatch(intent_name='joke', confidence=0.917960967116358, entities={})","title":"Usage"},{"location":"jurebes/#advanced-usage","text":"you can select the classifiers or enable fuzzy matching and influence predictions, jurebes is stateful from jurebes import JurebesIntentContainer # single clf clf = SVC(probability=True) # any scikit-learn clf # multiple classifiers will use soft voting to select prediction # clf = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] / default if not in args tagger = OVOSNgramTagger(default_tag=\"O\") # classic nltk / default if not in args #tagger = SVC(probability=True) # any scikit-learn clf #tagger = [SVC(probability=True), LogisticRegression(), DecisionTreeClassifier()] # pre defined pipelines from ovos-classifiers clf_pipeline = \"tfidf\" # default if not in args tagger_pipeline = \"words\" # default if not in args engine = JurebesIntentContainer(clf, tagger, clf_pipeline, tagger_pipeline) (...) # register intents # fuzzy matching engine.enable_fuzzy() sent = \"they call me Ana Ferreira\" print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8716633619210677, entities={'name': 'ana ferreira'}) engine.disable_fuzzy() print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8282293617609358, entities={'name': 'ferreira'}) # temporarily disable a intent engine.detach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='hello', confidence=0.06113697262028985, entities={'name': 'ferreira'}) engine.reatach_intent(\"name\") print(engine.calc_intent(sent)) # IntentMatch(intent_name='name', confidence=0.8548664325189478, entities={'name': 'ferreira'}) # force correct predictions engine.exclude_keywords(\"name\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=0.5125373111690074, entities={}) engine.exclude_keywords(\"hello\", [\"laugh\"]) print(engine.calc_intent(\"make me laugh\")) # IntentMatch(intent_name='joke', confidence=1.0, entities={}) # inject context engine.set_context(\"joke\", \"joke_type\", \"chuck_norris\") # if a value is passed it will populate entities print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9707841337857908, entities={'joke_type': 'chuck_norris'}) # require context engine.require_context(\"joke\", \"joke_type\") engine.unset_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='hello', confidence=0.060199275248566525, entities={}) engine.unrequire_context(\"joke\", \"joke_type\") print(engine.calc_intent(\"tell me a chuch norris joke\")) # IntentMatch(intent_name='joke', confidence=0.9462089582801377, entities={}) # exclude intent matches based on context engine.exclude_context(\"hello\", \"said_hello\") print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='hello', confidence=1, entities={}) engine.set_context(\"hello\", \"said_hello\") # now wont predict hello intent print(engine.calc_intent(\"hello\")) # IntentMatch(intent_name='joke', confidence=0.06986199472674888, entities={})","title":"Advanced Usage"},{"location":"lang_plugins/","text":"Language Detection/Translation Plugins These plugins can be used to detect the language of text and to translate it They are not used internally by ovos-core but are integrated with external tools neon-core also makes heavy use of OPM language plugins List of Language plugins Plugin Detect Tx Offline Type neon-lang-plugin-cld2 yes no yes FOSS neon-lang-plugin-cld3 yes no yes FOSS neon-lang-plugin-langdetect yes no yes FOSS neon-lang-plugin-fastlang yes no yes FOSS neon-lang-plugin-lingua_podre yes no yes FOSS neon-lang-plugin-libretranslate yes yes no API (self hosted) neon-lang-plugin-apertium no yes no API (self hosted) neon-lang-plugin-amazon_translate yes yes no API (key) neon-lang-plugin-google_translate yes yes no API (key)","title":"Language Detection/Translation"},{"location":"lang_plugins/#language-detectiontranslation-plugins","text":"These plugins can be used to detect the language of text and to translate it They are not used internally by ovos-core but are integrated with external tools neon-core also makes heavy use of OPM language plugins","title":"Language Detection/Translation Plugins"},{"location":"lang_plugins/#list-of-language-plugins","text":"Plugin Detect Tx Offline Type neon-lang-plugin-cld2 yes no yes FOSS neon-lang-plugin-cld3 yes no yes FOSS neon-lang-plugin-langdetect yes no yes FOSS neon-lang-plugin-fastlang yes no yes FOSS neon-lang-plugin-lingua_podre yes no yes FOSS neon-lang-plugin-libretranslate yes yes no API (self hosted) neon-lang-plugin-apertium no yes no API (self hosted) neon-lang-plugin-amazon_translate yes yes no API (key) neon-lang-plugin-google_translate yes yes no API (key)","title":"List of Language plugins"},{"location":"license/","text":"License We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects. Notable licensing exceptions The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-espeakng GPL depends on espeak-ng (GPL) ovos-g2p-plugin-espeak GPL depends on espeak-phonemizer (GPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"License"},{"location":"license/#license","text":"We have a universal donor policy, our code should be able to be used anywhere by anyone, no ifs or conditions attached. OVOS is predominately Apache2 or BSD licensed. There are only a few exceptions to this, which are all licensed under other compatible open source licenses. Individual plugins or skills may have their own license, for example mimic3 is AGPL, so we can not change the license of our plugin. We are committed to maintain all core components fully free, any code that we have no control over the license will live in an optional plugin and be flagged as such. This includes avoiding LGPL code for reasons explained here . Our license policy has the following properties: It gives you, the user of the software, complete and unrestrained access to the software, such that you may inspect, modify, and redistribute your changes Inspection - Anyone may inspect the software for security vulnerabilities Modification - Anyone may modify the software to fix issues or add features Redistribution - Anyone may redistribute the software on their terms It is compatible with GPL licenses - Projects licensed as GPL can be distributed with OVOS It allows for the incorporation of GPL-incompatible free software, such as software that is CDDL licensed The license does not restrict the software that may run on OVOS, however -- and thanks to the plugin architecture, even traditionally tightly-coupled components such as drivers can be distributed separately, so maintainers are free to choose whatever license they like for their projects.","title":"License"},{"location":"license/#notable-licensing-exceptions","text":"The following repositories do not respect our universal donor policy, please ensure their licenses are compatible before you use them Repository License Reason ovos-intent-plugin-padatious Apache2.0 padatious license might not be valid, depends on libfann2 (LGPL) ovos-tts-plugin-mimic3 AGPL depends on mimic3 (AGPL) ovos-tts-plugin-espeakng GPL depends on espeak-ng (GPL) ovos-g2p-plugin-espeak GPL depends on espeak-phonemizer (GPL) ovos-tts-plugin-SAM ? reverse engineered abandonware","title":"Notable licensing exceptions"},{"location":"linguistika/","text":"Open Linguistika Open Linguistika is a tool to allow Mycroft Skill developers working on GUI\u2019s to easily translate their GUI\u2019s to other languages. For Mycroft\u2019s GUI, the UI interface used currently by Mycroft for which you can find QML files under the UI directory of skills, is based on Qt. Mycroft GUI uses Qt\u2019s translation mechanism to translate GUI\u2019s to other languages. To get your skills GUI translated and ready for other languages involves several manual steps from running Qt tools like lupdate against each QML UI file for each translatable language to running Qt\u2019s tool lrelease for specific language targets to compile a language for the QT environment to understand. To make your developer experience smarter and easier the OpenVoiceOS team is introducing an all-in-one toolkit for GUI language translations. The Open Linguistika toolkit allows developers to use auto-translate from various supported translator providers, and additionally support more languages, with the possibility for manual translations without having to go through the different Qt tools and command chain required to manually support a skill GUI for a different language. As a GUI skill Developer, the only know-how you need is to add the translation calls to your skill QML files, Developers can get more information about how to add them here Internationalization and Localization with Qt Quick | Qt 6.3. The \u201cTLDR\u201d version is that for every hard-coded string in your QML UI skill file you need to decorate your strings with the qsTr() decorator and your model list elements with the QT_TR_NOOP() decorator. Open Linguistika when installed by default on your distribution by choice, currently supports 6 European languages and 2 auto-translation providers. The tool provides extensibility through its JSON configuration interface to add more language support, where using a simple JSON language addition mechanism you can extend the tool to support a number of additional languages you would like to support for your skills UI. You can read more about adding additional languages on the tool\u2019s GitHub repository. How-To-Use Demo:","title":"Open Linguistika"},{"location":"linguistika/#open-linguistika","text":"Open Linguistika is a tool to allow Mycroft Skill developers working on GUI\u2019s to easily translate their GUI\u2019s to other languages. For Mycroft\u2019s GUI, the UI interface used currently by Mycroft for which you can find QML files under the UI directory of skills, is based on Qt. Mycroft GUI uses Qt\u2019s translation mechanism to translate GUI\u2019s to other languages. To get your skills GUI translated and ready for other languages involves several manual steps from running Qt tools like lupdate against each QML UI file for each translatable language to running Qt\u2019s tool lrelease for specific language targets to compile a language for the QT environment to understand. To make your developer experience smarter and easier the OpenVoiceOS team is introducing an all-in-one toolkit for GUI language translations. The Open Linguistika toolkit allows developers to use auto-translate from various supported translator providers, and additionally support more languages, with the possibility for manual translations without having to go through the different Qt tools and command chain required to manually support a skill GUI for a different language. As a GUI skill Developer, the only know-how you need is to add the translation calls to your skill QML files, Developers can get more information about how to add them here Internationalization and Localization with Qt Quick | Qt 6.3. The \u201cTLDR\u201d version is that for every hard-coded string in your QML UI skill file you need to decorate your strings with the qsTr() decorator and your model list elements with the QT_TR_NOOP() decorator. Open Linguistika when installed by default on your distribution by choice, currently supports 6 European languages and 2 auto-translation providers. The tool provides extensibility through its JSON configuration interface to add more language support, where using a simple JSON language addition mechanism you can extend the tool to support a number of additional languages you would like to support for your skills UI. You can read more about adding additional languages on the tool\u2019s GitHub repository. How-To-Use Demo:","title":"Open Linguistika"},{"location":"manjaro/","text":"ovos-image-arch-recipe Make a manjaro based OpenVoiceOS image source code: https://github.com/OpenVoiceOS/ovos-image-arch-recipe/ Building Docker Automated Image Building The included Dockerfile can be used to build a default image in a Docker environment. The following dependencies must be installed on the build system before running the container: chroot qemu-user-static First, create the Docker container: docker build . -t ovos-image-builder Then, run the container to create a OVOS Image. Set CORE_REF to the branch of ovos-core that you want to build and RECIPE_REF to the branch of ovos-image-recipe you want to use. Set MAKE_THREADS to the number of threads to use for make processes. docker run \\ -v /home/${USER}/output:/output:rw \\ -v /run/systemd/resolve:/run/systemd/resolve \\ -e CORE_REF=${CORE_REF:-dev} \\ -e RECIPE_REF=${RECIPE_REF:-master} \\ -e MAKE_THREADS=${MAKE_THREADS:-4} \\ --privileged \\ --network=host \\ --name=ovos-image-builder \\ ovos-image-builder The entire build process will generally take several hours; it takes 1-2 hours on a build server with 2x Xeon Gold 5118 CPUs (48T Total). Interactive Image Building The scripts in the automation directory are available to help automate building a default image. For building an image interactively: bash automation/prepare.sh bash /tmp/run_scripts.sh The below documentation describes how to manually build an image using the individual scripts in this repository. Steps Getting Started The scripts and overlay files in this repository are designed to be applied to a base image as the root user. It is recommended to apply these scripts to a clean base image. Instructions are available at opensource.com . Note : The GUI shell is not installable under some base images For each step except boot_overlay , the directory corresponding to the step should be copied to the mounted image and the script run from a terminal chroot -ed to the image. If running scripts from a booted image, they should be run as root . Preparation From the host system where this repository is cloned, running prepare.sh <base_image> will copy boot overlay files, mount the image, mount DNS resolver config from the host system, copy all other image overlay files to /tmp , and chroot into the image. From here, you can run any/all of the following scripts to prepare the image before cleaning up core_configuration Configures user accounts and base functionality for RPi. ovos user is created with proper permissions here. At this stage, a booted image should resize its file system to fill the drive it is flashed to. Local login and ssh connections should use ovos / ovos to authenticate and be prompted to change password on login. network_manager Adds Balena wifi-connect to enable a portal for connecting the Pi device to a Wi-Fi network. A booted image will now be ready to connect to a network via SSID OVOS . sj201 For SJ201 board support, the included script will build/install drivers, add required overlays, install required system packages, and add a systemd service to flash the SJ201 chip on boot. This will modify pulseaudio and potentially overwrite any previous settings. Note: Running this scripts grants GPIO permissions to the gpio group. Any user that interfaces with the SJ201 board should be a member of the gpio group. Group permissions are not modified by this script Audio devices should now show up with pactl list . Audio devices can be tested in the image by recording a short audio clip and playing it back. parecord test.wav paplay test.wav embedded_shell Installs ovos-shell and mycroft-gui-app. Adds and enables ovos-gui.service to start the shell on system boot. The image should now boot to the GUI shell. ovos_core Installs ovos-core and dependencies. Configures services for core modules. At this stage, the image is complete and when booted should start OVOS. dashboard Installs the OVOS Dashboard and service to start the dashboard from the GUI. From the GUI Settings -> Developer Settings menu, Enable Dashboard will now start the dashboard for remote access to device diagnostics. camera Installs libcamera and other dependencies for using a CSI camera. The default camera skill can be used to take a photo; libcamera-apps are also installed for testing via CLI. splash_screen Enables a custom splash screen and disables on-device TTY at boot. On boot, a static image should be shown until the GUI Shell starts. Clean Up cleanup.sh removes any temporary files from the mounted image before unmounting it. After running cleanup.sh , the image is ready to burn to a drive and boot.","title":"ovos-image-arch-recipe"},{"location":"manjaro/#ovos-image-arch-recipe","text":"Make a manjaro based OpenVoiceOS image source code: https://github.com/OpenVoiceOS/ovos-image-arch-recipe/","title":"ovos-image-arch-recipe"},{"location":"manjaro/#building","text":"","title":"Building"},{"location":"manjaro/#docker-automated-image-building","text":"The included Dockerfile can be used to build a default image in a Docker environment. The following dependencies must be installed on the build system before running the container: chroot qemu-user-static First, create the Docker container: docker build . -t ovos-image-builder Then, run the container to create a OVOS Image. Set CORE_REF to the branch of ovos-core that you want to build and RECIPE_REF to the branch of ovos-image-recipe you want to use. Set MAKE_THREADS to the number of threads to use for make processes. docker run \\ -v /home/${USER}/output:/output:rw \\ -v /run/systemd/resolve:/run/systemd/resolve \\ -e CORE_REF=${CORE_REF:-dev} \\ -e RECIPE_REF=${RECIPE_REF:-master} \\ -e MAKE_THREADS=${MAKE_THREADS:-4} \\ --privileged \\ --network=host \\ --name=ovos-image-builder \\ ovos-image-builder The entire build process will generally take several hours; it takes 1-2 hours on a build server with 2x Xeon Gold 5118 CPUs (48T Total).","title":"Docker Automated Image Building"},{"location":"manjaro/#interactive-image-building","text":"The scripts in the automation directory are available to help automate building a default image. For building an image interactively: bash automation/prepare.sh bash /tmp/run_scripts.sh The below documentation describes how to manually build an image using the individual scripts in this repository.","title":"Interactive Image Building"},{"location":"manjaro/#steps","text":"","title":"Steps"},{"location":"manjaro/#getting-started","text":"The scripts and overlay files in this repository are designed to be applied to a base image as the root user. It is recommended to apply these scripts to a clean base image. Instructions are available at opensource.com . Note : The GUI shell is not installable under some base images For each step except boot_overlay , the directory corresponding to the step should be copied to the mounted image and the script run from a terminal chroot -ed to the image. If running scripts from a booted image, they should be run as root .","title":"Getting Started"},{"location":"manjaro/#preparation","text":"From the host system where this repository is cloned, running prepare.sh <base_image> will copy boot overlay files, mount the image, mount DNS resolver config from the host system, copy all other image overlay files to /tmp , and chroot into the image. From here, you can run any/all of the following scripts to prepare the image before cleaning up","title":"Preparation"},{"location":"manjaro/#core_configuration","text":"Configures user accounts and base functionality for RPi. ovos user is created with proper permissions here. At this stage, a booted image should resize its file system to fill the drive it is flashed to. Local login and ssh connections should use ovos / ovos to authenticate and be prompted to change password on login.","title":"core_configuration"},{"location":"manjaro/#network_manager","text":"Adds Balena wifi-connect to enable a portal for connecting the Pi device to a Wi-Fi network. A booted image will now be ready to connect to a network via SSID OVOS .","title":"network_manager"},{"location":"manjaro/#sj201","text":"For SJ201 board support, the included script will build/install drivers, add required overlays, install required system packages, and add a systemd service to flash the SJ201 chip on boot. This will modify pulseaudio and potentially overwrite any previous settings. Note: Running this scripts grants GPIO permissions to the gpio group. Any user that interfaces with the SJ201 board should be a member of the gpio group. Group permissions are not modified by this script Audio devices should now show up with pactl list . Audio devices can be tested in the image by recording a short audio clip and playing it back. parecord test.wav paplay test.wav","title":"sj201"},{"location":"manjaro/#embedded_shell","text":"Installs ovos-shell and mycroft-gui-app. Adds and enables ovos-gui.service to start the shell on system boot. The image should now boot to the GUI shell.","title":"embedded_shell"},{"location":"manjaro/#ovos_core","text":"Installs ovos-core and dependencies. Configures services for core modules. At this stage, the image is complete and when booted should start OVOS.","title":"ovos_core"},{"location":"manjaro/#dashboard","text":"Installs the OVOS Dashboard and service to start the dashboard from the GUI. From the GUI Settings -> Developer Settings menu, Enable Dashboard will now start the dashboard for remote access to device diagnostics.","title":"dashboard"},{"location":"manjaro/#camera","text":"Installs libcamera and other dependencies for using a CSI camera. The default camera skill can be used to take a photo; libcamera-apps are also installed for testing via CLI.","title":"camera"},{"location":"manjaro/#splash_screen","text":"Enables a custom splash screen and disables on-device TTY at boot. On boot, a static image should be shown until the GUI Shell starts.","title":"splash_screen"},{"location":"manjaro/#clean-up","text":"cleanup.sh removes any temporary files from the mounted image before unmounting it. After running cleanup.sh , the image is ready to burn to a drive and boot.","title":"Clean Up"},{"location":"mycroft/","text":"ovos-core vs mycroft-core Speech Client Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module Audio Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify Skills Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching Hardware Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core Misc Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat","title":"OVOS vs Mycroft"},{"location":"mycroft/#ovos-core-vs-mycroft-core","text":"","title":"ovos-core vs mycroft-core"},{"location":"mycroft/#speech-client","text":"Feature Mycroft OVOS Description Wake Word (listen) yes yes Only transcribe speech (STT) after a certain word is spoken Wake Up Word (sleep mode) yes yes When in sleep mode only listen for \"wake up\" (no STT) Hotword (bus event) no yes Emit bus events when a hotword is detected (no STT) Multiple Wake Words no yes Load multiple hotword engines/models simultaneously Fallback STT no yes fallback STT if the main one fails (eg, internet outage) Instant Listen no yes Do not pause between wake word detection and recording start Hybrid Listen no WIP Do not require wake word for follow up questions Continuous Listen no WIP Do not require wake word, always listen using VAD Recording mode no WIP Save audio instead of processing speech Wake Word Plugins yes yes Supports 3rd party integrations for hotword detection STT Plugins yes yes Supports 3rd party integrations for STT VAD plugins no * yes Supports 3rd party integrations for voice activity detection NOTES: HiveMind Voice Satellite uses ovos-core and supports the same features Pyaudio has a bug in python 3.10, you may need to use this fork (ovos-core and mk2 only) VAD is supported in mycroft mark2 branch, but is hardcoded for silero Sleep mode loop has been rewritten in ovos-core and is much more responsive than mycroft Mic handling logic has been ported from mk2 branch and is much more responsive than mycroft dev branch Instant / Hybrid / Continuous listen settings are experimental, good microphone and AEC are highly recommended (such as a mark2) in ovos-core this functionality has been refactored and moved to the new mycroft.listener module","title":"Speech Client"},{"location":"mycroft/#audio","text":"Feature Mycroft OVOS Description MPRIS integration no yes Integrate with MPRIS protocol NOTES: OCP can be used with mycroft-core, but not mk2 OCP can be controlled via MPRIS, e.g. KDEConnect OCP can control MPRIS enabled players, e.g. spotify","title":"Audio"},{"location":"mycroft/#skills","text":"Feature Mycroft OVOS Description Skill Plugins no yes skills can be packaged like standard python projects and installed via setup.py (eg. with pip or your package manager) User Resources no yes Users can override resource files, eg. customize dialogs for installed skills Skill permissions no WIP Users can limit converse and fallback functionality per skill and configure the order in which skills are executed Intent Plugins no WIP Supports 3rd party integrations for Intent Matching","title":"Skills"},{"location":"mycroft/#hardware","text":"Feature Mycroft OVOS Description System Plugins no yes Support for 3rd party hardware (eg. mk2-plugin ) and OS level integrations (eg. wifi-setup ) NOTES: PHAL can be used with mycroft-core","title":"Hardware"},{"location":"mycroft/#misc","text":"Feature Mycroft OVOS Description Offline usage no yes Can be configured to work without internet connectivity MultiLingual no WIP Can be configured to work in multiple languages at the same time HiveMind support WIP WIP Supports HiveMind for a distributed/remote mycroft experience XDG compliance WIP yes All resources respect XDG standards and support multiple users Usage as a lib no yes Packaged as a library, supports derivative voice assistants NOTES: HiveMind is being developed against ovos-core, development under mycroft-core is stalled, see the hivemind wiki for details XDG support includes multiple skill directories, all skill data, all configuration files, and all cached files You can build your own assistant on top of ovos-core, multiple assistants can co-exist in the same machine and use their own configuration files , ovos-core is packaged like a regular python package and can be handled as a requirement by package managers examples projects: neon-core , hivemind-voice-sat","title":"Misc"},{"location":"osm/","text":"OVOS skills manager Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi Supported stores OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills Install pip install ovos-skills-manager Usage osm provides a few command line utilities, explained below Install Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit. Enable Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit. Disable Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit. Sync Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit. Priority Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit. Print config print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit. Search Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit.","title":"Skills Manager"},{"location":"osm/#ovos-skills-manager","text":"Install skills from any appstore! The mycroft-skills-manager alternative that is not vendor locked , this means you must use it responsibly! Do not install random skills, different appstores have different policies! Keep in mind any skill you install can modify mycroft-core at runtime , and very likely has root access if you are running on a raspberry pi","title":"OVOS skills manager"},{"location":"osm/#supported-stores","text":"OVOS - this one is really a proof of concept for now, stay tuned! Mycroft Marketplace - the official mycroft skills store, all skills are reviewed by humans! Pling - the official plasma bigscreen skills store, skills are accepted by default and only removed if flagged as malicious Andlo's skill list - not a real appstore, this is a web scrapped automatically generated list of 900+ skills from all over github, there is no review at all, it will catch malicious skills","title":"Supported stores"},{"location":"osm/#install","text":"pip install ovos-skills-manager","title":"Install"},{"location":"osm/#usage","text":"osm provides a few command line utilities, explained below","title":"Usage"},{"location":"osm/#install_1","text":"Install a mycroft skill! Either pass a search query or a github url (.venv) user@hostname:~$ osm install --help Usage: osm install [OPTIONS] Options: --skill TEXT skill to install --branch TEXT select skill github branch to use --folder TEXT path where skill will be installed, default /opt/mycroft/skills --search search appstores, otherwise assume it's a github url --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, default search appstores enabled in config file --method [all|name|url|category|author|tag|description] match this metadata field when searching --fuzzy / --exact exact or fuzzy matching, default fuzzy --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match), default 80 --no-ignore-case ignore upper/lower case, default ignore --help Show this message and exit.","title":"Install"},{"location":"osm/#enable","text":"Enable a new skills store (.venv) user@hostname:~$ osm enable --help Usage: osm enable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] enable a specific appstore --help Show this message and exit.","title":"Enable"},{"location":"osm/#disable","text":"Disable a skills store (.venv) user@hostname:~$ osm disable --help Usage: osm disable [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all] disable a specific appstore --help Show this message and exit.","title":"Disable"},{"location":"osm/#sync","text":"Sync skill list for a skills store Suggestion: set a cronjob for this (.venv) user@hostname:~$ osm sync --help Usage: osm sync [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|default|all] sync a specific appstore, default syncs appstores enabled in config file --rebuild rebuild skill database, if not set only sync data for new skills --merge merge skill data, if not set replaces skill entries --github augment skill data from github, by default only saves data provided directly by the appstore --help Show this message and exit.","title":"Sync"},{"location":"osm/#priority","text":"Change priority of a skills store, this will affect order of results and have impact in the OSM-skill (coming soon) (.venv) user@hostname:~$ osm priority --help Usage: osm priority [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo] change priority of a specific appstore --priority INTEGER RANGE appstore priority, from 0 (highest) to 100 (lowest) --help Show this message and exit.","title":"Priority"},{"location":"osm/#print-config","text":"print current configuration of osm, config file can be found at ~/.config/OpenVoiceOS/OVOS-SkillsManager.json (.venv) user@hostname:~$ osm print --help Usage: osm print [OPTIONS] Options: --appstore [ovos|mycroft|pling|andlo|all|default] print config of a specific appstore --help Show this message and exit.","title":"Print config"},{"location":"osm/#search","text":"Search skills and print results, searching can be done according any number of criteria, this is useful for discovery (.venv) user@hostname:~$ osm search --help Usage: osm search [OPTIONS] Options: --query TEXT Search a skill with this query --method [all|name|url|category|author|tag|description] match this metadata field when searching --appstore [ovos|mycroft|pling|andlo|default|all] search a specific appstore, by default searches appstores enabled in config file --fuzzy / --exact exact or fuzzy matching --thresh INTEGER RANGE fuzzy matching threshold from 0 (everything is a match) to 100 (exact match) --no-ignore-case ignore upper/lower case --help Show this message and exit.","title":"Search"},{"location":"padacioso/","text":"Padacioso A lightweight, dead-simple intent parser Built on top of simplematch , inspired by Padaos Example from padacioso import IntentContainer container = IntentContainer() ## samples container.add_intent('hello', ['hello', 'hi', 'how are you', \"what's up\"]) ## \"optionally\" syntax container.add_intent('hello world', [\"hello [world]\"]) ## \"one_of\" syntax container.add_intent('greeting', [\"(hi|hey|hello)\"]) ## entity extraction container.add_intent('buy', [ 'buy {item}', 'purchase {item}', 'get {item}', 'get {item} for me' ]) container.add_intent('search', [ 'search for {query} on {engine}', 'using {engine} (search|look) for {query}', 'find {query} (with|using) {engine}' ]) container.add_entity('engine', ['abc', 'xyz']) container.calc_intent('find cats using xyz') # {'conf': 1.0, 'name': 'search', 'entities': {'query': 'cats', 'engine': 'xyz'}} ## wildcards syntax container.add_intent('say', [\"say *\"]) container.calc_intent('say something, whatever') # {'conf': 0.85, 'entities': {}, 'name': 'test'} ## typed entities syntax container.add_intent('pick_number', ['* number {number:int}']) container.calc_intent('i want number 3') # {'conf': 0.85, 'entities': {'number': 3}, 'name': 'pick_number'})","title":"Padacioso"},{"location":"padacioso/#padacioso","text":"A lightweight, dead-simple intent parser Built on top of simplematch , inspired by Padaos","title":"Padacioso"},{"location":"padacioso/#example","text":"from padacioso import IntentContainer container = IntentContainer() ## samples container.add_intent('hello', ['hello', 'hi', 'how are you', \"what's up\"]) ## \"optionally\" syntax container.add_intent('hello world', [\"hello [world]\"]) ## \"one_of\" syntax container.add_intent('greeting', [\"(hi|hey|hello)\"]) ## entity extraction container.add_intent('buy', [ 'buy {item}', 'purchase {item}', 'get {item}', 'get {item} for me' ]) container.add_intent('search', [ 'search for {query} on {engine}', 'using {engine} (search|look) for {query}', 'find {query} (with|using) {engine}' ]) container.add_entity('engine', ['abc', 'xyz']) container.calc_intent('find cats using xyz') # {'conf': 1.0, 'name': 'search', 'entities': {'query': 'cats', 'engine': 'xyz'}} ## wildcards syntax container.add_intent('say', [\"say *\"]) container.calc_intent('say something, whatever') # {'conf': 0.85, 'entities': {}, 'name': 'test'} ## typed entities syntax container.add_intent('pick_number', ['* number {number:int}']) container.calc_intent('i want number 3') # {'conf': 0.85, 'entities': {'number': 3}, 'name': 'pick_number'})","title":"Example"},{"location":"personal_backend/","text":"OVOS Personal Backend Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers :warning: there are no user accounts :warning: This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network For a full backend experience, the official mycroft backend has been open sourced, read the blog post NOTE: There is no pairing, devices will just activate themselves and work Install from pip pip install ovos-local-backend Mycroft Setup There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } } Companion projects ovos-backend-client - reference python library to interact with selene/local backend ovos-backend-manager - graphical interface to manage all things backend ovos-stt-plugin-selene - stt plugin for selene/local backend Usage start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host Docker There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux) How it works Configuration configure backend by editing/creating ~/.config/json_database/ovos_backend.json see default values here { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances Databases Since the local backend is not meant to provision hundreds of devices or manage user accounts it works only with json databases metadata about uploaded wakewords can be found at ~/.local/share/json_database/ovos_wakewords.jsondb metadata about uploaded utterances can be found at ~/.local/share/json_database/ovos_utterances.jsondb database of uploaded metrics can be found at ~/.local/share/json_database/ovos_metrics.jsondb paired devices database can be found at ~/.local/share/json_database/ovos_devices.json per device skill settings database can be found at ~/.local/share/json_database/ovos_skill_settings.json shared skill settings database can be found at ~/.local/share/json_database/ovos_shared_skill_settings.json metrics, wake words and utterances respect the individual devices opt_in flag, nothing will be saved unless devices opt_in (default True) Device Settings Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api you can also change these settings per device by manually editing paired devices database Location Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database Device Preferences Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api you can also set these values per device by manually editing paired devices database Skill settings in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database Email Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps Selene Proxy You can integrate local backend with selene, the backend will show up as a device you can manage in mycroft.home wait... what? isn't the point of local backend to disable selene? Open Dataset, You do not want to use selene, but you want to opt_in to the open dataset (share recordings with mycroft) Privacy, you want to use selene, but you do not want to give away your personal data (email, location, ip address...) Control, you want to use only a subset of selene features Convenience, pair once, manage all your devices Functionality, extra features such as isolated skill settings and forced 2 way sync Esoteric Setups, isolated mycroft services that can not share an identity file, such as ovos-qubes Pairing To pair the local backend with selene you have 2 options 1 - pair a mycroft-core instance, then copy the identity file 2 - enable proxy_pairing, whenever a device pairs with local backend the code it speaks is also valid for selene, use that code to pair local backend with selene If a device tries to use a selene enabled endpoint without the backend being paired a 401 authentication error will be returned, if the endpoint does not use selene (e.g. disabled in config) this check is skipped Selene Config In your backend config add the following section \"selene\": { \"enabled\": False, # needs to be explicitly enabled by user \"url\": \"https://api.mycroft.ai\", # change if you are self-hosting selene \"version\": \"v1\", # pairing settings # NOTE: the file should be used exclusively by backend, do not share with a mycroft-core instance \"identity_file\": BACKEND_IDENTITY, # path to identity2.json file # send the pairing from selene to any device that attempts to pair with local backend # this will provide voice/gui prompts to the user and avoid the need to copy an identity file # only happens if backend is not paired with selene (hopefully exactly once) # if False you need to pair an existing mycroft-core as usual and move the file for backend usage \"proxy_pairing\": False, # micro service settings # NOTE: STT is handled at plugin level, configure ovos-stt-plugin-selene \"proxy_weather\": True, # use selene for weather api calls \"proxy_wolfram\": True, # use selene for wolfram alpha api calls \"proxy_geolocation\": True, # use selene for geolocation api calls \"proxy_email\": False, # use selene for sending email (only for email registered in selene) # device settings - if you want to spoof data in selene set these to False \"download_location\": True, # set default location from selene \"download_prefs\": True, # set default device preferences from selene \"download_settings\": True, # download shared skill settings from selene \"upload_settings\": True, # upload shared skill settings to selene \"force2way\": False, # this forcefully re-enables 2way settings sync with selene # this functionality was removed from core, we hijack the settingsmeta endpoint to upload settings # upload will happen when mycroft-core boots and overwrite any values in selene (no checks for settings changed) # the assumption is that selene changes are downloaded instantaneously # if a device is offline when selene changes those changes will be discarded on next device boot # opt-in settings - what data to share with selene # NOTE: these also depend on opt_in being set in selene \"opt_in\": False, # share data from all devices with selene (as if from a single device) \"opt_in_blacklist\": [], # list of uuids that should ignore opt_in flag (never share data) \"upload_metrics\": True, # upload device metrics to selene \"upload_wakewords\": True, # upload wake word samples to selene \"upload_utterances\": True # upload utterance samples to selene } Project Timeline Jan 2018 - initial release of reverse engineered mycroft backend - by JarbasAI July 2018 - Personal backend added to Mycroft Roadmap October 2018 - Community involved in discussion Jan 2019 - JarbasAI implementation adopted by Mycroft NOTE: this should have been a fork or repository transferred, but was a bare clone Original repository was archived October 2019 - Official mycroft backend open sourced under a viral license Jun 2020 - original project repurposed to be a mock backend instead of a full alternative, skill-mock-backend released Jan 2021 - mock-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred","title":"Personal Backend"},{"location":"personal_backend/#ovos-personal-backend","text":"Personal mycroft backend alternative to mycroft.home, written in flask This repo is an alternative to the backend meant for personal usage, this allows you to run without mycroft servers :warning: there are no user accounts :warning: This is NOT meant to provision third party devices, but rather to run on the mycroft devices directly or on a private network For a full backend experience, the official mycroft backend has been open sourced, read the blog post NOTE: There is no pairing, devices will just activate themselves and work","title":"OVOS Personal Backend"},{"location":"personal_backend/#install","text":"from pip pip install ovos-local-backend","title":"Install"},{"location":"personal_backend/#mycroft-setup","text":"There are 2 main intended ways to run local backend with mycroft on same device as mycroft-core, tricking it to run without mycroft servers on a private network, to manage all your devices locally NOTE: you can not fully run mycroft-core offline, it refuses to launch without internet connection, you can only replace the calls to use this backend instead of mycroft.home We recommend you use ovos-core instead update your mycroft config to use this backend, delete identity2.json and restart mycroft { \"server\": { \"url\": \"http://0.0.0.0:6712\", \"version\": \"v1\", \"update\": true, \"metrics\": true }, \"listener\": { \"wake_word_upload\": { \"url\": \"http://0.0.0.0:6712/precise/upload\" } } }","title":"Mycroft Setup"},{"location":"personal_backend/#companion-projects","text":"ovos-backend-client - reference python library to interact with selene/local backend ovos-backend-manager - graphical interface to manage all things backend ovos-stt-plugin-selene - stt plugin for selene/local backend","title":"Companion projects"},{"location":"personal_backend/#usage","text":"start backend $ ovos-local-backend -h usage: ovos-local-backend [-h] [--flask-port FLASK_PORT] [--flask-host FLASK_HOST] optional arguments: -h, --help show this help message and exit --flask-port FLASK_PORT Mock backend port number --flask-host FLASK_HOST Mock backend host","title":"Usage"},{"location":"personal_backend/#docker","text":"There is also a docker container you can use docker run -p 8086:6712 -d --restart always --name local_backend ghcr.io/openvoiceos/local-backend:dev a docker-compose.yml could look like this version: '3.6' services: # ... ovosbackend: container_name: ovos_backend image: ghcr.io/openvoiceos/local-backend:dev # or build from local source (relative to docker-compose.yml) # build: ../ovos/ovos-personal-backend/. restart: unless-stopped ports: - \"6712:6712\" # default port backend API - \"36535:36535\" # default port backend-manager volumes: # <host>:<guest>:<SELinux flag> - ./ovos/backend/config:/root/.config/json_database:z # shared config directory - ./ovos/backend/data:/root/.local/share/ovos_backend:Z # shared data directory # set `data_path` to `/root/.local/share/ovos_backend` about selinux flags (omit if you don't deal with selinux)","title":"Docker"},{"location":"personal_backend/#how-it-works","text":"","title":"How it works"},{"location":"personal_backend/#configuration","text":"configure backend by editing/creating ~/.config/json_database/ovos_backend.json see default values here { \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, \"backend_port\": 6712, \"geolocate\": true, \"override_location\": false, \"api_version\": \"v1\", \"data_path\": \"~\", \"record_utterances\": false, \"record_wakewords\": false, \"wolfram_key\": \"$KEY\", \"owm_key\": \"$KEY\", \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\", \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } stt config follows the same format of mycroft.conf and uses ovos-plugin-manager set wolfram alpha key for wolfram alpha proxy expected by official mycroft skill set open weather map key for weather proxy expected by official mycroft skill if record_wakewords is set, recordings can be found at DATA_PATH/wakewords if record_utterances is set, recordings can be found at DATA_PATH/utterances","title":"Configuration"},{"location":"personal_backend/#databases","text":"Since the local backend is not meant to provision hundreds of devices or manage user accounts it works only with json databases metadata about uploaded wakewords can be found at ~/.local/share/json_database/ovos_wakewords.jsondb metadata about uploaded utterances can be found at ~/.local/share/json_database/ovos_utterances.jsondb database of uploaded metrics can be found at ~/.local/share/json_database/ovos_metrics.jsondb paired devices database can be found at ~/.local/share/json_database/ovos_devices.json per device skill settings database can be found at ~/.local/share/json_database/ovos_skill_settings.json shared skill settings database can be found at ~/.local/share/json_database/ovos_shared_skill_settings.json metrics, wake words and utterances respect the individual devices opt_in flag, nothing will be saved unless devices opt_in (default True)","title":"Databases"},{"location":"personal_backend/#device-settings","text":"Each paired device has a few settings that control behaviour backend side name - default \"Device-{uuid}\" , friendly device name for display opt_in - default True , flag to control if metrics and speech from this device will be saved device_location - default \"unknown\" , friendly name for indoor location email - default from backend config, email to send notifications to isolated_skills - default False , flag to control if skill settings are shared across devices (ovos only) In selene this info would be populated during pairing process, in local backend it needs to be updated manually you can change these settings per device via the admin api you can also change these settings per device by manually editing paired devices database","title":"Device Settings"},{"location":"personal_backend/#location","text":"Device location can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"geolocate\": true, \"override_location\": false, \"default_location\": { \"city\": {\"...\": \"...\"}, \"coordinate\": {\"...\": \"...\"}, \"timezone\": {\"...\": \"...\"} } } if override location is True, then location will be set to configured default value if geolocate is True then location will be set from your ip address you can set a default location per device via the admin api you can also set a default location per device by manually editing paired devices database","title":"Location"},{"location":"personal_backend/#device-preferences","text":"Some settings can be updated via the backend, mycroft-core will request this info on its own from time to time default values comes from the local backend config file { \"lang\": \"en-us\", \"date_format\": \"DMY\", \"system_unit\": \"metric\", \"time_format\": \"full\" } these settings are also used for wolfram alpha / weather default values you can set these values per device via the admin api you can also set these values per device by manually editing paired devices database","title":"Device Preferences"},{"location":"personal_backend/#skill-settings","text":"in selene all device share skill settings, with local backend you can control this per device via isolated_skills flag \"old selene\" supported a single endpoint for both skill settings and settings meta, this allowed devices both to download and upload settings \"new selene\" split this into two endpoints, settingsMeta (upload only) and settings (download only), this disabled two-way sync across devices you can set isolated_skills per device via the admin api you can also set isolated_skills per device by manually editing paired devices database both endpoints are available, but mycroft-core by default will use the new endpoints and does not support two-way sync you can edit settings by using the \"old selene\" endpoint you can also edit settings by manually editing settings database","title":"Skill settings"},{"location":"personal_backend/#email","text":"Mycroft skills can request the backend to send an email to the account used for pairing the device Email will be sent to a pre-defined recipient email since there are no user accounts you can set a recipient email per device via the admin api you can set a recipient email per device by manually editing paired devices database with the local backend you need to configure your own SMTP server and recipient email, add the following section to your .conf { \"email\": { \"smtp\": { \"username\": \"sender@gmail.com\", \"password\": \"123456\", \"host\": \"\", \"port\": 465 }, \"recipient\": \"receiver@gmail.com\" } } If using gmail you will need to enable less secure apps","title":"Email"},{"location":"personal_backend/#selene-proxy","text":"You can integrate local backend with selene, the backend will show up as a device you can manage in mycroft.home wait... what? isn't the point of local backend to disable selene? Open Dataset, You do not want to use selene, but you want to opt_in to the open dataset (share recordings with mycroft) Privacy, you want to use selene, but you do not want to give away your personal data (email, location, ip address...) Control, you want to use only a subset of selene features Convenience, pair once, manage all your devices Functionality, extra features such as isolated skill settings and forced 2 way sync Esoteric Setups, isolated mycroft services that can not share an identity file, such as ovos-qubes","title":"Selene Proxy"},{"location":"personal_backend/#pairing","text":"To pair the local backend with selene you have 2 options 1 - pair a mycroft-core instance, then copy the identity file 2 - enable proxy_pairing, whenever a device pairs with local backend the code it speaks is also valid for selene, use that code to pair local backend with selene If a device tries to use a selene enabled endpoint without the backend being paired a 401 authentication error will be returned, if the endpoint does not use selene (e.g. disabled in config) this check is skipped","title":"Pairing"},{"location":"personal_backend/#selene-config","text":"In your backend config add the following section \"selene\": { \"enabled\": False, # needs to be explicitly enabled by user \"url\": \"https://api.mycroft.ai\", # change if you are self-hosting selene \"version\": \"v1\", # pairing settings # NOTE: the file should be used exclusively by backend, do not share with a mycroft-core instance \"identity_file\": BACKEND_IDENTITY, # path to identity2.json file # send the pairing from selene to any device that attempts to pair with local backend # this will provide voice/gui prompts to the user and avoid the need to copy an identity file # only happens if backend is not paired with selene (hopefully exactly once) # if False you need to pair an existing mycroft-core as usual and move the file for backend usage \"proxy_pairing\": False, # micro service settings # NOTE: STT is handled at plugin level, configure ovos-stt-plugin-selene \"proxy_weather\": True, # use selene for weather api calls \"proxy_wolfram\": True, # use selene for wolfram alpha api calls \"proxy_geolocation\": True, # use selene for geolocation api calls \"proxy_email\": False, # use selene for sending email (only for email registered in selene) # device settings - if you want to spoof data in selene set these to False \"download_location\": True, # set default location from selene \"download_prefs\": True, # set default device preferences from selene \"download_settings\": True, # download shared skill settings from selene \"upload_settings\": True, # upload shared skill settings to selene \"force2way\": False, # this forcefully re-enables 2way settings sync with selene # this functionality was removed from core, we hijack the settingsmeta endpoint to upload settings # upload will happen when mycroft-core boots and overwrite any values in selene (no checks for settings changed) # the assumption is that selene changes are downloaded instantaneously # if a device is offline when selene changes those changes will be discarded on next device boot # opt-in settings - what data to share with selene # NOTE: these also depend on opt_in being set in selene \"opt_in\": False, # share data from all devices with selene (as if from a single device) \"opt_in_blacklist\": [], # list of uuids that should ignore opt_in flag (never share data) \"upload_metrics\": True, # upload device metrics to selene \"upload_wakewords\": True, # upload wake word samples to selene \"upload_utterances\": True # upload utterance samples to selene }","title":"Selene Config"},{"location":"personal_backend/#project-timeline","text":"Jan 2018 - initial release of reverse engineered mycroft backend - by JarbasAI July 2018 - Personal backend added to Mycroft Roadmap October 2018 - Community involved in discussion Jan 2019 - JarbasAI implementation adopted by Mycroft NOTE: this should have been a fork or repository transferred, but was a bare clone Original repository was archived October 2019 - Official mycroft backend open sourced under a viral license Jun 2020 - original project repurposed to be a mock backend instead of a full alternative, skill-mock-backend released Jan 2021 - mock-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred","title":"Project Timeline"},{"location":"playing_music/","text":"Playing music For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list Youtube Music A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter. Play the news Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news Some more features that come out of the box The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing. Homescreen widget The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Playing music"},{"location":"playing_music/#playing-music","text":"For playing music (and video as discussed within the next chapter), OpenVoiceOS uses OCP (OpenVoiceOS Common Play) and is basically a full fledge multimedia player on its own designed around open standards like MPRIS and with the vision of being fully integrated within the OpenVoiceOS software stack. Skills designed for OCP provide search results for OCP (think about them as media providers/catalogs/scrapers), OCP will play the best search result for you. OpenVoiceOS comes with a few OCP skills pre-installed, however more can be installed just like any other OVOS skill. You can find more OCP skills in the awesome-ocp-skills list","title":"Playing music"},{"location":"playing_music/#youtube-music","text":"A voiceassistant with smartspeaker functionality should be able to play music straight out of the box. For that reason the buildroot edition of OpenVoiceOS comes with the Youtube Music OCP Skill pre-installed. Just ask it to play something will start playback from Youtube assuming the asked sonmg is present on Youtube ofcourse. Hey Mycroft, play disturbed sound of silence This should just start playing utilizing OCP as shown below. More information about the full functionality of OCP can be found at its own chapter.","title":"Youtube Music"},{"location":"playing_music/#play-the-news","text":"Nothing more relaxing after you woke up, cancelling your alarm set on you OpenVoiceOS device than listening to your favorite news station while drinking some coffee (No OpenVoiceOS can not make you that coffee yet). Hey Mycroft, play the BBC news","title":"Play the news"},{"location":"playing_music/#some-more-features-that-come-out-of-the-box","text":"The whole OCP framework has some benefits and features that are not skill specific, such as \"Playlists\" and a view of the search results. You can access those by swiping to the right when something is playing.","title":"Some more features that come out of the box"},{"location":"playing_music/#homescreen-widget","text":"The homescreen skill that comes pre-installed with OpenVoiceOS also comes with a widget for the OCP framework.","title":"Homescreen widget"},{"location":"playing_video/","text":"Playing video Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"playing_video/#playing-video","text":"Although the screen used on your OpenVoiceOS device might be small, the whole OCP mediaplaying frame does support video playback. You can find video OCP skills in the same awesome-ocp-skills list. The fourth column, \"playback type\" shows which type of payer is used for that specific skill. If you use a skill that utilizes the \"video player\" the below will be shown on your OpenVoiceOS it's screen at playback.","title":"Playing video"},{"location":"prompts/","text":"Prompts Here we look at how to implement the most common types of prompts. For more information on conversation design see the Voice User Interface Design Guidelines . Get Response Any Skill can request a response from the user - making a statement or asking a question before the microphone is activated to record the User's response. The base implementation of this is the get_response() method . To see it in action, let's create a simple Skill that asks the User what their favorite flavor of ice cream is. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): @intent_handler('set.favorite.intent') def handle_set_favorite(self): favorite_flavor = self.get_response('what.is.your.favorite.flavor') self.speak_dialog('confirm.favorite.flavor', {'flavor': favorite_flavor}) def create_skill(): return IceCreamSkill() In this Skill we have used get_response() and passed it the name of our dialog file 'what.is.your.favorite.flavor.dialog' . This is the simplest form of this method. It will speak dialog from the given file, then activate the microphone for 3-10 seconds allowing the User to respond. The transcript of their response will then be assigned to our variable favorite_flavor . To confirm that we have heard the User correctly we then speak a confirmation dialog passing the value of favorite_flavor to be spoken as part of that dialog. Optional Arguments The get_response() method also takes the following optional arguments: data (dict) - used to populate the dialog file, just like speak_dialog() validator (function) - returns a boolean to define whether the response meets some criteria for success on_fail (function) - returns a string that will be spoken if the validator returns False num_retries (int) - number of times the system should repeat the question to get a successful result Yes / No Questions ask_yesno() checks if the response contains \"yes\" or \"no\" like phrases. The vocab for this check is sourced from the Skills yes.voc and no.voc files (if they exist), as well as mycroft-cores defaults (contained within mycroft-core/res/text/en-us/yes.voc ). A longer phrase containing the required vocab is considered successful e.g. both \"yes\" and \"yeah that would be great thanks\" would be considered a successful \"yes\". If \"yes\" or \"no\" responses are detected, then the method will return the string \"yes\" or \"no\". If the response does not contain \"yes\" or \"no\" vocabulary then the entire utterance will be returned. If no speech was detected indicating the User did not respond, then the method will return None . Let's add a new intent to our IceCreamSkill to see how this works. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): @intent_handler('do.you.like.intent') def handle_do_you_like(self): likes_ice_cream = self.ask_yesno('do.you.like.ice.cream') if likes_ice_cream == 'yes': self.speak_dialog('does.like') elif likes_ice_cream == 'no': self.speak_dialog('does.not.like') else: self.speak_dialog('could.not.understand') def create_skill(): return IceCreamSkill() In this example we have asked the User if they like ice cream. We then speak different dialog whether they respond yes or no. We also speak some error dialog if neither yes nor no are returned. Providing a list of options ask_selection() provides a list of options to the User for them to select from. The User can respond with either the name of one of these options or select with a numbered ordinal eg \"the third\". This method automatically manages fuzzy matching the users response against the list of options provided. Let's jump back into our IceCreamSkill to give the User a list of options to choose from. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) def create_skill(): return IceCreamSkill() In this example we first speak some welcome.dialog . The list of flavors is then spoken, followed by the what.flavor.dialog . Finally, we confirm the Users selection by speaking coming.right.up.dialog Optional arguments There are two optional arguments for this method. min_conf (float) defines the minimum confidence level for fuzzy matching the Users response against the list of options. numeric (bool) if set to True will speak the options as a numbered list eg \"One, vanilla. Two, chocolate. Or three, mint\" Returning responses to the intent parser So far we have looked at ways to prompt the User, and return their response directly to our Skill. It is also possible to speak some dialog, and activate the listener, directing the response back to the standard intent parsing engine. We may do this to let the user trigger another Skill, or because we want to make use of our own intents to handle the response. To do this, we use the expect_response parameter of the speak_dialog() method. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) self.speak_dialog('now.what', expect_response=True) def create_skill(): return IceCreamSkill() Here we have added a new dialog after confirming the Users selection. We may use it to tell the User other things they can do with their OVOS device while they enjoy their delicious ice cream.","title":"Prompts"},{"location":"prompts/#prompts","text":"Here we look at how to implement the most common types of prompts. For more information on conversation design see the Voice User Interface Design Guidelines .","title":"Prompts"},{"location":"prompts/#get-response","text":"Any Skill can request a response from the user - making a statement or asking a question before the microphone is activated to record the User's response. The base implementation of this is the get_response() method . To see it in action, let's create a simple Skill that asks the User what their favorite flavor of ice cream is. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): @intent_handler('set.favorite.intent') def handle_set_favorite(self): favorite_flavor = self.get_response('what.is.your.favorite.flavor') self.speak_dialog('confirm.favorite.flavor', {'flavor': favorite_flavor}) def create_skill(): return IceCreamSkill() In this Skill we have used get_response() and passed it the name of our dialog file 'what.is.your.favorite.flavor.dialog' . This is the simplest form of this method. It will speak dialog from the given file, then activate the microphone for 3-10 seconds allowing the User to respond. The transcript of their response will then be assigned to our variable favorite_flavor . To confirm that we have heard the User correctly we then speak a confirmation dialog passing the value of favorite_flavor to be spoken as part of that dialog.","title":"Get Response"},{"location":"prompts/#optional-arguments","text":"The get_response() method also takes the following optional arguments: data (dict) - used to populate the dialog file, just like speak_dialog() validator (function) - returns a boolean to define whether the response meets some criteria for success on_fail (function) - returns a string that will be spoken if the validator returns False num_retries (int) - number of times the system should repeat the question to get a successful result","title":"Optional Arguments"},{"location":"prompts/#yes-no-questions","text":"ask_yesno() checks if the response contains \"yes\" or \"no\" like phrases. The vocab for this check is sourced from the Skills yes.voc and no.voc files (if they exist), as well as mycroft-cores defaults (contained within mycroft-core/res/text/en-us/yes.voc ). A longer phrase containing the required vocab is considered successful e.g. both \"yes\" and \"yeah that would be great thanks\" would be considered a successful \"yes\". If \"yes\" or \"no\" responses are detected, then the method will return the string \"yes\" or \"no\". If the response does not contain \"yes\" or \"no\" vocabulary then the entire utterance will be returned. If no speech was detected indicating the User did not respond, then the method will return None . Let's add a new intent to our IceCreamSkill to see how this works. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): @intent_handler('do.you.like.intent') def handle_do_you_like(self): likes_ice_cream = self.ask_yesno('do.you.like.ice.cream') if likes_ice_cream == 'yes': self.speak_dialog('does.like') elif likes_ice_cream == 'no': self.speak_dialog('does.not.like') else: self.speak_dialog('could.not.understand') def create_skill(): return IceCreamSkill() In this example we have asked the User if they like ice cream. We then speak different dialog whether they respond yes or no. We also speak some error dialog if neither yes nor no are returned.","title":"Yes / No Questions"},{"location":"prompts/#providing-a-list-of-options","text":"ask_selection() provides a list of options to the User for them to select from. The User can respond with either the name of one of these options or select with a numbered ordinal eg \"the third\". This method automatically manages fuzzy matching the users response against the list of options provided. Let's jump back into our IceCreamSkill to give the User a list of options to choose from. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) def create_skill(): return IceCreamSkill() In this example we first speak some welcome.dialog . The list of flavors is then spoken, followed by the what.flavor.dialog . Finally, we confirm the Users selection by speaking coming.right.up.dialog","title":"Providing a list of options"},{"location":"prompts/#optional-arguments_1","text":"There are two optional arguments for this method. min_conf (float) defines the minimum confidence level for fuzzy matching the Users response against the list of options. numeric (bool) if set to True will speak the options as a numbered list eg \"One, vanilla. Two, chocolate. Or three, mint\"","title":"Optional arguments"},{"location":"prompts/#returning-responses-to-the-intent-parser","text":"So far we have looked at ways to prompt the User, and return their response directly to our Skill. It is also possible to speak some dialog, and activate the listener, directing the response back to the standard intent parsing engine. We may do this to let the user trigger another Skill, or because we want to make use of our own intents to handle the response. To do this, we use the expect_response parameter of the speak_dialog() method. from mycroft import MycroftSkill, intent_handler class IceCreamSkill(MycroftSkill): def __init__(self): MycroftSkill.__init__(self) self.flavors = ['vanilla', 'chocolate', 'mint'] @intent_handler('request.icecream.intent') def handle_request_icecream(self): self.speak_dialog('welcome') selection = self.ask_selection(self.flavors, 'what.flavor') self.speak_dialog('coming.right.up', {'flavor': selection}) self.speak_dialog('now.what', expect_response=True) def create_skill(): return IceCreamSkill() Here we have added a new dialog after confirming the Users selection. We may use it to tell the User other things they can do with their OVOS device while they enjoy their delicious ice cream.","title":"Returning responses to the intent parser"},{"location":"qml_guidelines/","text":"QML Design Guidelines An introduction to QML and additional documentation are available here Mycroft-GUI frameworks provides you with some base delegates you should use when designing your QML GUI. The base delegates provide you with a basic presentation layer for your skill with some property assignments that can help you set up background images, background dim to give you the control you need for rendering an experience. Before we dive deeper into the Design Guidelines, lets look at some concepts that a GUI developer should learn about: Units & Theming Units: Mycroft.Units.GridUnit is the fundamental unit of space that should be used for all sizing inside the QML UI, expressed in pixels. Each GridUnit is predefined as 16 pixels // Usage in QML Components example width: Mycroft.Units.gridUnit * 2 // 32px Wide height: Mycroft.Units.gridUnit // 16px Tall Theming: OVOS Shell uses a custom Kirigami Platform Theme plugin to provide global theming to all our skills and user interfaces, which also allows our GUI's to be fully compatible with the system themes on platforms that are not running the OVOS Shell. Kirigami Theme and Color Scheme guide is extensive and can be found here OVOS GUI's developed to follow the color scheme depend on only a subset of available colors, mainly: Kirigami.Theme.backgroundColor = Primary Color (Background Color: This will always be a dark palette or light palette depending on the dark or light chosen color scheme) Kirigami.Theme.highlightColor = Secondary Color (Accent Color: This will always be a standout palette that defines the themes dominating color and can be used for buttons, cards, borders, highlighted text etc.) Kirigami.Theme.textColor = Text Color (This will always be an opposite palette to the selected primary color) QML Delegate Design Best Practise Let's look at this image and qml example below, this is a representation of the Mycroft Delegate: When designing your first QML file, it is important to note the red triangles in the above image, these triangles represent the margin from the screen edge the GUI needs to be designed within, these margins ensure your GUI content does not overlap with features like edge lighting and menus in the platforms that support it like OVOS-Shell The content items and components all utilize the selected color scheme, where black is the primary background color, red is our accent color and white is our contrasting text color Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" anchors.top: parent.top anchors.left: parent.left anchors.right: parent.right height: Mycroft.Units.gridUnit * 3 // Setting the text color to always follow the color scheme for this item displayed on the screen color: Kirigami.Theme.textColor } PieChart { anchors.centerIn: parent pieColorMinor: Kirigami.Theme.backgroundColor // As in the image above the minor area of the pie chart uses our primary color pieColorMid: Kirigami.Theme.highlightColor // As in the image above the middle area is assigned the highlight or our accent color pieColorMajor: Kirigami.Theme.textColor // As in the image above the major area is assigned the text color } } } QML Delegate Multi Platform and Screen Guidelines OVOS Skill GUIs are designed to be multi-platform and screen friendly, to support this we always try to support both Horizontal and Vertical display's. Let's look at an example and a general approach to writing multi resolution friendly UI's Let's look at these images below that represent a Delegate as seen in a Horizontal screen: Let's look at these images below that represent a Delegate as seen in a Vertical screen: When designing for different screens it is preferred to utilize Grids, GridLayouts and GridViews this allows easier content placement as one can control the number of columns and rows displayed on the screen It is also recommended to use Flickables when you believe your content is going to not fit on the screen, this allows for content to always be scrollable. To make it easier to design scrollable content, Mycroft GUI provides you with a ready to use Mycroft.ScrollableDelegate. It is also preferred to use the width vs height comparison on the root delegate item to know when the screen should be using a vertical layout vs horizontal layout Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 property bool horizontalMode: width >= height ? 1 : 0 // Using a ternary operator to detect if width of the delegate is greater than the height, which provides if the delegate is in horizontalMode Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" // Setting the text color to always follow the color scheme color: Kirigami.Theme.textColor } GridLayout { id: examplesGridView // Checking if we are in horizontal mode, we should display two columns to display the items in the image above, or if we are in vertical mode, we should display a single column only columns: root.horizontalMode ? 2 : 1 Repeater { model: examplesModel delegates: ExamplesDelegate { ... } } } } }","title":"QML Design Guidelines"},{"location":"qml_guidelines/#qml-design-guidelines","text":"An introduction to QML and additional documentation are available here Mycroft-GUI frameworks provides you with some base delegates you should use when designing your QML GUI. The base delegates provide you with a basic presentation layer for your skill with some property assignments that can help you set up background images, background dim to give you the control you need for rendering an experience. Before we dive deeper into the Design Guidelines, lets look at some concepts that a GUI developer should learn about:","title":"QML Design Guidelines"},{"location":"qml_guidelines/#units-theming","text":"","title":"Units &amp; Theming"},{"location":"qml_guidelines/#units","text":"Mycroft.Units.GridUnit is the fundamental unit of space that should be used for all sizing inside the QML UI, expressed in pixels. Each GridUnit is predefined as 16 pixels // Usage in QML Components example width: Mycroft.Units.gridUnit * 2 // 32px Wide height: Mycroft.Units.gridUnit // 16px Tall","title":"Units:"},{"location":"qml_guidelines/#theming","text":"OVOS Shell uses a custom Kirigami Platform Theme plugin to provide global theming to all our skills and user interfaces, which also allows our GUI's to be fully compatible with the system themes on platforms that are not running the OVOS Shell. Kirigami Theme and Color Scheme guide is extensive and can be found here OVOS GUI's developed to follow the color scheme depend on only a subset of available colors, mainly: Kirigami.Theme.backgroundColor = Primary Color (Background Color: This will always be a dark palette or light palette depending on the dark or light chosen color scheme) Kirigami.Theme.highlightColor = Secondary Color (Accent Color: This will always be a standout palette that defines the themes dominating color and can be used for buttons, cards, borders, highlighted text etc.) Kirigami.Theme.textColor = Text Color (This will always be an opposite palette to the selected primary color)","title":"Theming:"},{"location":"qml_guidelines/#qml-delegate-design-best-practise","text":"Let's look at this image and qml example below, this is a representation of the Mycroft Delegate: When designing your first QML file, it is important to note the red triangles in the above image, these triangles represent the margin from the screen edge the GUI needs to be designed within, these margins ensure your GUI content does not overlap with features like edge lighting and menus in the platforms that support it like OVOS-Shell The content items and components all utilize the selected color scheme, where black is the primary background color, red is our accent color and white is our contrasting text color Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" anchors.top: parent.top anchors.left: parent.left anchors.right: parent.right height: Mycroft.Units.gridUnit * 3 // Setting the text color to always follow the color scheme for this item displayed on the screen color: Kirigami.Theme.textColor } PieChart { anchors.centerIn: parent pieColorMinor: Kirigami.Theme.backgroundColor // As in the image above the minor area of the pie chart uses our primary color pieColorMid: Kirigami.Theme.highlightColor // As in the image above the middle area is assigned the highlight or our accent color pieColorMajor: Kirigami.Theme.textColor // As in the image above the major area is assigned the text color } } }","title":"QML Delegate Design Best Practise"},{"location":"qml_guidelines/#qml-delegate-multi-platform-and-screen-guidelines","text":"OVOS Skill GUIs are designed to be multi-platform and screen friendly, to support this we always try to support both Horizontal and Vertical display's. Let's look at an example and a general approach to writing multi resolution friendly UI's Let's look at these images below that represent a Delegate as seen in a Horizontal screen: Let's look at these images below that represent a Delegate as seen in a Vertical screen: When designing for different screens it is preferred to utilize Grids, GridLayouts and GridViews this allows easier content placement as one can control the number of columns and rows displayed on the screen It is also recommended to use Flickables when you believe your content is going to not fit on the screen, this allows for content to always be scrollable. To make it easier to design scrollable content, Mycroft GUI provides you with a ready to use Mycroft.ScrollableDelegate. It is also preferred to use the width vs height comparison on the root delegate item to know when the screen should be using a vertical layout vs horizontal layout Let's look at this in QML: import ... import Mycroft 1.0 as Mycroft Mycroft.Delegate { id: root skillBackgroundSource: sessionData.exampleImage leftPadding: 0 rightPadding: 0 topPadding: 0 bottomPadding: 0 property bool horizontalMode: width >= height ? 1 : 0 // Using a ternary operator to detect if width of the delegate is greater than the height, which provides if the delegate is in horizontalMode Rectangle { anchors.fill: parent // Setting margins that need to be left for the screen edges anchors.margins: Mycroft.Units.gridUnit * 2 //Setting a background dim using our primary theme / background color on top of our skillBackgroundSource image for better readability and contrast color: Qt.rgba(Kirigami.Theme.backgroundColor.r, Kirigami.Theme.backgroundColor.g, Kirigami.Theme.backgroundColor.b, 0.3) Kirigami.Heading { level: 2 text: \"An Example Pie Chart\" // Setting the text color to always follow the color scheme color: Kirigami.Theme.textColor } GridLayout { id: examplesGridView // Checking if we are in horizontal mode, we should display two columns to display the items in the image above, or if we are in vertical mode, we should display a single column only columns: root.horizontalMode ? 2 : 1 Repeater { model: examplesModel delegates: ExamplesDelegate { ... } } } } }","title":"QML Delegate Multi Platform and Screen Guidelines"},{"location":"raspbian_ovos/","text":"raspbian-ovos Pre-built images are available at the OpenVoiceOS Downloads Site OVOS on top of RaspberryPiOS Lite Build your own image Purpose of this guide This guide will provide you with a minimal HEADLESS ovos system sutable for running on a Raspberry Pi 3. The RPi3 does not have the processing power to reliably run ovos-shell , the GUI system for OVOS, but has pleanty to run the rest of the stack. By the end of the guide, you should have a running OVOS stack, (messagebus, phal, skills, voice, and speech), along with a \"lite\" version of RaspberryPiOS. Which means you have a package manager, (apt) available to you also. Source files used by this guide can be found at raspbian-ovos . Any issues or pull requests should be made in this repository. Step 1: Create the boot medium Download latest lite image and install to SD card or USB. There are lots of guides, but this one is the official guide This is the suggested download Step 2: Setup the system Boot your newly created medium and follow the prompts to create an user. - Create user ovos with password ovos - The system will reboot and ask you to log in. Log in with the above credentals raspi-config RaspberryPiOS comes with a great tool raspi-config . We need to access that to get started setting things up. Run the command sudo raspi-config to enter the utility. We will be running everything as a regular user, so we want to auto login. Enter the System Options page Enter the Boot / Autologin page - Use the second option in the menu. Console Autologin - This enables the ovos user to login to a console terminal on every boot Now we will enable a few interface options. This will allow us to access our device from a ssh shell and prep the PI for other devices that may be used. Some microphone hats require SPI, or I2C (Respeaker, AIY-Voicebonnet, etc) Go back to the main menu and enter the Interface Options page - Enable SSH, SPI, I2C - After SSH is enabled, the rest of the guide can be done from a remote computer Back to the main menu and enter the Localisation Options page - Configure Locale, Timezone, WLAN Country You will need an internet connection to complete the rest of the guide Optional: Setup WIFI Return to the main menu and enter System Options again. Enter the Wireless LAN section and follow the prompts Exit out of the raspi-config tool. And find your IP address. The command is the same if you used the WiFi setup, or have a LAN connected. Enter the command ip addr In the output,if things were configured correctly, there will be one or more lines that are relavant. Find the device that you used to connect, WiFi will start with something like wlan and a LAN connection should begin with eth or enp or something similar. In the device section, there is an inet entry. The number located there is your local IP address. It should be in the format 192.168.x.xxx or something similar. Write this down, or remember it. You will use it to log in with a SSH shell Now the device setup is done. Reboot sudo reboot now From this point on, you should be able to access your device from any SSH terminal. For guide for how to do this, see raspberrypi documentation remote-access From a linux machine, open a terminal and enter the command ssh ovos@<your-remembered-IP-address> . There will be a warning making sure you want to connect to this device. Enter yes, and when asked, enter the password for ovos that you made earlier in the setup. ovos First you need to make sure your system is up to date. It should be close as you just installed a new image. sudo apt -y update && sudo apt -y upgrade We should be done with the basic setup now. You should have a running RaspberryPiOS device with the user ovos Step 3: Install OVOS-CORE There are some recommendations to use a venv for ovos. This guide DOES NOT do that. The ovos headless stack on a RPi3 is about all it can handle. It is assumed that this is a dedicated ovos device, therefore no venv is required. There are a few packages required for ovos, so we will install those first sudo apt install build-essential python3-dev python3-pip swig libssl-dev libfann-dev portaudio19-dev libpulse-dev cmake git libncurses-dev pulseaudio-utils We will be using pip to install ovos-core and other related software. We will also be installing everything to the user environment instead of system wide. As ovos is the only user, this should be fine. We will assume that everything from here will be done in the home directory of ovos. cd ~ Clone this repository git clone https://github.com/OpenVoiceOS/raspbian-ovos.git cd raspbian-ovos Run the install script and follow the prompts ./manual_install.sh You should now have a running ovos device!! Check with this systemctl --user status mycroft* It takes a while to load, but they should all eventually say active (running) , except for mycroft.service which should say active (exited) Step 6: Final thoughts A generic install of ovos-core does not have any default skills shipped with it. Check this page for more information on skills. Audio is also not covered here. Pulseaudio should be running, check with systemctl --user status pulseaudio , but each piece of hardware is different to setup. I am sure there is a guide somewhere for your hardware. One thing to mention, this is a full raspbian install, so installing drivers should work also. Build your own image This image is built using pi-gen which is a tool to build official RaspberryPiOS images. From here on we will assume you are working from your home directory Return home and clone the raspbian-ovos repository cd ~ git clone https://github.com/OpenVoiceOS/raspbian-ovos.git Clone pi-gen from git git clone https://github.com/RPi-Distro/pi-gen.git Change to the arm64 branch cd pi-gen git checkout arm64 Edit the file ~/raspbian-ovos/raspbian-ovos-config nano ~/raspbian-ovos/raspbian-ovos-config Find the line STAGE_LIST=\"stage0 stage1 stage2 /<path/to/git/clone>/raspbian-ovos/stage-ovos\" and edit <path/to/git/clone> to where you cloned this repository You can also edit any other variables in this file to your liking also. See here for what variables are available and thier meaning. Save your changes Ctl-o And exit nano Ctl-x Use pi-gen's build.sh script to build an image with the configuration file provided by the raspbian-ovos repository. This script must be run using sudo or it will complain and make you try again. sudo ./build.sh -c /<path/to/git/clone>/raspbian-ovos/raspbian-ovos-config replace <path/to/git/clone> There is great documentation on all of the configuration options of pi-gen in the README.md on the github page. This build will take some time, mine took 1 1/2 hours with 32 core hyperthreaded 164G ram Ubuntu 22.04 LTS server. When complete, you will have a completed image in ~/pi-gen/deploy/ directory. Flash this image just like any other, and enjoy your new OVOS device","title":"Raspbian"},{"location":"raspbian_ovos/#raspbian-ovos","text":"Pre-built images are available at the OpenVoiceOS Downloads Site OVOS on top of RaspberryPiOS Lite Build your own image","title":"raspbian-ovos"},{"location":"raspbian_ovos/#purpose-of-this-guide","text":"This guide will provide you with a minimal HEADLESS ovos system sutable for running on a Raspberry Pi 3. The RPi3 does not have the processing power to reliably run ovos-shell , the GUI system for OVOS, but has pleanty to run the rest of the stack. By the end of the guide, you should have a running OVOS stack, (messagebus, phal, skills, voice, and speech), along with a \"lite\" version of RaspberryPiOS. Which means you have a package manager, (apt) available to you also. Source files used by this guide can be found at raspbian-ovos . Any issues or pull requests should be made in this repository.","title":"Purpose of this guide"},{"location":"raspbian_ovos/#step-1-create-the-boot-medium","text":"Download latest lite image and install to SD card or USB. There are lots of guides, but this one is the official guide This is the suggested download","title":"Step 1: Create the boot medium"},{"location":"raspbian_ovos/#step-2-setup-the-system","text":"Boot your newly created medium and follow the prompts to create an user. - Create user ovos with password ovos - The system will reboot and ask you to log in. Log in with the above credentals","title":"Step 2: Setup the system"},{"location":"raspbian_ovos/#raspi-config","text":"RaspberryPiOS comes with a great tool raspi-config . We need to access that to get started setting things up. Run the command sudo raspi-config to enter the utility. We will be running everything as a regular user, so we want to auto login. Enter the System Options page Enter the Boot / Autologin page - Use the second option in the menu. Console Autologin - This enables the ovos user to login to a console terminal on every boot Now we will enable a few interface options. This will allow us to access our device from a ssh shell and prep the PI for other devices that may be used. Some microphone hats require SPI, or I2C (Respeaker, AIY-Voicebonnet, etc) Go back to the main menu and enter the Interface Options page - Enable SSH, SPI, I2C - After SSH is enabled, the rest of the guide can be done from a remote computer Back to the main menu and enter the Localisation Options page - Configure Locale, Timezone, WLAN Country You will need an internet connection to complete the rest of the guide","title":"raspi-config"},{"location":"raspbian_ovos/#optional-setup-wifi","text":"Return to the main menu and enter System Options again. Enter the Wireless LAN section and follow the prompts Exit out of the raspi-config tool. And find your IP address. The command is the same if you used the WiFi setup, or have a LAN connected. Enter the command ip addr In the output,if things were configured correctly, there will be one or more lines that are relavant. Find the device that you used to connect, WiFi will start with something like wlan and a LAN connection should begin with eth or enp or something similar. In the device section, there is an inet entry. The number located there is your local IP address. It should be in the format 192.168.x.xxx or something similar. Write this down, or remember it. You will use it to log in with a SSH shell Now the device setup is done. Reboot sudo reboot now From this point on, you should be able to access your device from any SSH terminal. For guide for how to do this, see raspberrypi documentation remote-access From a linux machine, open a terminal and enter the command ssh ovos@<your-remembered-IP-address> . There will be a warning making sure you want to connect to this device. Enter yes, and when asked, enter the password for ovos that you made earlier in the setup. ovos First you need to make sure your system is up to date. It should be close as you just installed a new image. sudo apt -y update && sudo apt -y upgrade We should be done with the basic setup now. You should have a running RaspberryPiOS device with the user ovos","title":"Optional: Setup WIFI"},{"location":"raspbian_ovos/#step-3-install-ovos-core","text":"There are some recommendations to use a venv for ovos. This guide DOES NOT do that. The ovos headless stack on a RPi3 is about all it can handle. It is assumed that this is a dedicated ovos device, therefore no venv is required. There are a few packages required for ovos, so we will install those first sudo apt install build-essential python3-dev python3-pip swig libssl-dev libfann-dev portaudio19-dev libpulse-dev cmake git libncurses-dev pulseaudio-utils We will be using pip to install ovos-core and other related software. We will also be installing everything to the user environment instead of system wide. As ovos is the only user, this should be fine. We will assume that everything from here will be done in the home directory of ovos. cd ~ Clone this repository git clone https://github.com/OpenVoiceOS/raspbian-ovos.git cd raspbian-ovos Run the install script and follow the prompts ./manual_install.sh You should now have a running ovos device!! Check with this systemctl --user status mycroft* It takes a while to load, but they should all eventually say active (running) , except for mycroft.service which should say active (exited)","title":"Step 3: Install OVOS-CORE"},{"location":"raspbian_ovos/#step-6-final-thoughts","text":"A generic install of ovos-core does not have any default skills shipped with it. Check this page for more information on skills. Audio is also not covered here. Pulseaudio should be running, check with systemctl --user status pulseaudio , but each piece of hardware is different to setup. I am sure there is a guide somewhere for your hardware. One thing to mention, this is a full raspbian install, so installing drivers should work also.","title":"Step 6: Final thoughts"},{"location":"raspbian_ovos/#build-your-own-image","text":"This image is built using pi-gen which is a tool to build official RaspberryPiOS images. From here on we will assume you are working from your home directory Return home and clone the raspbian-ovos repository cd ~ git clone https://github.com/OpenVoiceOS/raspbian-ovos.git Clone pi-gen from git git clone https://github.com/RPi-Distro/pi-gen.git Change to the arm64 branch cd pi-gen git checkout arm64 Edit the file ~/raspbian-ovos/raspbian-ovos-config nano ~/raspbian-ovos/raspbian-ovos-config Find the line STAGE_LIST=\"stage0 stage1 stage2 /<path/to/git/clone>/raspbian-ovos/stage-ovos\" and edit <path/to/git/clone> to where you cloned this repository You can also edit any other variables in this file to your liking also. See here for what variables are available and thier meaning. Save your changes Ctl-o And exit nano Ctl-x Use pi-gen's build.sh script to build an image with the configuration file provided by the raspbian-ovos repository. This script must be run using sudo or it will complain and make you try again. sudo ./build.sh -c /<path/to/git/clone>/raspbian-ovos/raspbian-ovos-config replace <path/to/git/clone> There is great documentation on all of the configuration options of pi-gen in the README.md on the github page. This build will take some time, mine took 1 1/2 hours with 32 core hyperthreaded 164G ram Ubuntu 22.04 LTS server. When complete, you will have a completed image in ~/pi-gen/deploy/ directory. Flash this image just like any other, and enjoy your new OVOS device","title":"Build your own image"},{"location":"shell/","text":"OVOS Shell OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images Plugins OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider Alternative Clients Other distributions may offer alternative implementations such as: mycroft-gui also hosts a client for developers on the desktop. plasma-bigscreen mycroft mark2 Configuration The Shell can be configured in a few ways. GUI Display settings Color Theme editor Shell Options ~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true Themes Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"Shell"},{"location":"shell/#ovos-shell","text":"OVOS-shell is the OpenVoiceOS client implementation of the mycroft-gui library used in our embedded device images","title":"OVOS Shell"},{"location":"shell/#plugins","text":"OVOS-shell is tightly coupled to PHAL , the following companion plugins should be installed if you are using ovos-shell ovos-PHAL-plugin-notification-widgets ovos-PHAL-plugin-network-manager ovos-PHAL-plugin-gui-network-client ovos-PHAL-plugin-wifi-setup ovos-PHAL-plugin-alsa ovos-PHAL-plugin-system ovos-PHAL-plugin-dashboard ovos-PHAL-plugin-brightness-control-rpi ovos-PHAL-plugin-color-scheme-manager ovos-PHAL-plugin-configuration-provider","title":"Plugins"},{"location":"shell/#alternative-clients","text":"Other distributions may offer alternative implementations such as: mycroft-gui also hosts a client for developers on the desktop. plasma-bigscreen mycroft mark2","title":"Alternative Clients"},{"location":"shell/#configuration","text":"The Shell can be configured in a few ways.","title":"Configuration"},{"location":"shell/#gui","text":"Display settings Color Theme editor","title":"GUI"},{"location":"shell/#shell-options","text":"~/.config/OpenvoiceOS/OvosShell.conf can be edited to change shell options that may also be changed via UI. An example config would look like: [General] fakeBrightness=1 menuLabels=true","title":"Shell Options"},{"location":"shell/#themes","text":"Shell themes can be included in /usr/share/OVOS/ColorSchemes/ or ~/.local/share/OVOS/ColorSchemes/ in json format. Note that colors should include an alpha value (usually FF ). { \"name\": \"Neon Green\", \"primaryColor\": \"#FF072103\", \"secondaryColor\": \"#FF2C7909\", \"textColor\": \"#FFF1F1F1\" }","title":"Themes"},{"location":"skill_api/","text":"Skill API The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use. Making a method available through the Skill API A method can be tagged with the skill_api_method decorator. This will handle all the basics of making the method available to other Skills over the Message Bus. @skill_api_method def my_exported_method(self, my_arg, my_other_arg): \"\"\"My skill api method documentation \"\"\" The decorator will generate everything needed for accessing the method over the Message Bus and extract the associated docstring. Limitations The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported. Example from mycroft.skills import MycroftSkill, skill_api_method class RobberSkill(MycroftSkill): @skill_api_method def robber_lang(self, sentence): \"\"\"Encode a sentence to \"R\u00f6varspr\u00e5ket\". Each consonant gets converted to consonant + \"o\" + consonant, vowels are left as is. Returns: (str) sentence in the robber language. \"\"\" wovels = \"aeiouy\u00e5\u00e4\u00f6\" tokens = [] for char in sentence.lower() and char.isalpha(): if char not in wovels: tokens.append(char + 'o' + char) else: tokens.append(char) return ' '.join(tokens) def create_skill(): return RobberSkill() Using another Skill's API If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods. To access the robber_lang() method we created above, we could write: from mycroft.skills.api import SkillApi class NewRobberSkill(MycroftSkill): def initialize(self): self.robber = SkillApi.get('robber-skill.forslund') self.speak(self.robber.robber_lang('hello world')) def create_skill(): return NewRobberSkill() When the NewRobberSkill is initialized, it will assign the API from the Skill robber-skill.forslund to self.robber . We then run the exported method robber_lang() passing the argument 'hello world' . Our NewRobberSkill will therefore speak something like \"hoh e lol lol o wow o ror lol dod\".","title":"Skill API"},{"location":"skill_api/#skill-api","text":"The Skill API uses the Message Bus to communicate between Skills and wraps the interaction in simple Python objects making them easy to use.","title":"Skill API"},{"location":"skill_api/#making-a-method-available-through-the-skill-api","text":"A method can be tagged with the skill_api_method decorator. This will handle all the basics of making the method available to other Skills over the Message Bus. @skill_api_method def my_exported_method(self, my_arg, my_other_arg): \"\"\"My skill api method documentation \"\"\" The decorator will generate everything needed for accessing the method over the Message Bus and extract the associated docstring.","title":"Making a method available through the Skill API"},{"location":"skill_api/#limitations","text":"The Skill API works over the Message Bus. This requires that the return values are json serializable. All common Python builtin types (such as List, String, None, etc.) work well, however custom classes are not currently supported.","title":"Limitations"},{"location":"skill_api/#example","text":"from mycroft.skills import MycroftSkill, skill_api_method class RobberSkill(MycroftSkill): @skill_api_method def robber_lang(self, sentence): \"\"\"Encode a sentence to \"R\u00f6varspr\u00e5ket\". Each consonant gets converted to consonant + \"o\" + consonant, vowels are left as is. Returns: (str) sentence in the robber language. \"\"\" wovels = \"aeiouy\u00e5\u00e4\u00f6\" tokens = [] for char in sentence.lower() and char.isalpha(): if char not in wovels: tokens.append(char + 'o' + char) else: tokens.append(char) return ' '.join(tokens) def create_skill(): return RobberSkill()","title":"Example"},{"location":"skill_api/#using-another-skills-api","text":"If you want to make use of exported functionality from another Skill, you must fetch that Skill's SkillApi . This will give you a small class with the target Skill's exported methods. These methods are nothing special and can be called like any other class's methods. To access the robber_lang() method we created above, we could write: from mycroft.skills.api import SkillApi class NewRobberSkill(MycroftSkill): def initialize(self): self.robber = SkillApi.get('robber-skill.forslund') self.speak(self.robber.robber_lang('hello world')) def create_skill(): return NewRobberSkill() When the NewRobberSkill is initialized, it will assign the API from the Skill robber-skill.forslund to self.robber . We then run the exported method robber_lang() passing the argument 'hello world' . Our NewRobberSkill will therefore speak something like \"hoh e lol lol o wow o ror lol dod\".","title":"Using another Skill's API"},{"location":"skill_filesystem/","text":"Filesystem access Many Skills may want access to parts of the filesystem. To account for the many different platforms that can run OVOS there are three locations that a Skill can utilize. Persistent filesystem Temporary cache Skill's own root directory Persistent Files When your Skill needs to store some data that will persist over time and cannot easily be rebuilt, there is a persistent filesystem namespaced to your Skill. Reading and writing to files This uses the standard Python open() method to read and write files. It takes two parameters: file_name (str) - a path relative to the namespace. subdirs not currently supported. mode (str) \u2013 a file handle mode [r, r+, w, w+, rb, rb+, wb+, a, ab, a+, ab+, x] Example: def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read() Check if a file exists Quick method to see if some file exists in the namespaced directory. Example: file_name = \"example.txt\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(\"Hello world\") self.log.info(self.file_system.exists(file_name)) # True self.log.info(self.file_system.exists(\"new.txt\")) # False Get the path of the namespaced directory. self.file_system.path is a member value containing the root path of the namespace. However, it is recommended that you use the self.file_system.open() method to read and write files. Example: from mycroft import MycroftSkill class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Log the path of this Skills persistent namespace.\"\"\" self.log.info(self.file_system.path) def create_skill(): return FileSystemSkill() Create subdirectories Now that we have the path of our namespaced filesystem, we can organize our files however we like within that directory. In this example, we create a subdirectory called \"cache\", then write to a text file inside of it. from os import mkdir from os.path import join from mycroft import MycroftSkill class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Create a cache subdirectory and write to a file inside it\"\"\" cache_dir = \"cache\" file_name = \"example.txt\" if not self.file_system.exists(cache_dir): mkdir(join(self.file_system.path, cache_dir)) with self.file_system.open(join(cache_dir, file_name), \"w\") as my_file: my_file.write('hello') def create_skill(): return FileSystemSkill() Example Skill from mycroft import MycroftSkill, intent_handler class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Log the path of this directory. 2. Write to a file in the directory. 3. Check that our file exists. 4. Read the contents of our file from disk. \"\"\" file_name = \"example.txt\" self.log.info(self.file_system.path) self.write_line_to_file(file_name, \"hello world\") self.log.info(self.file_system.exists(file_name)) self.log.info(self.read_file(file_name)) def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read() def create_skill(): return FileSystemSkill() Temporary Cache Skills can create a directory for caching temporary data to speed up performance. This directory will likely be part of a small RAM disk and may be cleared at any time. So code that uses these cached files must be able to fall back and regenerate the file. Example Skill from os.path import join from mycroft import MycroftSkill, intent_handler from mycroft.util import get_cache_directory class CachingSkill(MycroftSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Get a cache directory namespaced for our Skill. 2. Define a file path for the cache_file. 3. Write some data to the cache_file 4. Log the path of the cache_file 4. Log the contents of the cache_file. \"\"\" cache_dir = get_cache_directory('CachingSkill') self.cache_file = join(cache_dir, \"myfile.txt\") self.cache_data() self.log.info(self.cache_file) self.log.info(self.read_cached_data()) def cache_data(self): with open(self.cache_file, \"w\") as cache_file: cache_file.write(\"Some cached data\") def read_cached_data(self): with open(self.cache_file, \"r\") as cache_file: return cache_file.read() def create_skill(): return CachingSkill() Skill Root Directory self.root_dir This member variable contains the absolute path of a Skill\u2019s root directory e.g. ~.local/share/mycroft/skills/my-skill.me/ . Generally Skills should not modify anything within this directory. Modifying anything in the Skill directory will reload the Skill. It is also not guaranteed that the Skill will have permission to write to this directory.","title":"Filesystem"},{"location":"skill_filesystem/#filesystem-access","text":"Many Skills may want access to parts of the filesystem. To account for the many different platforms that can run OVOS there are three locations that a Skill can utilize. Persistent filesystem Temporary cache Skill's own root directory","title":"Filesystem access"},{"location":"skill_filesystem/#persistent-files","text":"When your Skill needs to store some data that will persist over time and cannot easily be rebuilt, there is a persistent filesystem namespaced to your Skill.","title":"Persistent Files"},{"location":"skill_filesystem/#reading-and-writing-to-files","text":"This uses the standard Python open() method to read and write files. It takes two parameters: file_name (str) - a path relative to the namespace. subdirs not currently supported. mode (str) \u2013 a file handle mode [r, r+, w, w+, rb, rb+, wb+, a, ab, a+, ab+, x] Example: def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read()","title":"Reading and writing to files"},{"location":"skill_filesystem/#check-if-a-file-exists","text":"Quick method to see if some file exists in the namespaced directory. Example: file_name = \"example.txt\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(\"Hello world\") self.log.info(self.file_system.exists(file_name)) # True self.log.info(self.file_system.exists(\"new.txt\")) # False","title":"Check if a file exists"},{"location":"skill_filesystem/#get-the-path-of-the-namespaced-directory","text":"self.file_system.path is a member value containing the root path of the namespace. However, it is recommended that you use the self.file_system.open() method to read and write files. Example: from mycroft import MycroftSkill class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Log the path of this Skills persistent namespace.\"\"\" self.log.info(self.file_system.path) def create_skill(): return FileSystemSkill()","title":"Get the path of the namespaced directory."},{"location":"skill_filesystem/#create-subdirectories","text":"Now that we have the path of our namespaced filesystem, we can organize our files however we like within that directory. In this example, we create a subdirectory called \"cache\", then write to a text file inside of it. from os import mkdir from os.path import join from mycroft import MycroftSkill class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Create a cache subdirectory and write to a file inside it\"\"\" cache_dir = \"cache\" file_name = \"example.txt\" if not self.file_system.exists(cache_dir): mkdir(join(self.file_system.path, cache_dir)) with self.file_system.open(join(cache_dir, file_name), \"w\") as my_file: my_file.write('hello') def create_skill(): return FileSystemSkill()","title":"Create subdirectories"},{"location":"skill_filesystem/#example-skill","text":"from mycroft import MycroftSkill, intent_handler class FileSystemSkill(MycroftSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Log the path of this directory. 2. Write to a file in the directory. 3. Check that our file exists. 4. Read the contents of our file from disk. \"\"\" file_name = \"example.txt\" self.log.info(self.file_system.path) self.write_line_to_file(file_name, \"hello world\") self.log.info(self.file_system.exists(file_name)) self.log.info(self.read_file(file_name)) def write_line_to_file(self, file_name, line): \"\"\"Write a single line to a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"w\") as my_file: my_file.write(line) def read_file(self, file_name): \"\"\"Read the contents of a file in the Skills persistent filesystem.\"\"\" with self.file_system.open(file_name, \"r\") as my_file: return my_file.read() def create_skill(): return FileSystemSkill()","title":"Example Skill"},{"location":"skill_filesystem/#temporary-cache","text":"Skills can create a directory for caching temporary data to speed up performance. This directory will likely be part of a small RAM disk and may be cleared at any time. So code that uses these cached files must be able to fall back and regenerate the file.","title":"Temporary Cache"},{"location":"skill_filesystem/#example-skill_1","text":"from os.path import join from mycroft import MycroftSkill, intent_handler from mycroft.util import get_cache_directory class CachingSkill(MycroftSkill): def initialize(self): \"\"\"Perform initial setup for the Skill. For this example we do four things: 1. Get a cache directory namespaced for our Skill. 2. Define a file path for the cache_file. 3. Write some data to the cache_file 4. Log the path of the cache_file 4. Log the contents of the cache_file. \"\"\" cache_dir = get_cache_directory('CachingSkill') self.cache_file = join(cache_dir, \"myfile.txt\") self.cache_data() self.log.info(self.cache_file) self.log.info(self.read_cached_data()) def cache_data(self): with open(self.cache_file, \"w\") as cache_file: cache_file.write(\"Some cached data\") def read_cached_data(self): with open(self.cache_file, \"r\") as cache_file: return cache_file.read() def create_skill(): return CachingSkill()","title":"Example Skill"},{"location":"skill_filesystem/#skill-root-directory","text":"self.root_dir This member variable contains the absolute path of a Skill\u2019s root directory e.g. ~.local/share/mycroft/skills/my-skill.me/ . Generally Skills should not modify anything within this directory. Modifying anything in the Skill directory will reload the Skill. It is also not guaranteed that the Skill will have permission to write to this directory.","title":"Skill Root Directory"},{"location":"skill_gui/","text":"GUI Framework Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. OVOS uses the standard mycroft-gui framework, you can find the official documentation here Show Simple Content Skills can use self.gui to interact with the GUI service, any ovos component may provide a similar interface via GUIInterface class provided by ovos_utils Text Display simple strings of text. self.gui.show_text(self, text, title=None, override_idle=None, override_animations=False) Arguments: text (str): Main text content. It will auto-paginate title (str): A title to display above the text content. override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Static Image Display a static image such as a jpeg or png. self.gui.show_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Animated Image Display an animated image such as a gif. self.gui.show_animated_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the .gif image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. HTML Page Display a local HTML page. self.gui.show_html(self, html, resource_url=None, override_idle=None, override_animations=False) Arguments: html (str): HTML text to display resource_url (str): Pointer to HTML resources override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Remote URL Display a webpage. self.gui.show_url(self, url, override_idle=None, override_animations=False) Arguments: url (str): URL to render override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations. Advanced QML You can take full advantage of QML directly, you can find mycroft official docs here We have additional design guidelines under our dedicated GUI documentation","title":"Skill GUI"},{"location":"skill_gui/#gui-framework","text":"Mycroft-GUI is an open source visual and display framework for Mycroft running on top of KDE Plasma Technology and built using Kirigami a lightweight user interface framework for convergent applications which are empowered by Qt. OVOS uses the standard mycroft-gui framework, you can find the official documentation here","title":"GUI Framework"},{"location":"skill_gui/#show-simple-content","text":"Skills can use self.gui to interact with the GUI service, any ovos component may provide a similar interface via GUIInterface class provided by ovos_utils","title":"Show Simple Content"},{"location":"skill_gui/#text","text":"Display simple strings of text. self.gui.show_text(self, text, title=None, override_idle=None, override_animations=False) Arguments: text (str): Main text content. It will auto-paginate title (str): A title to display above the text content. override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Text"},{"location":"skill_gui/#static-image","text":"Display a static image such as a jpeg or png. self.gui.show_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Static Image"},{"location":"skill_gui/#animated-image","text":"Display an animated image such as a gif. self.gui.show_animated_image(self, url, caption=None, title=None, fill=None, override_idle=None, override_animations=False) Arguments: url (str): Pointer to the .gif image caption (str): A caption to show under the image title (str): A title to display above the image content fill (str): Fill type - supports: 'PreserveAspectFit', 'PreserveAspectCrop', 'Stretch' override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Animated Image"},{"location":"skill_gui/#html-page","text":"Display a local HTML page. self.gui.show_html(self, html, resource_url=None, override_idle=None, override_animations=False) Arguments: html (str): HTML text to display resource_url (str): Pointer to HTML resources override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"HTML Page"},{"location":"skill_gui/#remote-url","text":"Display a webpage. self.gui.show_url(self, url, override_idle=None, override_animations=False) Arguments: url (str): URL to render override_idle (boolean, int): True: Takes over the resting page indefinitely (int): Delays resting page for the specified number of seconds. override_animations (boolean): True: Disables showing all platform skill animations. False: 'Default' always show animations.","title":"Remote URL"},{"location":"skill_gui/#advanced-qml","text":"You can take full advantage of QML directly, you can find mycroft official docs here We have additional design guidelines under our dedicated GUI documentation","title":"Advanced QML"},{"location":"skill_install/","text":"Installing New Skills There are a few ways to install skills in ovos. The official way is to use opm Manual Install Finding Skills Most skills are found throughout github. The official skills can be found at the OpenVoiceOS github page. Search the repositories for \"skills\". There are a few other places they can be found. Neon AI has several skills, and a search through github will for sure find more. Installing a found skill pip install The prefered method is with pip . If a skill has a setup.py file, it can be installed this way. The syntax is pip install git+<github/repository.git> . ex. pip install git+https://github.com/OpenVoiceOS/skill-ovos-date-time.git should install the ovos-date-time skill After installing skills this way, ovos skills service needs to be restarted buildroot / ovos-picroft systemctl --user restart mycroft-skills manjaro / other system installed services systemctl restart mycroft-skills git install Skills can also be directly cloned to the skill directory, usually located at ~/.local/share/mycroft/skills/ enter the skill directory cd ~/.local/share/mycroft/skills and clone the found skill here with git git clone <github/repository.git> ex. git clone https://github.com/OpenVoiceOS/skill-ovos-date-time.git will install the ovos-date-time skill. A restart of the ovos-skills service is not required when installing this way.","title":"Installing new skills"},{"location":"skill_install/#installing-new-skills","text":"There are a few ways to install skills in ovos. The official way is to use opm","title":"Installing New Skills"},{"location":"skill_install/#manual-install","text":"","title":"Manual Install"},{"location":"skill_install/#finding-skills","text":"Most skills are found throughout github. The official skills can be found at the OpenVoiceOS github page. Search the repositories for \"skills\". There are a few other places they can be found. Neon AI has several skills, and a search through github will for sure find more.","title":"Finding Skills"},{"location":"skill_install/#installing-a-found-skill","text":"","title":"Installing a found skill"},{"location":"skill_install/#pip-install","text":"The prefered method is with pip . If a skill has a setup.py file, it can be installed this way. The syntax is pip install git+<github/repository.git> . ex. pip install git+https://github.com/OpenVoiceOS/skill-ovos-date-time.git should install the ovos-date-time skill After installing skills this way, ovos skills service needs to be restarted buildroot / ovos-picroft systemctl --user restart mycroft-skills manjaro / other system installed services systemctl restart mycroft-skills","title":"pip install"},{"location":"skill_install/#git-install","text":"Skills can also be directly cloned to the skill directory, usually located at ~/.local/share/mycroft/skills/ enter the skill directory cd ~/.local/share/mycroft/skills and clone the found skill here with git git clone <github/repository.git> ex. git clone https://github.com/OpenVoiceOS/skill-ovos-date-time.git will install the ovos-date-time skill. A restart of the ovos-skills service is not required when installing this way.","title":"git install"},{"location":"skill_json/","text":"skill.json Skills can include a skill.json file in the root of the skill directory This file contains metadata about the skill and is all you need to submit a skill to some skill marketplaces (eg. Pling) { \"name\": \"Wolfram Alpha\", \"description\": \"Get information from Wolfram Alpha\", \"skillname\": \"skill-wolfie\", \"authorname\": \"JarbasSkills\", \"url\": \"https://github.com/JarbasSkills/skill-wolfie\", \"branch\": \"v0.1\", \"desktopFile\": false, \"systemDeps\": false, \"download_url\": \"https://github.com/JarbasSkills/skill-wolfie/archive/v0.1.tar.gz\", \"examples\": [ \"ask the wolf what is the speed of light\", \"How tall is Mount Everest?\", \"When was The Rocky Horror Picture Show released?\", \"What is Madonna's real name?\", \"What's 18 times 4?\", \"How many inches in a meter?\" ], \"lang\": \"en-us\", \"lang_support\": { \"pt-pt\": { \"name\": \"Wolfram Alpha\", \"description\": \"...\", \"examples\": [...] } } } Spec On the JSON file: name is a human-readable name for the skill to be used in store listings, UI, etc. description is a human-readable short description of the skill. This should be limited to one or two sentences max branch refers to the git tag mentioned above, and can be omitted if the tag is in the URL. desktop_file should be true if your Skill is associated with a FreeDesktop-compliant desktop entry. Most Skills should leave this false . systemDeps should be true if your skill requirements include system packages, or false if not. icon can be omitted if the Skill's icon can be resolved another way by OSM. folder should be omitted unless you know your target device requires it. categories can list as many categories as you like, and the Skill will appear under each in the Skills Store. category refers to the Skill's primary category, which is the one that will appear next to its entry when clicked. tags refers to other search terms you'd like to apply to this Skill. You are encouraged to add as many tags as you feel are appropriate. These will be carefully checked as part of the review process. examples is a list of example utterances that this skill should handle lang is the language the skill.json and README.md files are written in lang_support is a dict of other supported languages to translated skill name , description , and examples . Missing fields will default to top-level fields","title":"Skill Json"},{"location":"skill_json/#skilljson","text":"Skills can include a skill.json file in the root of the skill directory This file contains metadata about the skill and is all you need to submit a skill to some skill marketplaces (eg. Pling) { \"name\": \"Wolfram Alpha\", \"description\": \"Get information from Wolfram Alpha\", \"skillname\": \"skill-wolfie\", \"authorname\": \"JarbasSkills\", \"url\": \"https://github.com/JarbasSkills/skill-wolfie\", \"branch\": \"v0.1\", \"desktopFile\": false, \"systemDeps\": false, \"download_url\": \"https://github.com/JarbasSkills/skill-wolfie/archive/v0.1.tar.gz\", \"examples\": [ \"ask the wolf what is the speed of light\", \"How tall is Mount Everest?\", \"When was The Rocky Horror Picture Show released?\", \"What is Madonna's real name?\", \"What's 18 times 4?\", \"How many inches in a meter?\" ], \"lang\": \"en-us\", \"lang_support\": { \"pt-pt\": { \"name\": \"Wolfram Alpha\", \"description\": \"...\", \"examples\": [...] } } }","title":"skill.json"},{"location":"skill_json/#spec","text":"On the JSON file: name is a human-readable name for the skill to be used in store listings, UI, etc. description is a human-readable short description of the skill. This should be limited to one or two sentences max branch refers to the git tag mentioned above, and can be omitted if the tag is in the URL. desktop_file should be true if your Skill is associated with a FreeDesktop-compliant desktop entry. Most Skills should leave this false . systemDeps should be true if your skill requirements include system packages, or false if not. icon can be omitted if the Skill's icon can be resolved another way by OSM. folder should be omitted unless you know your target device requires it. categories can list as many categories as you like, and the Skill will appear under each in the Skills Store. category refers to the Skill's primary category, which is the one that will appear next to its entry when clicked. tags refers to other search terms you'd like to apply to this Skill. You are encouraged to add as many tags as you feel are appropriate. These will be carefully checked as part of the review process. examples is a list of example utterances that this skill should handle lang is the language the skill.json and README.md files are written in lang_support is a dict of other supported languages to translated skill name , description , and examples . Missing fields will default to top-level fields","title":"Spec"},{"location":"skill_settings/","text":"Skill Settings Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional. Using settings in your Skill Settings can be managed via command line, shipped by images or managed via a backend if ovos-core us configured to use one When settings are fetched from the backend server, they are saved into a settings.json file. This file is automatically created when a Skill is loaded even if the Skill does not have any settings. Your Skill then accesses the settings from this file. Nowadays, the file is located in the Skill's XDG_CONFIG_DIR (usually ~/config/mycroft/skills/<skillname> ), however if a settings.json file already exists in the Skill's root directory (the deprecated location) that location is used for compatibility. Reading settings Skill settings are available on the MycroftSkill class and inherit from a Python Dict. This means that you can use it just like you would any other Python dictionary. To access the show_time variable from our example above we would use the Dict.get method: self.settings.get('show_time') If the setting we are trying to access is not available, the get method will return None . Instead of assigning this to a variable and then testing for None , we can provide a default value as the second argument to the get method. self.settings.get('show_time', False) In this example, if the settings have not been received, or the show_time setting has not been assigned, it will return the default value False . A few warnings We recommend using the Dict.get method above rather than accessing the setting directly with: self.settings['show_time'] Directly referencing the value may throw a KeyError if the setting has not yet been fetched from the server. It is also important to note that the settings dictionary will not be available in your Skills __init__ method as this is setting up your Skills Class. You should instead use an initialize method which is called after the Skill is fully constructed and registered with the system. Handling settings changes OVOS will check for updates to a users settings regularly, both locally and on the configured backend. To perform some action when settings are updated, you can register a callback function in your Skill. def initialize(self): self.settings_change_callback = self.on_settings_changed self.on_settings_changed() def on_settings_changed(self): show_time = self.settings.get('show_time', False) self.trigger_time_display(show_time) In the example above, we have registered the on_settings_changed method to be our callback function. We have then immediately called the method to perform the relevant actions when the Skill is being initialized even though the Skills settings have not changed. In the on_settings_changed method we have assigned the value of the show_time setting to a local variable. Then we have passed it as an argument to another method in our Skill that will trigger the display of the time based on its value. Writing to settings Your Skill can reassign a setting locally. To do this we assign a value like you would with any other dictionary key. self.settings['show_time'] = True The new value for the show_time setting will persist until a new setting is assigned locally by the Skill, or remotely if the user configured a backend Define settings UI for a Skill To define our Skills settings UI we use a settingsmeta.json or settingsmeta.yaml file. This file must be in the root directory of the Skill and must follow a specific structure. Once settings have been defined using a settingsmeta file, they will be presented to the user in the configured backend or helper application Example settingsmeta file To see it in action, lets look at a simple example from the Date-Time Skill . First using the JSON syntax as a settingsmeta.json file: { \"skillMetadata\": { \"sections\": [ { \"name\": \"Display\", \"fields\": [ { \"name\": \"show_time\", \"type\": \"checkbox\", \"label\": \"Show digital clock when idle\", \"value\": \"true\" } ] } ] } } Now, here is the same settings, as it would be defined with YAML in a settingsmeta.yaml file: skillMetadata: sections: - name: Display fields: - name: show_time type: checkbox label: Show digital clock when idle value: \"true\" Notice that the value of false is surrounded by \"quotation marks\". This is because OVOS expects a string of \"true\" or \"false\" rather than a Boolean. Both of these files would result in the same settings block. It is up to your personal preference which syntax you choose. Structure of the settingsmeta file Whilst the syntax differs, the structure of these two filetypes is the same. This starts at the top level of the file by defining a skillMetadata object. This object must contain one or more sections elements. Sections Each section represents a group of settings that logically sit together. This enables us to display the settings more clearly in the web interface for users. In the simple example above we have just one section. However, the Spotify Skill settings contains two sections. The first is for Spotify Account authentication, and the second section contains settings to define your default playback device. Each section must contain a name attribute that is used as the heading for that section, and an Array of fields . Fields Each section has one or more fields . Each field is a setting available to the user. Each field takes four properties: name (String) The name of the field is used by the Skill to get and set the value of the field . It will not usually be displayed to the user, unless the label property has not been set. * type (Enum) The data type of this field. The supported types are: text : any kind of text email : text validated as an email address checkbox : boolean, True or False number : text validated as a number password : text hidden from view by default select : a drop-down menu of options label : special field to display text for information purposes only. No name or value is required for a label field. label (String) The text to be displayed above the setting field. * value (String) The initial value of the field. Examples for each type of field are provided in JSON and YAML at the end of this page. SettingsMeta Examples Label Field skillMetadata: sections: - name: Label Field Example fields: - type: label label: This is descriptive text. Text Field skillMetadata: sections: - name: Text Field Example fields: - name: my_string type: text label: Enter any text value: Email skillMetadata: sections: - name: Email Field Example fields: - name: my_email_address type: email label: Enter your email address value: Checkbox skillMetadata: sections: - name: Checkbox Field Example fields: - name: my_boolean type: checkbox label: This is an example checkbox. It creates a Boolean value. value: \"false\" Number skillMetadata: sections: - name: Number Field Example fields: - name: my_number type: number label: Enter any number value: 7 Password skillMetadata: sections: - name: Password Field Example fields: - name: my_password type: password label: Enter your password value: Select skillMetadata: sections: - name: Select Field Example fields: - name: my_selected_option type: select label: Select an option options: Option 1|option_one;Option 2|option_two;Option 3|option_three value: option_one","title":"Skill Settings"},{"location":"skill_settings/#skill-settings","text":"Skill settings provide the ability for users to configure a Skill using the command line or a web-based interface. This is often used to: Change default behaviors - such as the sound used for users alarms. Authenticate with external services - such as Spotify Enter longer data as text rather than by voice - such as the IP address of the users Home Assistant server. Skill settings are completely optional.","title":"Skill Settings"},{"location":"skill_settings/#using-settings-in-your-skill","text":"Settings can be managed via command line, shipped by images or managed via a backend if ovos-core us configured to use one When settings are fetched from the backend server, they are saved into a settings.json file. This file is automatically created when a Skill is loaded even if the Skill does not have any settings. Your Skill then accesses the settings from this file. Nowadays, the file is located in the Skill's XDG_CONFIG_DIR (usually ~/config/mycroft/skills/<skillname> ), however if a settings.json file already exists in the Skill's root directory (the deprecated location) that location is used for compatibility.","title":"Using settings in your Skill"},{"location":"skill_settings/#reading-settings","text":"Skill settings are available on the MycroftSkill class and inherit from a Python Dict. This means that you can use it just like you would any other Python dictionary. To access the show_time variable from our example above we would use the Dict.get method: self.settings.get('show_time') If the setting we are trying to access is not available, the get method will return None . Instead of assigning this to a variable and then testing for None , we can provide a default value as the second argument to the get method. self.settings.get('show_time', False) In this example, if the settings have not been received, or the show_time setting has not been assigned, it will return the default value False . A few warnings We recommend using the Dict.get method above rather than accessing the setting directly with: self.settings['show_time'] Directly referencing the value may throw a KeyError if the setting has not yet been fetched from the server. It is also important to note that the settings dictionary will not be available in your Skills __init__ method as this is setting up your Skills Class. You should instead use an initialize method which is called after the Skill is fully constructed and registered with the system.","title":"Reading settings"},{"location":"skill_settings/#handling-settings-changes","text":"OVOS will check for updates to a users settings regularly, both locally and on the configured backend. To perform some action when settings are updated, you can register a callback function in your Skill. def initialize(self): self.settings_change_callback = self.on_settings_changed self.on_settings_changed() def on_settings_changed(self): show_time = self.settings.get('show_time', False) self.trigger_time_display(show_time) In the example above, we have registered the on_settings_changed method to be our callback function. We have then immediately called the method to perform the relevant actions when the Skill is being initialized even though the Skills settings have not changed. In the on_settings_changed method we have assigned the value of the show_time setting to a local variable. Then we have passed it as an argument to another method in our Skill that will trigger the display of the time based on its value.","title":"Handling settings changes"},{"location":"skill_settings/#writing-to-settings","text":"Your Skill can reassign a setting locally. To do this we assign a value like you would with any other dictionary key. self.settings['show_time'] = True The new value for the show_time setting will persist until a new setting is assigned locally by the Skill, or remotely if the user configured a backend","title":"Writing to settings"},{"location":"skill_settings/#define-settings-ui-for-a-skill","text":"To define our Skills settings UI we use a settingsmeta.json or settingsmeta.yaml file. This file must be in the root directory of the Skill and must follow a specific structure. Once settings have been defined using a settingsmeta file, they will be presented to the user in the configured backend or helper application","title":"Define settings UI for a Skill"},{"location":"skill_settings/#example-settingsmeta-file","text":"To see it in action, lets look at a simple example from the Date-Time Skill . First using the JSON syntax as a settingsmeta.json file: { \"skillMetadata\": { \"sections\": [ { \"name\": \"Display\", \"fields\": [ { \"name\": \"show_time\", \"type\": \"checkbox\", \"label\": \"Show digital clock when idle\", \"value\": \"true\" } ] } ] } } Now, here is the same settings, as it would be defined with YAML in a settingsmeta.yaml file: skillMetadata: sections: - name: Display fields: - name: show_time type: checkbox label: Show digital clock when idle value: \"true\" Notice that the value of false is surrounded by \"quotation marks\". This is because OVOS expects a string of \"true\" or \"false\" rather than a Boolean. Both of these files would result in the same settings block. It is up to your personal preference which syntax you choose.","title":"Example settingsmeta file"},{"location":"skill_settings/#structure-of-the-settingsmeta-file","text":"Whilst the syntax differs, the structure of these two filetypes is the same. This starts at the top level of the file by defining a skillMetadata object. This object must contain one or more sections elements.","title":"Structure of the settingsmeta file"},{"location":"skill_settings/#sections","text":"Each section represents a group of settings that logically sit together. This enables us to display the settings more clearly in the web interface for users. In the simple example above we have just one section. However, the Spotify Skill settings contains two sections. The first is for Spotify Account authentication, and the second section contains settings to define your default playback device. Each section must contain a name attribute that is used as the heading for that section, and an Array of fields .","title":"Sections"},{"location":"skill_settings/#fields","text":"Each section has one or more fields . Each field is a setting available to the user. Each field takes four properties: name (String) The name of the field is used by the Skill to get and set the value of the field . It will not usually be displayed to the user, unless the label property has not been set. * type (Enum) The data type of this field. The supported types are: text : any kind of text email : text validated as an email address checkbox : boolean, True or False number : text validated as a number password : text hidden from view by default select : a drop-down menu of options label : special field to display text for information purposes only. No name or value is required for a label field. label (String) The text to be displayed above the setting field. * value (String) The initial value of the field. Examples for each type of field are provided in JSON and YAML at the end of this page.","title":"Fields"},{"location":"skill_settings/#settingsmeta-examples","text":"","title":"SettingsMeta Examples"},{"location":"skill_settings/#label-field","text":"skillMetadata: sections: - name: Label Field Example fields: - type: label label: This is descriptive text.","title":"Label Field"},{"location":"skill_settings/#text-field","text":"skillMetadata: sections: - name: Text Field Example fields: - name: my_string type: text label: Enter any text value:","title":"Text Field"},{"location":"skill_settings/#email","text":"skillMetadata: sections: - name: Email Field Example fields: - name: my_email_address type: email label: Enter your email address value:","title":"Email"},{"location":"skill_settings/#checkbox","text":"skillMetadata: sections: - name: Checkbox Field Example fields: - name: my_boolean type: checkbox label: This is an example checkbox. It creates a Boolean value. value: \"false\"","title":"Checkbox"},{"location":"skill_settings/#number","text":"skillMetadata: sections: - name: Number Field Example fields: - name: my_number type: number label: Enter any number value: 7","title":"Number"},{"location":"skill_settings/#password","text":"skillMetadata: sections: - name: Password Field Example fields: - name: my_password type: password label: Enter your password value:","title":"Password"},{"location":"skill_settings/#select","text":"skillMetadata: sections: - name: Select Field Example fields: - name: my_selected_option type: select label: Select an option options: Option 1|option_one;Option 2|option_two;Option 3|option_three value: option_one","title":"Select"},{"location":"skill_structure/","text":"Anatomy of a Skill vocab , dialog , and locale directories The dialog , vocab , and locale directories contain subdirectories for each spoken language the skill supports. The subdirectories are named using the IETF language tag for the language. For example, Brazilian Portuguese is 'pt-br', German is 'de-de', and Australian English is 'en-au'. dialog and vocab have been deprecated, they are still supported, but we strongly recommend you use locale for new skills inside the locale folder you will find subfolders for each language (e.g. en-us ), often all you need to do in order to translate a skill is adding a new folder for your language here each language folder can have the structure it wants, you may see files grouped by type in subfolder or all in the base folder You will find several unfamiliar file extensions in this folder, but these are simple text files .dialog files used for defining speech responses .intent files used for defining Padatious Intents .voc files define keywords primarily used in Adapt Intents .entity files define a named entity primarily used in Padatious Intents init .py The __init__.py file is where most of the Skill is defined using Python code. Importing libraries from adapt.intent import IntentBuilder from mycroft import intent_handler from ovos_workshop.skills import OVOSSkill This section of code imports the required libraries . Some libraries will be required on every Skill, and your skill may need to import additional libraries. Class definition The class definition extends the OVOSSkill class: class HelloWorldSkill(OVOSSkill): The class should be named logically, for example \"TimeSkill\", \"WeatherSkill\", \"NewsSkill\", \"IPaddressSkill\". If you would like guidance on what to call your Skill, please join the skills Channel on OVOS Chat . Inside the class, methods are then defined. init () This method is the constructor . It is called when the Skill is first constructed. It is often used to declare state variables or perform setup actions, however it cannot utilise OVOSSkill methods as the class does not yet exist. You usually don't have to include the constructor. An example __init__ method might be: def __init__(self): super().__init__() self.already_said_hello = False self.be_friendly = True initialize() Perform any final setup needed for the skill here. This function is invoked after the skill is fully constructed and registered with the system. Intents will be registered and Skill settings will be available. If you need to access self.skill_id , self.bus , self.settings or self.filesystem you must do it here instead of __init__ def initialize(self): my_setting = self.settings.get('my_setting') @intent_handler We can use the initialize function to manually register intents, however the @intent_handler decorator is a cleaner way to achieve this. We will learn all about the different Intents shortly. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated, and you can now replace any instance of this with the simpler @intent_handler decorator. In skills we can see two different intent styles. An Adapt handler, triggered by a keyword defined in a ThankYouKeyword.voc file. @intent_handler(IntentBuilder('ThankYouIntent').require('ThankYouKeyword')) def handle_thank_you_intent(self, message): self.speak_dialog(\"welcome\") A Padatious intent handler, triggered using a list of sample phrases. @intent_handler('HowAreYou.intent') def handle_how_are_you_intent(self, message): self.speak_dialog(\"how.are.you\") In both cases, the function receives two parameters : self - a reference to the HelloWorldSkill object itself message - an incoming message from the messagebus . Both intents call the self.speak_dialog() method, passing the name of a dialog file to it. In this case welcome.dialog and how.are.you.dialog . stop() You will usually also have a stop() method. The stop method is called anytime a User says \"Stop\" or a similar command. It is useful for stopping any output or process that a User might want to end without needing to issue a Skill specific utterance such as media playback or an expired alarm notification. In the following example, we call a method stop_beeping to end a notification that our Skill has created. def stop(self): self.stop_beeping() If a Skill has any active functionality, the stop() method should terminate the functionality, leaving the Skill in a known good state. shutdown() The shutdown method is called during the Skill process termination. It is used to perform any final actions to ensure all processes and operations in execution are stopped safely. This might be particularly useful for Skills that have scheduled future events, may be writing to a file or database, or that have initiated new processes. In the following example we cancel a scheduled event and call a method in our Skill to stop a subprocess we initiated. def shutdown(self): self.cancel_scheduled_event('my_event') self.stop_my_subprocess() create_skill() The final code block in our Skill is the create_skill function that returns our new Skill: def create_skill(): return HelloWorldSkill() This is required by OVOS and is responsible for actually creating an instance of your Skill that OVOS can load. Please note that this function is not scoped within your Skills class. It should not be indented to the same level as the methods discussed above. settingsmeta.yaml This file defines the settings UI that will be available to a User through a backend or companion app Jump to Skill Settings for more information on this file and handling of Skill settings. setup.py This file allows a skill to be installed just like any other python package. This means you can publish your skill on pypi or favorite package manager and use it as a dependency A typical setup.py file looks like this #!/usr/bin/env python3 from setuptools import setup import os from os import walk, path # update this! URL = \"https://github.com/OpenVoiceOS/skill-awesome-stuff\" SKILL_CLAZZ = \"AwesomeSkill\" # needs to match __init__.py class name PYPI_NAME = \"skill-awesome-stuff\" # pip install PYPI_NAME # below derived from github url to ensure standard skill_id SKILL_AUTHOR, SKILL_NAME = URL.split(\".com/\")[-1].split(\"/\") SKILL_PKG = SKILL_NAME.lower().replace('-', '_') PLUGIN_ENTRY_POINT = f'{SKILL_NAME.lower()}.{SKILL_AUTHOR.lower()}={SKILL_PKG}:{SKILL_CLAZZ}' # skill_id=package_name:SkillClass def get_requirements(requirements_filename: str): requirements_file = path.join(path.abspath(path.dirname(__file__)), requirements_filename) with open(requirements_file, 'r', encoding='utf-8') as r: requirements = r.readlines() requirements = [r.strip() for r in requirements if r.strip() and not r.strip().startswith(\"#\")] if 'MYCROFT_LOOSE_REQUIREMENTS' in os.environ: print('USING LOOSE REQUIREMENTS!') requirements = [r.replace('==', '>=').replace('~=', '>=') for r in requirements] return requirements def find_resource_files(): # add any folder with files your skill uses here! resource_base_dirs = (\"locale\", \"ui\", \"vocab\", \"dialog\", \"regex\", \"skill\") base_dir = path.dirname(__file__) package_data = [\"*.json\"] for res in resource_base_dirs: if path.isdir(path.join(base_dir, res)): for (directory, _, files) in walk(path.join(base_dir, res)): if files: package_data.append( path.join(directory.replace(base_dir, \"\").lstrip('/'), '*')) return package_data # TODO - add description, author, email, license, etc setup( # this is the package name that goes on pip name=PYPI_NAME, version=\"0.0.0\", url=URL, license='Apache-2.0', package_dir={SKILL_PKG: \"\"}, package_data={SKILL_PKG: find_resource_files()}, packages=[SKILL_PKG], include_package_data=True, install_requires=get_requirements(\"requirements.txt\"), keywords='ovos skill plugin', entry_points={'ovos.plugin.skill': PLUGIN_ENTRY_POINT} )","title":"Anatomy"},{"location":"skill_structure/#anatomy-of-a-skill","text":"","title":"Anatomy of a Skill"},{"location":"skill_structure/#vocab-dialog-and-locale-directories","text":"The dialog , vocab , and locale directories contain subdirectories for each spoken language the skill supports. The subdirectories are named using the IETF language tag for the language. For example, Brazilian Portuguese is 'pt-br', German is 'de-de', and Australian English is 'en-au'. dialog and vocab have been deprecated, they are still supported, but we strongly recommend you use locale for new skills inside the locale folder you will find subfolders for each language (e.g. en-us ), often all you need to do in order to translate a skill is adding a new folder for your language here each language folder can have the structure it wants, you may see files grouped by type in subfolder or all in the base folder You will find several unfamiliar file extensions in this folder, but these are simple text files .dialog files used for defining speech responses .intent files used for defining Padatious Intents .voc files define keywords primarily used in Adapt Intents .entity files define a named entity primarily used in Padatious Intents","title":"vocab, dialog, and locale directories"},{"location":"skill_structure/#initpy","text":"The __init__.py file is where most of the Skill is defined using Python code.","title":"init.py"},{"location":"skill_structure/#importing-libraries","text":"from adapt.intent import IntentBuilder from mycroft import intent_handler from ovos_workshop.skills import OVOSSkill This section of code imports the required libraries . Some libraries will be required on every Skill, and your skill may need to import additional libraries.","title":"Importing libraries"},{"location":"skill_structure/#class-definition","text":"The class definition extends the OVOSSkill class: class HelloWorldSkill(OVOSSkill): The class should be named logically, for example \"TimeSkill\", \"WeatherSkill\", \"NewsSkill\", \"IPaddressSkill\". If you would like guidance on what to call your Skill, please join the skills Channel on OVOS Chat . Inside the class, methods are then defined.","title":"Class definition"},{"location":"skill_structure/#init","text":"This method is the constructor . It is called when the Skill is first constructed. It is often used to declare state variables or perform setup actions, however it cannot utilise OVOSSkill methods as the class does not yet exist. You usually don't have to include the constructor. An example __init__ method might be: def __init__(self): super().__init__() self.already_said_hello = False self.be_friendly = True","title":"init()"},{"location":"skill_structure/#initialize","text":"Perform any final setup needed for the skill here. This function is invoked after the skill is fully constructed and registered with the system. Intents will be registered and Skill settings will be available. If you need to access self.skill_id , self.bus , self.settings or self.filesystem you must do it here instead of __init__ def initialize(self): my_setting = self.settings.get('my_setting')","title":"initialize()"},{"location":"skill_structure/#intent_handler","text":"We can use the initialize function to manually register intents, however the @intent_handler decorator is a cleaner way to achieve this. We will learn all about the different Intents shortly. You may also see the @intent_file_handler decorator used in Skills. This has been deprecated, and you can now replace any instance of this with the simpler @intent_handler decorator. In skills we can see two different intent styles. An Adapt handler, triggered by a keyword defined in a ThankYouKeyword.voc file. @intent_handler(IntentBuilder('ThankYouIntent').require('ThankYouKeyword')) def handle_thank_you_intent(self, message): self.speak_dialog(\"welcome\") A Padatious intent handler, triggered using a list of sample phrases. @intent_handler('HowAreYou.intent') def handle_how_are_you_intent(self, message): self.speak_dialog(\"how.are.you\") In both cases, the function receives two parameters : self - a reference to the HelloWorldSkill object itself message - an incoming message from the messagebus . Both intents call the self.speak_dialog() method, passing the name of a dialog file to it. In this case welcome.dialog and how.are.you.dialog .","title":"@intent_handler"},{"location":"skill_structure/#stop","text":"You will usually also have a stop() method. The stop method is called anytime a User says \"Stop\" or a similar command. It is useful for stopping any output or process that a User might want to end without needing to issue a Skill specific utterance such as media playback or an expired alarm notification. In the following example, we call a method stop_beeping to end a notification that our Skill has created. def stop(self): self.stop_beeping() If a Skill has any active functionality, the stop() method should terminate the functionality, leaving the Skill in a known good state.","title":"stop()"},{"location":"skill_structure/#shutdown","text":"The shutdown method is called during the Skill process termination. It is used to perform any final actions to ensure all processes and operations in execution are stopped safely. This might be particularly useful for Skills that have scheduled future events, may be writing to a file or database, or that have initiated new processes. In the following example we cancel a scheduled event and call a method in our Skill to stop a subprocess we initiated. def shutdown(self): self.cancel_scheduled_event('my_event') self.stop_my_subprocess()","title":"shutdown()"},{"location":"skill_structure/#create_skill","text":"The final code block in our Skill is the create_skill function that returns our new Skill: def create_skill(): return HelloWorldSkill() This is required by OVOS and is responsible for actually creating an instance of your Skill that OVOS can load. Please note that this function is not scoped within your Skills class. It should not be indented to the same level as the methods discussed above.","title":"create_skill()"},{"location":"skill_structure/#settingsmetayaml","text":"This file defines the settings UI that will be available to a User through a backend or companion app Jump to Skill Settings for more information on this file and handling of Skill settings.","title":"settingsmeta.yaml"},{"location":"skill_structure/#setuppy","text":"This file allows a skill to be installed just like any other python package. This means you can publish your skill on pypi or favorite package manager and use it as a dependency A typical setup.py file looks like this #!/usr/bin/env python3 from setuptools import setup import os from os import walk, path # update this! URL = \"https://github.com/OpenVoiceOS/skill-awesome-stuff\" SKILL_CLAZZ = \"AwesomeSkill\" # needs to match __init__.py class name PYPI_NAME = \"skill-awesome-stuff\" # pip install PYPI_NAME # below derived from github url to ensure standard skill_id SKILL_AUTHOR, SKILL_NAME = URL.split(\".com/\")[-1].split(\"/\") SKILL_PKG = SKILL_NAME.lower().replace('-', '_') PLUGIN_ENTRY_POINT = f'{SKILL_NAME.lower()}.{SKILL_AUTHOR.lower()}={SKILL_PKG}:{SKILL_CLAZZ}' # skill_id=package_name:SkillClass def get_requirements(requirements_filename: str): requirements_file = path.join(path.abspath(path.dirname(__file__)), requirements_filename) with open(requirements_file, 'r', encoding='utf-8') as r: requirements = r.readlines() requirements = [r.strip() for r in requirements if r.strip() and not r.strip().startswith(\"#\")] if 'MYCROFT_LOOSE_REQUIREMENTS' in os.environ: print('USING LOOSE REQUIREMENTS!') requirements = [r.replace('==', '>=').replace('~=', '>=') for r in requirements] return requirements def find_resource_files(): # add any folder with files your skill uses here! resource_base_dirs = (\"locale\", \"ui\", \"vocab\", \"dialog\", \"regex\", \"skill\") base_dir = path.dirname(__file__) package_data = [\"*.json\"] for res in resource_base_dirs: if path.isdir(path.join(base_dir, res)): for (directory, _, files) in walk(path.join(base_dir, res)): if files: package_data.append( path.join(directory.replace(base_dir, \"\").lstrip('/'), '*')) return package_data # TODO - add description, author, email, license, etc setup( # this is the package name that goes on pip name=PYPI_NAME, version=\"0.0.0\", url=URL, license='Apache-2.0', package_dir={SKILL_PKG: \"\"}, package_data={SKILL_PKG: find_resource_files()}, packages=[SKILL_PKG], include_package_data=True, install_requires=get_requirements(\"requirements.txt\"), keywords='ovos skill plugin', entry_points={'ovos.plugin.skill': PLUGIN_ENTRY_POINT} )","title":"setup.py"},{"location":"skill_tests/","text":"Skill Tests Testing is an important part of skill development. Here are some recommendations for how to implement good tests for a skill. Unit Tests Like any other module, testing each method in you skill can help prevent debugging headaches when something goes wrong. These tests are specific to a skill, and it is up to the skill author to test that methods act as expected. OSM Installation Tests This test case may be used to test skill installation via OSM: import unittest from os.path import exists from shutil import rmtree from ovos_skills_manager import SkillEntry branch = \"dev\" # TODO: Specify default branch repo = \"skill-ovos-volume\" # TODO: Specify repository name author = \"OpenVoiceOS\" # TODO: Specify repository author url = f\"https://github.com/{author}/{repo}@{branch}\" class TestOSM(unittest.TestCase): @classmethod def setUpClass(self): self.skill_id = \"{repo.lower()}.{author.lower()}\" def test_osm_install(self): skill = SkillEntry.from_github_url(url) tmp_skills = \"/tmp/osm_installed_skills\" skill_folder = f\"{tmp_skills}/{skill.uuid}\" if exists(skill_folder): rmtree(skill_folder) updated = skill.install(folder=tmp_skills, default_branch=branch) self.assertEqual(updated, True) self.assertTrue(exists(skill_folder)) updated = skill.install(folder=tmp_skills, default_branch=branch) self.assertEqual(updated, False) Skill Resource Tests This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests. test/test_resources.yaml This file specifies the expected skill resources, including files, entities, and intent names. An example from the Support Helper Skill is included for reference: # Specify resources to test here. # Specify languages to be tested languages: - \"en-us\" # vocab is lowercase .voc file basenames vocab: [] # dialog is .dialog file basenames (case-sensitive) dialog: - ask_description - cancelled - complete - confirm_support - email_intro - email_signature - email_title - no_email - one_moment - support # regex entities, not necessarily filenames regex: [] intents: # Padatious intents are the `.intent` file names padatious: - contact_support.intent # Adapt intents are the name passed to the constructor adapt: [] .github/workflows/skill_tests.yml The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_resources.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_resource_tests: uses: neongeckocom/.github/.github/workflows/skill_test_resources.yml@master Skill Intent Tests This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests. test/test_intents.yaml This file specifies skill intents and utterances that should match those intents. Specific vocabulary and entity matches may also be specified, An example from the IP Address Skill is included for reference: # Specify intents to test here. Valid test cases are as follows: # Basic intent match tests only: #lang: # intent_name: # - example utterance # - other example utterance # Intent tests with expected vocab/entity matches: #lang: # intent_name: # - example_utterance: # - expected vocab name # - other expected vocab name # Intent tests with specific vocab/entity extraction tests: #lang: # intent_name: # - example_utterance: # - expected_vocab_key: expected_vocab_value # - expected_entity_key: expected_entity_value en-us: IPIntent: - what is your ip address - what is my ip address: - IP - what is my i.p. address - What is your I.P. address? - what is my public IP address?: - public: public .github/workflows/skill_tests.yml The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_intents.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_intent_tests: uses: neongeckocom/.github/.github/workflows/skill_test_intents.yml@master","title":"Skill Tests"},{"location":"skill_tests/#skill-tests","text":"Testing is an important part of skill development. Here are some recommendations for how to implement good tests for a skill.","title":"Skill Tests"},{"location":"skill_tests/#unit-tests","text":"Like any other module, testing each method in you skill can help prevent debugging headaches when something goes wrong. These tests are specific to a skill, and it is up to the skill author to test that methods act as expected.","title":"Unit Tests"},{"location":"skill_tests/#osm-installation-tests","text":"This test case may be used to test skill installation via OSM: import unittest from os.path import exists from shutil import rmtree from ovos_skills_manager import SkillEntry branch = \"dev\" # TODO: Specify default branch repo = \"skill-ovos-volume\" # TODO: Specify repository name author = \"OpenVoiceOS\" # TODO: Specify repository author url = f\"https://github.com/{author}/{repo}@{branch}\" class TestOSM(unittest.TestCase): @classmethod def setUpClass(self): self.skill_id = \"{repo.lower()}.{author.lower()}\" def test_osm_install(self): skill = SkillEntry.from_github_url(url) tmp_skills = \"/tmp/osm_installed_skills\" skill_folder = f\"{tmp_skills}/{skill.uuid}\" if exists(skill_folder): rmtree(skill_folder) updated = skill.install(folder=tmp_skills, default_branch=branch) self.assertEqual(updated, True) self.assertTrue(exists(skill_folder)) updated = skill.install(folder=tmp_skills, default_branch=branch) self.assertEqual(updated, False)","title":"OSM Installation Tests"},{"location":"skill_tests/#skill-resource-tests","text":"This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests.","title":"Skill Resource Tests"},{"location":"skill_tests/#testtest_resourcesyaml","text":"This file specifies the expected skill resources, including files, entities, and intent names. An example from the Support Helper Skill is included for reference: # Specify resources to test here. # Specify languages to be tested languages: - \"en-us\" # vocab is lowercase .voc file basenames vocab: [] # dialog is .dialog file basenames (case-sensitive) dialog: - ask_description - cancelled - complete - confirm_support - email_intro - email_signature - email_title - no_email - one_moment - support # regex entities, not necessarily filenames regex: [] intents: # Padatious intents are the `.intent` file names padatious: - contact_support.intent # Adapt intents are the name passed to the constructor adapt: []","title":"test/test_resources.yaml"},{"location":"skill_tests/#githubworkflowsskill_testsyml","text":"The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_resources.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_resource_tests: uses: neongeckocom/.github/.github/workflows/skill_test_resources.yml@master","title":".github/workflows/skill_tests.yml"},{"location":"skill_tests/#skill-intent-tests","text":"This test case may be used to test skill resource files and event registration. A shared GitHub Action is provided by Neon AI to easily implement automated tests. The following files should be included in the skill repository to use these tests.","title":"Skill Intent Tests"},{"location":"skill_tests/#testtest_intentsyaml","text":"This file specifies skill intents and utterances that should match those intents. Specific vocabulary and entity matches may also be specified, An example from the IP Address Skill is included for reference: # Specify intents to test here. Valid test cases are as follows: # Basic intent match tests only: #lang: # intent_name: # - example utterance # - other example utterance # Intent tests with expected vocab/entity matches: #lang: # intent_name: # - example_utterance: # - expected vocab name # - other expected vocab name # Intent tests with specific vocab/entity extraction tests: #lang: # intent_name: # - example_utterance: # - expected_vocab_key: expected_vocab_value # - expected_entity_key: expected_entity_value en-us: IPIntent: - what is your ip address - what is my ip address: - IP - what is my i.p. address - What is your I.P. address? - what is my public IP address?: - public: public","title":"test/test_intents.yaml"},{"location":"skill_tests/#githubworkflowsskill_testsyml_1","text":"The filename isn't important here, but a workflow must specify a job using neongeckocom/.github/.github/workflows/skill_test_intents.yml@master . A minimal example would be: name: Test Skill Resources on: pull_request: workflow_dispatch: jobs: skill_intent_tests: uses: neongeckocom/.github/.github/workflows/skill_test_intents.yml@master","title":".github/workflows/skill_tests.yml"},{"location":"skills/","text":"OVOS Skills What can a Skill do? Skills give OVOS the ability to perform a variety of functions. They can be installed or removed by the user, and can be easily updated to expand functionality. To get a good idea of what skills to build, let\u2019s talk about the best use cases for a voice assistant, and what types of things OVOS can do. OVOS can run on a variety of platforms from the Linux Desktop to SBCs like the raspberry pi. Different devices will have slightly different use cases. Devices in the home are generally located in the living room or kitchen and are ideal for listening to the news, playing music, general information, using timers while cooking, checking the weather, and other similar activities that are easily accomplished hands free. Basic functions We cover a lot of the basics with our Default Skills, things like Timers, Alarms, Weather, Time and Date, and more. Information We also call this General Question and Answer, and it covers all of those factual questions someone might think to ask a voice assistant. Questions like \u201cwho was the 32nd President of the United States?\u201d, or \u201chow tall is Eiffel Tower?\u201d Although the Default Skills cover a great deal of questions there is room for more. There are many topics that could use a specific skill such as Science, Academics, Movie Info, TV info, and Music info, etc.. Media One of the biggest use cases for Smart Speakers is playing media. The reason media playback is so popular is that it makes playing a song so easy, all you have to do is say \u201cHey Mycroft play the Beatles,\u201d and you can be enjoying music without having to reach for a phone or remote. In addition to listening to music, there are skills that handle videos as well. News Much like listening to music, getting the latest news with a simple voice interaction is extremely convenient. OVOS supports multiple news feeds, and has the ability to support multiple news skills. Smart Home Another popular use case for Voice Assistants is to control Smart Home and IoT products. Within the mycroft ecosystem there are skills for Home Assistant, Wink IoT, Lifx and more, but there are many products that we do not have skill for yet. The open source community has been enthusiastically expanding OVOS's ability to voice control all kinds of smart home products. Games Voice games are becoming more and more popular, especially those that allow multiple users to play together. Trivia games are some of the most popular types of games to develop for voice assistants. There are several games already available for OVOS. There are native voice adventure games, ports of the popular text adventure games from infocom, a Crystal Ball game, a Number Guessing game and much more!","title":"Introduction"},{"location":"skills/#ovos-skills","text":"","title":"OVOS Skills"},{"location":"skills/#what-can-a-skill-do","text":"Skills give OVOS the ability to perform a variety of functions. They can be installed or removed by the user, and can be easily updated to expand functionality. To get a good idea of what skills to build, let\u2019s talk about the best use cases for a voice assistant, and what types of things OVOS can do. OVOS can run on a variety of platforms from the Linux Desktop to SBCs like the raspberry pi. Different devices will have slightly different use cases. Devices in the home are generally located in the living room or kitchen and are ideal for listening to the news, playing music, general information, using timers while cooking, checking the weather, and other similar activities that are easily accomplished hands free.","title":"What can a Skill do?"},{"location":"skills/#basic-functions","text":"We cover a lot of the basics with our Default Skills, things like Timers, Alarms, Weather, Time and Date, and more.","title":"Basic functions"},{"location":"skills/#information","text":"We also call this General Question and Answer, and it covers all of those factual questions someone might think to ask a voice assistant. Questions like \u201cwho was the 32nd President of the United States?\u201d, or \u201chow tall is Eiffel Tower?\u201d Although the Default Skills cover a great deal of questions there is room for more. There are many topics that could use a specific skill such as Science, Academics, Movie Info, TV info, and Music info, etc..","title":"Information"},{"location":"skills/#media","text":"One of the biggest use cases for Smart Speakers is playing media. The reason media playback is so popular is that it makes playing a song so easy, all you have to do is say \u201cHey Mycroft play the Beatles,\u201d and you can be enjoying music without having to reach for a phone or remote. In addition to listening to music, there are skills that handle videos as well.","title":"Media"},{"location":"skills/#news","text":"Much like listening to music, getting the latest news with a simple voice interaction is extremely convenient. OVOS supports multiple news feeds, and has the ability to support multiple news skills.","title":"News"},{"location":"skills/#smart-home","text":"Another popular use case for Voice Assistants is to control Smart Home and IoT products. Within the mycroft ecosystem there are skills for Home Assistant, Wink IoT, Lifx and more, but there are many products that we do not have skill for yet. The open source community has been enthusiastically expanding OVOS's ability to voice control all kinds of smart home products.","title":"Smart Home"},{"location":"skills/#games","text":"Voice games are becoming more and more popular, especially those that allow multiple users to play together. Trivia games are some of the most popular types of games to develop for voice assistants. There are several games already available for OVOS. There are native voice adventure games, ports of the popular text adventure games from infocom, a Crystal Ball game, a Number Guessing game and much more!","title":"Games"},{"location":"skills_service/","text":"Skills Service The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain Configuration \"skills\": { // don't start loading skills until internet is detected // this config value is not present in mycroft-core (internet is required) // ovos-core expects that some instances will be running fully offline \"wait_for_internet\": false, // folder name, relative to \"data_dir\" \"directory\": \"skills\", // used by selene marketplace integration \"upload_skill_manifest\": true, // blacklisted skills to not load // NB: This is the skill_id, usually the basename() of the directory where the skill lives, so if // the skill you want to blacklist is in /usr/share/mycroft/skills/mycroft-alarm.mycroftai/ // then you should write `[\"mycroft-alarm.mycroftai\"]` below. \"blacklisted_skills\": [], // DEPRECATED: priority skills to be loaded first // setup.py installed skills take precedence over this // this option is an artifact from the past when internet and pairing blocked skill loading \"priority_skills\": [], // fallback skill configuration (see below) \"fallbacks\": {...}, // converse stage configuration (see below) \"converse\": {...} }, Converse A malicious or badly designed skill using the converse method can potentially hijack the whole conversation loop and render the skills service unusable Some settings are exposed to add some limitations to which skills can be activated and under what circumstances The concept of \"converse priority\" is under active development, work on a \"converse session\" mechanism is also underway @ovos-core/pull/160 \"skills\": { // converse stage configuration \"converse\": { // the default number of seconds a skill remains active, // if the user does not interact with the skill in this timespan it // will be deactivated, default 5 minutes (same as mycroft) \"timeout\": 300, // override of \"skill_timeouts\" per skill_id // you can configure specific skills to remain active longer \"skill_timeouts\": {}, // conversational mode has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call converse for skills in \"converse_whitelist\" // - \"blacklist\" # only call converse for skills NOT in \"converse_blacklist\" \"converse_mode\": \"accept_all\", \"converse_whitelist\": [], \"converse_blacklist\": [], // converse activation has 4 modes of operations: // - \"accept_all\" # default mycroft-core behavior, any skill can // # activate itself unconditionally // - \"priority\" # skills can only activate themselves if no skill with // # higher priority is active // - \"whitelist\" # only skills in \"converse_whitelist\" can activate themselves // - \"blacklist\" # only skills NOT in converse \"converse_blacklist\" can activate themselves // NOTE: this does not apply for regular skill activation, only to skill // initiated activation requests, eg, self.make_active() \"converse_activation\": \"accept_all\", // number of consecutive times a skill is allowed to activate itself // per minute, -1 for no limit (default), 0 to disable self-activation \"max_activations\": -1, // override of \"max_activations\" per skill_id // you can configure specific skills to activate more/less often \"skill_activations\": {}, // if false only skills can activate themselves // if true any skill can activate any other skill \"cross_activation\": true, // if false only skills can deactivate themselves // if true any skill can deactivate any other skill // NOTE: skill deactivation is not yet implemented \"cross_deactivation\": true, // you can add skill_id: priority to override the developer defined // priority of those skills, // converse priority is work in progress and not yet exposed to skills // priority is assumed to be 50 // the only current source for converse priorities is this setting \"converse_priorities\": { // \"skill_id\": 10 } } }, Fallback Skills Just like with converse a badly designed or malicious skill can hijack the fallback skill loop, while this is not as serious as with converse some protections are also provided You can configure what skills are allowed to use the fallback mechanism, you can also modify the fallback priority to ensure skills behave well together. Since priority is defined by developers sometimes the default value is not appropriate and does not fit well with the installed skills collection \"skills\": { // fallback skill configuration \"fallbacks\": { // you can add skill_id: priority to override the developer defined // priority of those skills, this allows customization // of unknown intent handling for default_skills + user preferences \"fallback_priorities\": { // \"skill_id\": 10 }, // fallback skill handling has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call fallback for skills in \"fallback_whitelist\" // - \"blacklist\" # only call fallback for skills NOT in \"fallback_blacklist\" \"fallback_mode\": \"accept_all\", \"fallback_whitelist\": [], \"fallback_blacklist\": [] } }, Intent Engines ovos-core currently supports the same intent parsers as mycroft, a plugin solution is being actively developed @ovos-plugin-manager/pull/47 While plugin support is not implemented we provide a drop in replacement alternative to padatious that is lightweight and performs similarly. This was done to avoid the non-python dependency on libfann2 which made core hard to package in some systems NOTE: fann2 is LGPL licensed, it is one of the exceptions to our licensing policy Padacioso will be used if padatious is not installed or if explicitly enabled in mycroft.conf \"padatious\": { \"intent_cache\": \"~/.local/share/mycroft/intent_cache\", \"train_delay\": 4, \"single_thread\": false, // fallback settings for padacioso (pure regex) \"regex_only\": false, \"fuzz\": true },","title":"Skills"},{"location":"skills_service/#skills-service","text":"The skills service is responsible for loading skills and intent parsers All user queries are handled by the skills service, you can think of it as OVOS's brain","title":"Skills Service"},{"location":"skills_service/#configuration","text":"\"skills\": { // don't start loading skills until internet is detected // this config value is not present in mycroft-core (internet is required) // ovos-core expects that some instances will be running fully offline \"wait_for_internet\": false, // folder name, relative to \"data_dir\" \"directory\": \"skills\", // used by selene marketplace integration \"upload_skill_manifest\": true, // blacklisted skills to not load // NB: This is the skill_id, usually the basename() of the directory where the skill lives, so if // the skill you want to blacklist is in /usr/share/mycroft/skills/mycroft-alarm.mycroftai/ // then you should write `[\"mycroft-alarm.mycroftai\"]` below. \"blacklisted_skills\": [], // DEPRECATED: priority skills to be loaded first // setup.py installed skills take precedence over this // this option is an artifact from the past when internet and pairing blocked skill loading \"priority_skills\": [], // fallback skill configuration (see below) \"fallbacks\": {...}, // converse stage configuration (see below) \"converse\": {...} },","title":"Configuration"},{"location":"skills_service/#converse","text":"A malicious or badly designed skill using the converse method can potentially hijack the whole conversation loop and render the skills service unusable Some settings are exposed to add some limitations to which skills can be activated and under what circumstances The concept of \"converse priority\" is under active development, work on a \"converse session\" mechanism is also underway @ovos-core/pull/160 \"skills\": { // converse stage configuration \"converse\": { // the default number of seconds a skill remains active, // if the user does not interact with the skill in this timespan it // will be deactivated, default 5 minutes (same as mycroft) \"timeout\": 300, // override of \"skill_timeouts\" per skill_id // you can configure specific skills to remain active longer \"skill_timeouts\": {}, // conversational mode has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call converse for skills in \"converse_whitelist\" // - \"blacklist\" # only call converse for skills NOT in \"converse_blacklist\" \"converse_mode\": \"accept_all\", \"converse_whitelist\": [], \"converse_blacklist\": [], // converse activation has 4 modes of operations: // - \"accept_all\" # default mycroft-core behavior, any skill can // # activate itself unconditionally // - \"priority\" # skills can only activate themselves if no skill with // # higher priority is active // - \"whitelist\" # only skills in \"converse_whitelist\" can activate themselves // - \"blacklist\" # only skills NOT in converse \"converse_blacklist\" can activate themselves // NOTE: this does not apply for regular skill activation, only to skill // initiated activation requests, eg, self.make_active() \"converse_activation\": \"accept_all\", // number of consecutive times a skill is allowed to activate itself // per minute, -1 for no limit (default), 0 to disable self-activation \"max_activations\": -1, // override of \"max_activations\" per skill_id // you can configure specific skills to activate more/less often \"skill_activations\": {}, // if false only skills can activate themselves // if true any skill can activate any other skill \"cross_activation\": true, // if false only skills can deactivate themselves // if true any skill can deactivate any other skill // NOTE: skill deactivation is not yet implemented \"cross_deactivation\": true, // you can add skill_id: priority to override the developer defined // priority of those skills, // converse priority is work in progress and not yet exposed to skills // priority is assumed to be 50 // the only current source for converse priorities is this setting \"converse_priorities\": { // \"skill_id\": 10 } } },","title":"Converse"},{"location":"skills_service/#fallback-skills","text":"Just like with converse a badly designed or malicious skill can hijack the fallback skill loop, while this is not as serious as with converse some protections are also provided You can configure what skills are allowed to use the fallback mechanism, you can also modify the fallback priority to ensure skills behave well together. Since priority is defined by developers sometimes the default value is not appropriate and does not fit well with the installed skills collection \"skills\": { // fallback skill configuration \"fallbacks\": { // you can add skill_id: priority to override the developer defined // priority of those skills, this allows customization // of unknown intent handling for default_skills + user preferences \"fallback_priorities\": { // \"skill_id\": 10 }, // fallback skill handling has 3 modes of operations: // - \"accept_all\" # default mycroft-core behavior // - \"whitelist\" # only call fallback for skills in \"fallback_whitelist\" // - \"blacklist\" # only call fallback for skills NOT in \"fallback_blacklist\" \"fallback_mode\": \"accept_all\", \"fallback_whitelist\": [], \"fallback_blacklist\": [] } },","title":"Fallback Skills"},{"location":"skills_service/#intent-engines","text":"ovos-core currently supports the same intent parsers as mycroft, a plugin solution is being actively developed @ovos-plugin-manager/pull/47 While plugin support is not implemented we provide a drop in replacement alternative to padatious that is lightweight and performs similarly. This was done to avoid the non-python dependency on libfann2 which made core hard to package in some systems NOTE: fann2 is LGPL licensed, it is one of the exceptions to our licensing policy Padacioso will be used if padatious is not installed or if explicitly enabled in mycroft.conf \"padatious\": { \"intent_cache\": \"~/.local/share/mycroft/intent_cache\", \"train_delay\": 4, \"single_thread\": false, // fallback settings for padacioso (pure regex) \"regex_only\": false, \"fuzz\": true },","title":"Intent Engines"},{"location":"speech_service/","text":"Speech Client The speech client is responsible for loading STT, VAD and Wake Word plugins Speech is transcribed into text and forwarded to the skills service Hotwords OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . This is usually not desired unless you are looking to completely disabled wake word usage \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, \"hotwords\": { \"hey mycroft\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true, \"sound\": \"snd/start_listening.wav\" }, \"wake up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true } }, STT Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"fallback_module\": \"ovos-stt-plugin-vosk\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} }, Listener You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading \"listener\": { \"sample_rate\": 16000, // if enabled the noise level is saved to a ipc file, useful for // debuging if microphone is working but writes a lot to disk, // recommended that you set \"ipc_path\" to a tmpfs \"mic_meter_ipc\": true, // Set 'save_path' to configure the location of files stored if // 'record_wake_words' and/or 'save_utterances' are set to 'true'. // WARNING: Make sure that user 'mycroft' has write-access on the // directory! // \"save_path\": \"/tmp\", // Set 'record_wake_words' to save a copy of wake word triggers // as .wav files under: /'save_path'/mycroft_wake_words \"record_wake_words\": false, // Set 'save_utterances' to save each sentence sent to STT -- by default // they are only kept briefly in-memory. This can be useful for for // debugging or other custom purposes. Recordings are saved // under: /'save_path'/mycroft_utterances/<TIMESTAMP>.wav \"save_utterances\": false, \"wake_word_upload\": { \"disable\": false, \"url\": \"https://training.mycroft.ai/precise/upload\" }, // Override as SYSTEM or USER to select a specific microphone input instead of // the PortAudio default input. // \"device_name\": \"somename\", // can be regex pattern or substring // or // \"device_index\": 12, // Stop listing to the microphone during playback to prevent accidental triggering // This is enabled by default, but instances with good microphone noise cancellation // can disable this to listen all the time, allowing 'barge in' functionality. \"mute_during_output\" : true, // How much (if at all) to 'duck' the speaker output during listening. A // setting of 0.0 will not duck at all. A 1.0 will completely mute output // while in a listening state. Values in between will lower the volume // partially (this is optional behavior, depending on the enclosure). \"duck_while_listening\" : 0.3, // In milliseconds \"phoneme_duration\": 120, \"multiplier\": 1.0, \"energy_ratio\": 1.5, // Settings used by microphone to set recording timeout \"recording_timeout\": 10.0, \"recording_timeout_with_silence\": 3.0, // instant listen is an experimental setting, it removes the need for // the pause between \"hey mycroft\" and starting to speak the utterance, //however it might slightly downgrade STT accuracy depending on engine used \"instant_listen\": false }, VAD Voice Activity Detection is used by the speech client to determine when a user stopped speaking, this indicates the voice command is ready to be executed. Several VAD strategies are supported \"listener\": { // Voice Activity Detection is used to determine when speech ended \"VAD\": { // silence method defined the main vad strategy // valid values: // VAD_ONLY - Only use vad // RATIO_ONLY - Only use max/current energy ratio threshold // CURRENT_ONLY - Only use current energy threshold // VAD_AND_RATIO - Use vad and max/current energy ratio threshold // VAD_AND_CURRENT - Use vad and current energy threshold // ALL - Use vad, max/current energy ratio, and current energy threshold // NOTE: if a vad plugin is not available method will fallback to RATIO_ONLY \"silence_method\": \"vad_and_ratio\", // Seconds of speech before voice command has begun \"speech_seconds\": 0.1, // Seconds of silence before a voice command has finished \"silence_seconds\": 0.5, // Seconds of audio to keep before voice command has begun \"before_seconds\": 0.5, // Minimum length of voice command (seconds) // NOTE: max_seconds uses recording_timeout listener setting \"min_seconds\": 1, // Ratio of max/current energy below which audio is considered speech \"max_current_ratio_threshold\": 2, // Energy threshold above which audio is considered speech // NOTE: this is dynamic, only defining start value \"initial_energy_threshold\": 1000.0, // vad module can be any plugin, by default it is not used // recommended plugin: \"ovos-vad-plugin-silero\" \"module\": \"\", \"ovos-vad-plugin-silero\": {\"threshold\": 0.2}, \"ovos-vad-plugin-webrtcvad\": {\"vad_mode\": 3} } },","title":"Speech"},{"location":"speech_service/#speech-client","text":"The speech client is responsible for loading STT, VAD and Wake Word plugins Speech is transcribed into text and forwarded to the skills service","title":"Speech Client"},{"location":"speech_service/#hotwords","text":"OVOS allows you to load any number of hot words in parallel and trigger different actions when they are detected each hotword can do one or more of the following: trigger listening, also called a wake_word play a sound emit a bus event take ovos-core out of sleep mode, also called a wakeup_word or standup_word take ovos-core out of recording mode, also called a stop_word To add a new hotword add its configuration under \"hotwords\" section. By default, all hotwords are disabled unless you set \"active\": true . Under the \"listener\" setting a main wake word and stand up word are defined, those will be automatically enabled unless you set \"active\": false . This is usually not desired unless you are looking to completely disabled wake word usage \"listener\": { // Default wake_word and stand_up_word will be automatically set to active // unless explicitly disabled under \"hotwords\" section \"wake_word\": \"hey mycroft\", \"stand_up_word\": \"wake up\" }, \"hotwords\": { \"hey mycroft\": { \"module\": \"ovos-ww-plugin-precise\", \"version\": \"0.3\", \"model\": \"https://github.com/MycroftAI/precise-data/raw/models-dev/hey-mycroft.tar.gz\", \"phonemes\": \"HH EY . M AY K R AO F T\", \"threshold\": 1e-90, \"lang\": \"en-us\", \"listen\": true, \"sound\": \"snd/start_listening.wav\" }, \"wake up\": { \"module\": \"ovos-ww-plugin-pocketsphinx\", \"phonemes\": \"W EY K . AH P\", \"threshold\": 1e-20, \"lang\": \"en-us\", \"wakeup\": true } },","title":"Hotwords"},{"location":"speech_service/#stt","text":"Two STT plugins may be loaded at once, if the primary plugin fails for some reason the second plugin will be used. This allows you to have a lower accuracy offline model as fallback to account for internet outages, this ensures your device never becomes fully unusable \"stt\": { \"module\": \"ovos-stt-plugin-server\", \"fallback_module\": \"ovos-stt-plugin-vosk\", \"ovos-stt-plugin-server\": {\"url\": \"https://stt.openvoiceos.com/stt\"} },","title":"STT"},{"location":"speech_service/#listener","text":"You can modify microphone settings and enable additional features under the listener section such as wake word / utterance recording / uploading \"listener\": { \"sample_rate\": 16000, // if enabled the noise level is saved to a ipc file, useful for // debuging if microphone is working but writes a lot to disk, // recommended that you set \"ipc_path\" to a tmpfs \"mic_meter_ipc\": true, // Set 'save_path' to configure the location of files stored if // 'record_wake_words' and/or 'save_utterances' are set to 'true'. // WARNING: Make sure that user 'mycroft' has write-access on the // directory! // \"save_path\": \"/tmp\", // Set 'record_wake_words' to save a copy of wake word triggers // as .wav files under: /'save_path'/mycroft_wake_words \"record_wake_words\": false, // Set 'save_utterances' to save each sentence sent to STT -- by default // they are only kept briefly in-memory. This can be useful for for // debugging or other custom purposes. Recordings are saved // under: /'save_path'/mycroft_utterances/<TIMESTAMP>.wav \"save_utterances\": false, \"wake_word_upload\": { \"disable\": false, \"url\": \"https://training.mycroft.ai/precise/upload\" }, // Override as SYSTEM or USER to select a specific microphone input instead of // the PortAudio default input. // \"device_name\": \"somename\", // can be regex pattern or substring // or // \"device_index\": 12, // Stop listing to the microphone during playback to prevent accidental triggering // This is enabled by default, but instances with good microphone noise cancellation // can disable this to listen all the time, allowing 'barge in' functionality. \"mute_during_output\" : true, // How much (if at all) to 'duck' the speaker output during listening. A // setting of 0.0 will not duck at all. A 1.0 will completely mute output // while in a listening state. Values in between will lower the volume // partially (this is optional behavior, depending on the enclosure). \"duck_while_listening\" : 0.3, // In milliseconds \"phoneme_duration\": 120, \"multiplier\": 1.0, \"energy_ratio\": 1.5, // Settings used by microphone to set recording timeout \"recording_timeout\": 10.0, \"recording_timeout_with_silence\": 3.0, // instant listen is an experimental setting, it removes the need for // the pause between \"hey mycroft\" and starting to speak the utterance, //however it might slightly downgrade STT accuracy depending on engine used \"instant_listen\": false },","title":"Listener"},{"location":"speech_service/#vad","text":"Voice Activity Detection is used by the speech client to determine when a user stopped speaking, this indicates the voice command is ready to be executed. Several VAD strategies are supported \"listener\": { // Voice Activity Detection is used to determine when speech ended \"VAD\": { // silence method defined the main vad strategy // valid values: // VAD_ONLY - Only use vad // RATIO_ONLY - Only use max/current energy ratio threshold // CURRENT_ONLY - Only use current energy threshold // VAD_AND_RATIO - Use vad and max/current energy ratio threshold // VAD_AND_CURRENT - Use vad and current energy threshold // ALL - Use vad, max/current energy ratio, and current energy threshold // NOTE: if a vad plugin is not available method will fallback to RATIO_ONLY \"silence_method\": \"vad_and_ratio\", // Seconds of speech before voice command has begun \"speech_seconds\": 0.1, // Seconds of silence before a voice command has finished \"silence_seconds\": 0.5, // Seconds of audio to keep before voice command has begun \"before_seconds\": 0.5, // Minimum length of voice command (seconds) // NOTE: max_seconds uses recording_timeout listener setting \"min_seconds\": 1, // Ratio of max/current energy below which audio is considered speech \"max_current_ratio_threshold\": 2, // Energy threshold above which audio is considered speech // NOTE: this is dynamic, only defining start value \"initial_energy_threshold\": 1000.0, // vad module can be any plugin, by default it is not used // recommended plugin: \"ovos-vad-plugin-silero\" \"module\": \"\", \"ovos-vad-plugin-silero\": {\"threshold\": 0.2}, \"ovos-vad-plugin-webrtcvad\": {\"vad_mode\": 3} } },","title":"VAD"},{"location":"standard_skills/","text":"Standard Skills Usage Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage. Date / Time skill You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date? Setting an alarm Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM. Setting of timers Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes. Asking the weather You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen. File Browser The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices","title":"Standard Skills Usage"},{"location":"standard_skills/#standard-skills-usage","text":"Your OpenVoiceOS device comes with certain skills pre-installed for basic functionality out of the box. You can also install new skills however more about that at a later stage.","title":"Standard Skills Usage"},{"location":"standard_skills/#date-time-skill","text":"You can ask your device what time or date it is just in case you lost your watch. Hey Mycroft, what time is it? Hey Mycroft, what is the date?","title":"Date / Time skill"},{"location":"standard_skills/#setting-an-alarm","text":"Having your OpenVoiceOS device knowing and showing the time is great, but it is even better to be woken up in the morning by your device. Hey Mycroft, set an alarm for 8 AM.","title":"Setting an alarm"},{"location":"standard_skills/#setting-of-timers","text":"Sometimes you are just busy but want to be alerted after a certain time. For that you can use timers. Hey Mycroft, set a timer for 5 minutes. You can always set more timers and even name them, so you know which timers is for what. Hey, Mycroft, set another timer called rice cooking for 7 minutes.","title":"Setting of timers"},{"location":"standard_skills/#asking-the-weather","text":"You can ask your device what the weather is or would be at any given time or place. Hey Mycroft, what is the weather like today? The weather skill actually uses multiple pages indicated by the small dots at the bottom of the screen.","title":"Asking the weather"},{"location":"standard_skills/#file-browser","text":"The file browser allows you to browse the filesystem in your device and any connected media, you can view images and play music and videos. KDEConnect integration allows you to share files with your mobile devices","title":"File Browser"},{"location":"statements/","text":"Statements Speaking a statement One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism. Multilingualism To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\". The Tomato Skill Revisited To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement. Simple statement The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file. File locations The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us Statements with variables The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\". Waiting for speech By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this. Wait Parameter We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\") wait_while_speaking The mycroft.audio.wait_while_speaking() method allows us to execute some code, then wait for OVOS to finish speaking. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Returns from speak_dialog() before the speaking has completed, and logs some info. Then it, waits for the speech to complete. \"\"\" self.speak_dialog('tomato.description') self.log.info(\"I am executed immediately\") wait_while_speaking() self.log.info(\"But I waited for you\") Here we have executed one line of code immediately. Our Skill will then wait for the statement from i.do.like.dialog to be spoken before executing the final line of code. Using translatable resources There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The MycroftSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Dialog"},{"location":"statements/#statements","text":"","title":"Statements"},{"location":"statements/#speaking-a-statement","text":"One of OVOS's most important core capabilities is to convert text to speech, that is, to speak a statement. Within a Skill's Intent handler, you may pass a string of text to OVOS and OVOS will speak it. For example: self.speak('this is my statement') That's cool and fun to experiment with, but passing strings of text to Mycroft doesn't help to make Mycroft a multilingual product. Rather than hard-coded strings of text, OVOS has a design pattern for multilingualism.","title":"Speaking a statement"},{"location":"statements/#multilingualism","text":"To support multilingualism, the text that OVOS speaks must come from a file. That file is called a dialog file. The dialog file contains statements (lines of text) that a listener in a particular language would consider to be equivalent. For instance, in USA English, the statements \"I am okay\" and \"I am fine\" are equivalent, and both of these statements might appear in a dialog file used for responding to the USA English question: \"How are you?\". By convention, the dialog filename is formed by dot connected words and must end with \".dialog\". The dialog filename should be descriptive of the contents as a whole. Sometimes, the filename describes the question being answered, and other times, the filename describes the answer itself. For the example above, the dialog filename might be: how.are.you.dialog or i.am.fine.dialog . Multilingualism is accomplished by translating the dialog files into other languages, and storing them in their own directory named for the country and language. The filenames remain the same. Using the same filenames in separate language dependent directories allows the Skills to be language agnostic; no hard-coded text strings. Adjust the language setting for your Device **** and OVOS uses the corresponding set of dialog files. If the desired file does not exist in the directory for that language, Mycroft will use the file from the USA English directory. As an example of the concept, the contents of how.are.you.dialog in the directory for the French language in France (fr-fr) might include the statement: \"Je vais bien\".","title":"Multilingualism"},{"location":"statements/#the-tomato-skill-revisited","text":"To demonstrate the multilingualism design pattern, we examine the usage of the speak_dialog() method in the Tomato Skill . The Tomato Skill has two Intents: one demonstrates simple, straightforward statements, and the other demonstrates the use of variables within a statement.","title":"The Tomato Skill Revisited"},{"location":"statements/#simple-statement","text":"The first Intent within the Tomato Skill, what.is.a.tomato.intent , handles inquiries about tomatoes, and the dialog file, tomato.description.dialog , provides the statements for OVOS to speak in reply to that inquiry. Sample contents of the Intent and dialog files: what.is.a.tomato.intent what is a tomato what would you say a tomato is describe a tomato what defines a tomato tomato.description.dialog The tomato is a fruit of the nightshade family A tomato is an edible berry of the plant Solanum lycopersicum A tomato is a fruit but nutrionists consider it a vegetable Observe the statements in the tomato.description.dialog file. They are all acceptable answers to the question: \"What is a tomato?\" Providing more than one statement in a dialog file is one way to make OVOS to seem less robotic, more natural. OVOS will randomly select one of the statements. The Tomato Skill code snippet: @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file.\"\"\" self.speak_dialog('tomato.description') With the Tomato Skill installed, if the User utters **** \"Hey Mycroft, what is a tomato?\", the Intent handler method handle_what_is() will be called. Inside handle_what_is() , we find: self.speak_dialog('tomato.description') As you can probably guess, the parameter 'tomato.description' is the dialog filename without the \".dialog\" extension. Calling this method opens the dialog file, selects one of the statements, and converts that text to speech. OVOS will speak a statement from the dialog file. In this example, OVOS might say \"The tomato is a fruit of the nightshade family\". Remember, OVOS has a language setting that determines from which directory to find the dialog file.","title":"Simple statement"},{"location":"statements/#file-locations","text":"The Skill Structure section describes where to place the Intent file and dialog file. Basically, there are two choices: Put both files in locale/en-us Put the dialog file in dialog/en-us , and put the Intent file in vocab/en-us","title":"File locations"},{"location":"statements/#statements-with-variables","text":"The second Padatious Intent, do.you.like.intent , demonstrates the use of variables in the Intent file and in one of the dialog files: do.you.like.intent do you like tomatoes do you like {type} tomatoes like.tomato.type.dialog I do like {type} tomatoes {type} tomatoes are my favorite like.tomato.generic.dialog I do like tomatoes tomatoes are my favorite Compare these two dialog files. The like.tomato.generic.dialog file contains only simple statements. The statements in the like.tomato.type.dialog file include a variable named type . The variable is a placeholder in the statement specifying where text may be inserted. The speak_dialog() method accepts a dictionary as an optional parameter. If that dictionary contains an entry for a variable named in the statement, then the value from the dictionary will be inserted at the placeholder's location. Dialog file variables are formed by surrounding the variable's name with curly braces. In OVOS parlance, curly braces are known as a mustache . For multi-line dialog files, be sure to include the same variable on all lines. The Tomato Skill code snippet: @intent_handler('do.you.like.intent') def handle_do_you_like(self, message): tomato_type = message.data.get('type') if tomato_type is not None: self.speak_dialog('like.tomato.type', {'type': tomato_type}) else: self.speak_dialog('like.tomato.generic') When the User utters \"Hey Mycroft, do you like RED tomatoes?\", the second of the two Intent lines \"do you like {type} tomatoes\" is recognized by Mycroft, and the value 'RED' is returned in the message dictionary assigned to the 'type' entry when handle_do_you_like() is called. The line tomato_type = message.data.get('type') extracts the value from the dictionary for the entry 'type'. In this case, the variable tomato_type will receive the value 'RED', and speak_dialog() will be called with the 'like.tomato.type' dialog file, and a dictionary with 'RED' assigned to 'type'. The statement \"I do like {type} tomatoes\" might be randomly selected, and after insertion of the value 'RED' for the placeholder variable {type}, OVOS would say: \"I do like RED tomatoes\". Should the User utter \"Hey Mycroft, do you like tomatoes?\", the first line in the Intent file \"do you like tomatoes\" is recognized. There is no variable in this line, and when handle_do_you_like() is called, the dictionary in the message is empty. This means tomato_type is None , speak_dialog('like.tomato.generic') would be called, and Mycroft might reply with \"Yes, I do like tomatoes\".","title":"Statements with variables"},{"location":"statements/#waiting-for-speech","text":"By default, the speak_dialog() method is non-blocking. That is any code following the call to speak_dialog() will execute whilst OVOS is talking. This is useful to allow your Skill to perform actions while it is speaking. Rather than telling the User that we are fetching some data, then going out to fetch it, we can do the two things simultaneously providing a better experience. However, there are times when we need to wait until the statement has been spoken before doing something else. We have two options for this.","title":"Waiting for speech"},{"location":"statements/#wait-parameter","text":"We can pass a wait=True parameter to our speak_dialog() method. This makes the method blocking and no other code will execute until the statement has been spoken. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Waits (i.e. blocks) within speak_dialog() until the speaking has completed. \"\"\" self.speak_dialog('tomato.description', wait=True) self.log.info(\"I waited for you\")","title":"Wait Parameter"},{"location":"statements/#wait_while_speaking","text":"The mycroft.audio.wait_while_speaking() method allows us to execute some code, then wait for OVOS to finish speaking. @intent_handler('what.is.a.tomato.intent') def handle_what_is(self, message): \"\"\"Speaks a statement from the dialog file. Returns from speak_dialog() before the speaking has completed, and logs some info. Then it, waits for the speech to complete. \"\"\" self.speak_dialog('tomato.description') self.log.info(\"I am executed immediately\") wait_while_speaking() self.log.info(\"But I waited for you\") Here we have executed one line of code immediately. Our Skill will then wait for the statement from i.do.like.dialog to be spoken before executing the final line of code.","title":"wait_while_speaking"},{"location":"statements/#using-translatable-resources","text":"There may be a situation where the dialog file and the speak_dialog() method do not give the Skill enough flexibility. For instance, there may be a need to manipulate the statement from the dialog file before having it spoken by OVOS. The MycroftSkill class provides four multilingual methods to address these needs. Each method uses a file, and multilingualism is accomplished using the country/language directory system. The translate() method returns a random string from a \".dialog\" file (modified by a data dictionary). The translate_list() method returns a list of strings from a \".list\" file (each modified by the data dictionary). Same as translate_template() just with a different file extension. The translate_namedvalue() method returns a dictionary formed from CSV entries in a \".value\" file. The translate_template() method returns a list of strings from a \".template\" file (each modified by the data dictionary). Same as translate_list() just with a different file extension.","title":"Using translatable resources"},{"location":"stt_plugins/","text":"STT Plugins STT plugins are responsible for converting spoken audio into text List of STT plugins Plugin Offline Type ovos-stt-plugin-vosk yes FOSS ovos-stt-plugin-chromium no API (free) neon-stt-plugin-google_cloud_streaming no API (key) neon-stt-plugin-scribosermo yes FOSS neon-stt-plugin-silero yes FOSS neon-stt-plugin-polyglot yes FOSS neon-stt-plugin-deepspeech_stream_local yes FOSS ovos-stt-plugin-selene no API (free) ovos-stt-plugin-http-server no API (self hosted) ovos-stt-plugin-pocketsphinx yes FOSS Standalone Usage STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang) Plugin Template from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Speech To Text"},{"location":"stt_plugins/#stt-plugins","text":"STT plugins are responsible for converting spoken audio into text","title":"STT Plugins"},{"location":"stt_plugins/#list-of-stt-plugins","text":"Plugin Offline Type ovos-stt-plugin-vosk yes FOSS ovos-stt-plugin-chromium no API (free) neon-stt-plugin-google_cloud_streaming no API (key) neon-stt-plugin-scribosermo yes FOSS neon-stt-plugin-silero yes FOSS neon-stt-plugin-polyglot yes FOSS neon-stt-plugin-deepspeech_stream_local yes FOSS ovos-stt-plugin-selene no API (free) ovos-stt-plugin-http-server no API (self hosted) ovos-stt-plugin-pocketsphinx yes FOSS","title":"List of STT plugins"},{"location":"stt_plugins/#standalone-usage","text":"STT plugins can be used in your owm projects as follows from speech_recognition import Recognizer, AudioFile plug = STTPlug() # verify lang is supported lang = \"en-us\" assert lang in plug.available_languages # read file with AudioFile(\"test.wav\") as source: audio = Recognizer().record(source) # transcribe AudioData object transcript = plug.execute(audio, lang)","title":"Standalone Usage"},{"location":"stt_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.stt import STT # base plugin class class MySTTPlugin(STT): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # read config settings for your plugin lm = self.config.get(\"language-model\") hmm = self.config.get(\"acoustic-model\") def execute(self, audio, language=None): # TODO - convert audio into text and return string transcript = \"You said this\" return transcript @property def available_languages(self): \"\"\"Return languages supported by this STT implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this STT handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MySTTConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MySTT ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"stt_server/","text":"OpenVoiceOS STT HTTP Server Turn any OVOS STT plugin into a microservice! Install pip install ovos-stt-http-server Usage ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host Companion plugin Use with OpenVoiceOS companion plugin Docker Template you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"STT Server"},{"location":"stt_server/#openvoiceos-stt-http-server","text":"Turn any OVOS STT plugin into a microservice!","title":"OpenVoiceOS STT HTTP Server"},{"location":"stt_server/#install","text":"pip install ovos-stt-http-server","title":"Install"},{"location":"stt_server/#usage","text":"ovos-stt-server --help usage: ovos-stt-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] options: -h, --help show this help message and exit --engine ENGINE stt plugin to be used --port PORT port number --host HOST host","title":"Usage"},{"location":"stt_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"stt_server/#docker-template","text":"you can create easily create a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-stt-http-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-stt-http-server --engine {PLUGIN_HERE} build it docker build . -t my_ovos_stt_plugin run it docker run -p 8080:9666 my_ovos_stt_plugin Each plugin can provide its own Dockerfile in its repository using ovos-stt-http-server","title":"Docker Template"},{"location":"timeline/","text":"Fork timeline mycroft-core created neon-core forked from mycroft-core chatterbox forked from mycroft-core mycroft-lib forked from mycroft-core to become a library, it is only a properly packaged mycroft-core chatterbox rewritten to use mycroft-lib internally, no longer a fork neon-core rewritten to use mycroft-lib internally, no longer a fork mycroft-lib renamed to HolmesIV to avoid trademark issues HolmesV created to use HolmesIV internally, HolmesV gets features and bug fixes chatterbox updated to HolmesV neon-core updated to HolmesV HolmesIV+HolmesV abandoned, chatterbox focus on closed source product ovos-core forked from HolmesV neon-core updated to ovos-core mycroft-dinkum forked from mycroft-core Events timeline Aug 2015 - MycroftAI kickstarter launch Feb 2016 - MycroftAI Mimic TTS released May 2016 - mycroft-core repositories made public under GPL Jun 2016 - @Aix releases MycroftAI gnome shell extension Jun 2016 - @Aix becomes a contributor on the desktop-integration team Aug 2016 - Steve Penrod becomes MycroftAI CTO ??? 2016 - NeonGecko begins working with Mycroft AI Jan 2017 - @Aix release MycrofAI plasmoid for KDE Plasma desktop Mar 2017 - @Aix incubinates Mycroft plasmoid project under KDE Apr 2017 - @JarbasAI forks mycroft-core as jarbas-core May 2017 - @JarbasAI becomes a mycroft-core contributor Sep 2017 - MycroftAI Mark 1 released / mycroft kickstarter campaign fullfilled Sep 2017 - @Aix joins Blue Systems to work on MycroftAI and Plasma integration projects Sep 2017 - jarbas-core tacotron TTS integration added Aug 2017 - MycroftAI starts supporting KDE plasmoid project and installers Oct 2017 - mycroft-core relicensed as Apache 2.0 Nov 2017 - @Aix becomes a mycroft-core contributor Dec 2017 - jarbas-core abandoned ??? 2017 - Neon AI forks mycroft-core as neon-core to remove dependency on MycroftAI servers and work on other features Jan 2018 - initial release of personal-backend, reverse engineered MycroftAI backend by @JarbasAI, licensed as Apache 2.0 Jan 2018 - MycroftAI mark 2 kickstarter launch Jul 2018 - personal-backend added to MycroftAI Roadmap Aug 2018 - MycroftAI Mimic2 TTS based on tacotron released Sep 2018 - Mycroft-GUI was created by collaboration between MycroftAI, Blue Systems based on KDE frameworks, maintained by @Aix Oct 2018 - @j1nx creates \"MycroftOS,\" the first version of what will eventually become the OpenVoiceOS smart speaker Jan 2019 - @JarbasAI personal-backend implementation adopted by MycroftAI Mar 2019 - MycroftAI mark 2 completely redesigned and based on different hardware Apr 2019 - Steve Penrod and @JarbasAI create lingua-franca under MycroftAI with @JarbasAI as maintainer ??? 2019 - @JarbasAI partners with NeonAI to maintain neon-core Jul 2019 - steve penrod leaves MycroftAI Jul 2019 - Chatterbox kickstarter launched by @JarbasAI Oct 2019 - Official MycroftAI backend open sourced, licensed as AGPL Dec 2019 - @Chance joins lingua-franca as a maintainer Dec 2019 - Chatterbox released (closed source) / kickstarter campaign fullfilled Dec 2019 - MycroftAI starts to work on Kivy based UI ??? 2020 - MycroftAI starts neglecting community contributions Mar 2020 - MycroftAI abandons personal-backend Mar 2020 - Michael Lewis becomes MycroftAI CEO May 2020 - @JarbasAI and @AIX partner up to create a VOIP skill Sep 2020 - Community takes stand against reimplementation of GUI using Kivy Oct 2020 - Kivy has been dropped in support for Mycroft-GUI Oct 2020 - @JarbasAI, @AIX and @j1nx form OpenVoiceOS community project around mycroft-core ??? 2020 - Chatterbox forks mycroft-core as mycroft-lib (open source) ??? 2020 - Chatterbox (closed source) recreated on top of mycroft-lib, instead of a mycroft-core fork ??? 2020 - neon-core recreated on top of mycroft-lib, instead of a mycroft-core fork Oct 2020 - MycroftOS renamed to OpenVoiceOS - Mycroft Edition due to trademark issues ??? 2020 - mycroft-lib renamed to HolmesIV to avoid trademark issues ??? 2020 - @JarbasAI leaves lingua-franca ??? 2020 - @Chance joins OpenVoiceOS Jan 2021 - personal-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred Jan 2021 - Mycroft embedded shell adopted by OpenVoiceOS and renamed to ovos-shell Mar 2021 - MycroftAI mark 2 dev kits start shipping Apr 2021 - OpenVoiceOS adopts mycroft-lib instead of mycroft-core May 2021 - chatterbox forks lingua-franca as lingua-nostra May 2021 . OpenVoiceOS deprecates mycroft-lib and adopts HolmesIV Sep 2021 - HolmesIV abandoned by chatterbox Sep 2021 - lingua-nostra abandoned by chatterbox Oct 2021 - OpenVoiceOS forks HolmesIV as ovos-core Oct 2021 - NeonAI deprecates HolmesIV and adopts ovos-core ??? 2021 - @Chance leaves lingua-franca Nov 2021 - @JarbasAI leaves chatterbox ??? - @NeonDaniel joins OpenVoiceOS ??? 2021 - NeonAI adopts ovos-shell Jan 2022 - OpenVoiceOS forks lingua-franca ??? 2022 - OpenVoiceOS starts development in the open via matrix chat ??? 2022 - OpenVoiceOS starts releasing buildroot images for rpi4 and Mark 2 ??? 2022 - OpenVoiceOS starts releasing manjaro images for rpi4 and Mark 2 May 2022 - MycroftAI founders resign Jul 2022 - MycroftAI starts work on mycroft-dinkum behind the scenes, a replacement/rewrite of mycroft-core Sep 2022 - MycroftAI Mimic3 TTS released, AGPL licensed Sep 2022 . MycroftAI Mark 2 starts shipping ??? 2022 - mycroft-dinkum source code made public, Apache2 licensed ??? 2022 - Extremely negative feedback from developer community, several key members annouce no intention to support mycroft-dinkum ??? 2022 - NeonAI starts release NeonOS images for the Mark 2 Nov 2022 - MycroftAI staff lay off, only a skeleton crew remaining Dec 2022 - Home Assistant PHAL plugin initial release by @AIX Jan 2023 - negative user feedback from the community for Mark 2, lots of bugs and missing features Feb 2023 - MycroftAI partners up with NeonAI to maintain mycroft-core(?) Mar 2023 - Funraiser to form OpenVoiceOS V.z.w. (Dutch: \"Vereninging zonder winstoogmerk\") completed in a couple days Mar 2023 - first stretch goal of fundraiser reached and second stretch goal announced Mar 2023 - raspbian-ovos images start being released, maintained by community member @builderjer Mar 2023 - community doc webpage start being maintained by community members Mar 2023 - ovos-ww-plugin-openWakeWord released, maintained by author @dscripka Apr 2023 - @Aix leaves OpenVoiceOS Apr 2023 - precise-lite-trainer released Apr 2023 - ovos-dinkum-listener released May 2023 - ovos-docker released, maintained by community member @goldyfruit May 2023 - Open Voice OS TTS/STT status page released, maintained by community member @goldyfruit May 2023 - First successful run of OpenVoiceOS on Mac OS using containers with ovos-docker May 2023 - ovos-docker-stt released, maintained by community member @goldyfruit May 2023 - ovos-microphone-plugin-sounddevice released, which provides native Mac OS suuport, maintained by community member @goldyfruit Jun 2023 - Home Assistant plugin starts being maintained by community member @mikejgray Jun 2023 - Home Assistant Notify integration released by community member @mikejgray Jun 2023 - First (and second!) successful run of OpenVoiceOS on Windows, using WSL2 and ovos-docker","title":"Timeline"},{"location":"timeline/#fork-timeline","text":"mycroft-core created neon-core forked from mycroft-core chatterbox forked from mycroft-core mycroft-lib forked from mycroft-core to become a library, it is only a properly packaged mycroft-core chatterbox rewritten to use mycroft-lib internally, no longer a fork neon-core rewritten to use mycroft-lib internally, no longer a fork mycroft-lib renamed to HolmesIV to avoid trademark issues HolmesV created to use HolmesIV internally, HolmesV gets features and bug fixes chatterbox updated to HolmesV neon-core updated to HolmesV HolmesIV+HolmesV abandoned, chatterbox focus on closed source product ovos-core forked from HolmesV neon-core updated to ovos-core mycroft-dinkum forked from mycroft-core","title":"Fork timeline"},{"location":"timeline/#events-timeline","text":"Aug 2015 - MycroftAI kickstarter launch Feb 2016 - MycroftAI Mimic TTS released May 2016 - mycroft-core repositories made public under GPL Jun 2016 - @Aix releases MycroftAI gnome shell extension Jun 2016 - @Aix becomes a contributor on the desktop-integration team Aug 2016 - Steve Penrod becomes MycroftAI CTO ??? 2016 - NeonGecko begins working with Mycroft AI Jan 2017 - @Aix release MycrofAI plasmoid for KDE Plasma desktop Mar 2017 - @Aix incubinates Mycroft plasmoid project under KDE Apr 2017 - @JarbasAI forks mycroft-core as jarbas-core May 2017 - @JarbasAI becomes a mycroft-core contributor Sep 2017 - MycroftAI Mark 1 released / mycroft kickstarter campaign fullfilled Sep 2017 - @Aix joins Blue Systems to work on MycroftAI and Plasma integration projects Sep 2017 - jarbas-core tacotron TTS integration added Aug 2017 - MycroftAI starts supporting KDE plasmoid project and installers Oct 2017 - mycroft-core relicensed as Apache 2.0 Nov 2017 - @Aix becomes a mycroft-core contributor Dec 2017 - jarbas-core abandoned ??? 2017 - Neon AI forks mycroft-core as neon-core to remove dependency on MycroftAI servers and work on other features Jan 2018 - initial release of personal-backend, reverse engineered MycroftAI backend by @JarbasAI, licensed as Apache 2.0 Jan 2018 - MycroftAI mark 2 kickstarter launch Jul 2018 - personal-backend added to MycroftAI Roadmap Aug 2018 - MycroftAI Mimic2 TTS based on tacotron released Sep 2018 - Mycroft-GUI was created by collaboration between MycroftAI, Blue Systems based on KDE frameworks, maintained by @Aix Oct 2018 - @j1nx creates \"MycroftOS,\" the first version of what will eventually become the OpenVoiceOS smart speaker Jan 2019 - @JarbasAI personal-backend implementation adopted by MycroftAI Mar 2019 - MycroftAI mark 2 completely redesigned and based on different hardware Apr 2019 - Steve Penrod and @JarbasAI create lingua-franca under MycroftAI with @JarbasAI as maintainer ??? 2019 - @JarbasAI partners with NeonAI to maintain neon-core Jul 2019 - steve penrod leaves MycroftAI Jul 2019 - Chatterbox kickstarter launched by @JarbasAI Oct 2019 - Official MycroftAI backend open sourced, licensed as AGPL Dec 2019 - @Chance joins lingua-franca as a maintainer Dec 2019 - Chatterbox released (closed source) / kickstarter campaign fullfilled Dec 2019 - MycroftAI starts to work on Kivy based UI ??? 2020 - MycroftAI starts neglecting community contributions Mar 2020 - MycroftAI abandons personal-backend Mar 2020 - Michael Lewis becomes MycroftAI CEO May 2020 - @JarbasAI and @AIX partner up to create a VOIP skill Sep 2020 - Community takes stand against reimplementation of GUI using Kivy Oct 2020 - Kivy has been dropped in support for Mycroft-GUI Oct 2020 - @JarbasAI, @AIX and @j1nx form OpenVoiceOS community project around mycroft-core ??? 2020 - Chatterbox forks mycroft-core as mycroft-lib (open source) ??? 2020 - Chatterbox (closed source) recreated on top of mycroft-lib, instead of a mycroft-core fork ??? 2020 - neon-core recreated on top of mycroft-lib, instead of a mycroft-core fork Oct 2020 - MycroftOS renamed to OpenVoiceOS - Mycroft Edition due to trademark issues ??? 2020 - mycroft-lib renamed to HolmesIV to avoid trademark issues ??? 2020 - @JarbasAI leaves lingua-franca ??? 2020 - @Chance joins OpenVoiceOS Jan 2021 - personal-backend adopted by OpenVoiceOS, original repo unarchived and ownership transferred Jan 2021 - Mycroft embedded shell adopted by OpenVoiceOS and renamed to ovos-shell Mar 2021 - MycroftAI mark 2 dev kits start shipping Apr 2021 - OpenVoiceOS adopts mycroft-lib instead of mycroft-core May 2021 - chatterbox forks lingua-franca as lingua-nostra May 2021 . OpenVoiceOS deprecates mycroft-lib and adopts HolmesIV Sep 2021 - HolmesIV abandoned by chatterbox Sep 2021 - lingua-nostra abandoned by chatterbox Oct 2021 - OpenVoiceOS forks HolmesIV as ovos-core Oct 2021 - NeonAI deprecates HolmesIV and adopts ovos-core ??? 2021 - @Chance leaves lingua-franca Nov 2021 - @JarbasAI leaves chatterbox ??? - @NeonDaniel joins OpenVoiceOS ??? 2021 - NeonAI adopts ovos-shell Jan 2022 - OpenVoiceOS forks lingua-franca ??? 2022 - OpenVoiceOS starts development in the open via matrix chat ??? 2022 - OpenVoiceOS starts releasing buildroot images for rpi4 and Mark 2 ??? 2022 - OpenVoiceOS starts releasing manjaro images for rpi4 and Mark 2 May 2022 - MycroftAI founders resign Jul 2022 - MycroftAI starts work on mycroft-dinkum behind the scenes, a replacement/rewrite of mycroft-core Sep 2022 - MycroftAI Mimic3 TTS released, AGPL licensed Sep 2022 . MycroftAI Mark 2 starts shipping ??? 2022 - mycroft-dinkum source code made public, Apache2 licensed ??? 2022 - Extremely negative feedback from developer community, several key members annouce no intention to support mycroft-dinkum ??? 2022 - NeonAI starts release NeonOS images for the Mark 2 Nov 2022 - MycroftAI staff lay off, only a skeleton crew remaining Dec 2022 - Home Assistant PHAL plugin initial release by @AIX Jan 2023 - negative user feedback from the community for Mark 2, lots of bugs and missing features Feb 2023 - MycroftAI partners up with NeonAI to maintain mycroft-core(?) Mar 2023 - Funraiser to form OpenVoiceOS V.z.w. (Dutch: \"Vereninging zonder winstoogmerk\") completed in a couple days Mar 2023 - first stretch goal of fundraiser reached and second stretch goal announced Mar 2023 - raspbian-ovos images start being released, maintained by community member @builderjer Mar 2023 - community doc webpage start being maintained by community members Mar 2023 - ovos-ww-plugin-openWakeWord released, maintained by author @dscripka Apr 2023 - @Aix leaves OpenVoiceOS Apr 2023 - precise-lite-trainer released Apr 2023 - ovos-dinkum-listener released May 2023 - ovos-docker released, maintained by community member @goldyfruit May 2023 - Open Voice OS TTS/STT status page released, maintained by community member @goldyfruit May 2023 - First successful run of OpenVoiceOS on Mac OS using containers with ovos-docker May 2023 - ovos-docker-stt released, maintained by community member @goldyfruit May 2023 - ovos-microphone-plugin-sounddevice released, which provides native Mac OS suuport, maintained by community member @goldyfruit Jun 2023 - Home Assistant plugin starts being maintained by community member @mikejgray Jun 2023 - Home Assistant Notify integration released by community member @mikejgray Jun 2023 - First (and second!) successful run of OpenVoiceOS on Windows, using WSL2 and ovos-docker","title":"Events timeline"},{"location":"translate_server/","text":"OpenVoiceOS Translate Server Turn any OVOS Language plugin into a microservice! Use with OpenVoiceOS companion plugin Install pip install ovos-translate-server Usage ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the Google Translate plugin ovos-translate-server --tx-engine googletranslate_plug --detect-engine googletranslate_detection_plug then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro Docker Template you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"Translate Server"},{"location":"translate_server/#openvoiceos-translate-server","text":"Turn any OVOS Language plugin into a microservice! Use with OpenVoiceOS companion plugin","title":"OpenVoiceOS Translate Server"},{"location":"translate_server/#install","text":"pip install ovos-translate-server","title":"Install"},{"location":"translate_server/#usage","text":"ovos-translate-server --help usage: ovos-translate-server [-h] [--tx-engine TX_ENGINE] [--detect-engine DETECT_ENGINE] [--port PORT] [--host HOST] optional arguments: -h, --help show this help message and exit --tx-engine TX_ENGINE translate plugin to be used --detect-engine DETECT_ENGINE lang detection plugin to be used --port PORT port number --host HOST host eg, to use the Google Translate plugin ovos-translate-server --tx-engine googletranslate_plug --detect-engine googletranslate_detection_plug then you can do get requests http://0.0.0.0:9686/translate/en/o meu nome \u00e9 Casimiro (auto detect source lang) http://0.0.0.0:9686/translate/pt/en/o meu nome \u00e9 Casimiro (specify source lang) http://0.0.0.0:9686/detect/o meu nome \u00e9 Casimiro","title":"Usage"},{"location":"translate_server/#docker-template","text":"you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-translate-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-translate-server --tx-engine {PLUGIN_HERE} --detect-engine {PLUGIN_HERE} build it docker build . -t my_ovos_translate_plugin run it docker run -p 8080:9686 my_ovos_translate_plugin Each plugin can provide its own Dockerfile in its repository using ovos-translate-server","title":"Docker Template"},{"location":"tts_plugins/","text":"TTS Plugins TTS plugins are responsible for converting text into audio for playback List of TTS plugins Plugin Offline Type ovos-tts-plugin-mimic yes FOSS ovos-tts-plugin-mimic2 no API (free) ovos-tts-plugin-mimic3 yes FOSS ovos-tts-plugin-marytts no API (self hosted) neon-tts-plugin-larynx_server no API (self hosted) ovos-tts-server-plugin no API (self hosted) ovos-tts-plugin-pico yes FOSS neon-tts-plugin-glados yes FOSS neon-tts-plugin-mozilla_local yes FOSS neon-tts-plugin-polly no API (key) ovos-tts-plugin-voicerss no API (key) ovos-tts-plugin-google-TX no API (free) ovos-tts-plugin-responsivevoice no API (free) neon-tts-plugin-mozilla_remote no API (self hosted) neon-tts-plugin-tacotron2 yes FOSS ovos-tts-plugin-espeakNG yes FOSS ovos-tts-plugin-cotovia yes FOSS ovos-tts-plugin-catotron no API (self hosted) ovos-tts-plugin-softcatala no API (self hosted) ovos-tts-plugin-SAM yes Abandonware ovos-tts-plugin-beepspeak yes Fun Plugin Template from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Text To Speech"},{"location":"tts_plugins/#tts-plugins","text":"TTS plugins are responsible for converting text into audio for playback","title":"TTS Plugins"},{"location":"tts_plugins/#list-of-tts-plugins","text":"Plugin Offline Type ovos-tts-plugin-mimic yes FOSS ovos-tts-plugin-mimic2 no API (free) ovos-tts-plugin-mimic3 yes FOSS ovos-tts-plugin-marytts no API (self hosted) neon-tts-plugin-larynx_server no API (self hosted) ovos-tts-server-plugin no API (self hosted) ovos-tts-plugin-pico yes FOSS neon-tts-plugin-glados yes FOSS neon-tts-plugin-mozilla_local yes FOSS neon-tts-plugin-polly no API (key) ovos-tts-plugin-voicerss no API (key) ovos-tts-plugin-google-TX no API (free) ovos-tts-plugin-responsivevoice no API (free) neon-tts-plugin-mozilla_remote no API (self hosted) neon-tts-plugin-tacotron2 yes FOSS ovos-tts-plugin-espeakNG yes FOSS ovos-tts-plugin-cotovia yes FOSS ovos-tts-plugin-catotron no API (self hosted) ovos-tts-plugin-softcatala no API (self hosted) ovos-tts-plugin-SAM yes Abandonware ovos-tts-plugin-beepspeak yes Fun","title":"List of TTS plugins"},{"location":"tts_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.tts import TTS # base plugin class class MyTTSPlugin(TTS): def __init__(self, *args, **kwargs): # in here you should specify if your plugin return wav or mp3 files # you should also specify any valid ssml tags ssml_tags = [\"speak\", \"s\", \"w\", \"voice\", \"prosody\", \"say-as\", \"break\", \"sub\", \"phoneme\"] super().__init__(*args, **kwargs, audio_ext=\"wav\", ssml_tags=ssml_tags) # read config settings for your plugin if any self.pitch = self.config.get(\"pitch\", 0.5) def get_tts(self, sentence, wav_file): # TODO - create TTS audio @ wav_file (path) return wav_file, None @property def available_languages(self): \"\"\"Return languages supported by this TTS implementation in this state This property should be overridden by the derived class to advertise what languages that engine supports. Returns: set: supported languages \"\"\" # TODO - what langs can this TTS handle? return {\"en-us\", \"es-es\"} # sample valid configurations per language # \"display_name\" and \"offline\" provide metadata for UI # \"priority\" is used to calculate position in selection dropdown # 0 - top, 100-bottom # all other keys represent an example valid config for the plugin MyTTSConfig = { lang: [{\"lang\": lang, \"display_name\": f\"MyTTS ({lang}\", \"priority\": 70, \"offline\": True}] for lang in [\"en-us\", \"es-es\"] }","title":"Plugin Template"},{"location":"tts_server/","text":"OpenVoiceOS TTS Server Turn any OVOS TTS plugin into a microservice! Install pip install ovos-tts-server Usage ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello Companion plugin Use with OpenVoiceOS companion plugin Docker Template you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"TTS Server"},{"location":"tts_server/#openvoiceos-tts-server","text":"Turn any OVOS TTS plugin into a microservice!","title":"OpenVoiceOS TTS Server"},{"location":"tts_server/#install","text":"pip install ovos-tts-server","title":"Install"},{"location":"tts_server/#usage","text":"ovos-tts-server --help usage: ovos-tts-server [-h] [--engine ENGINE] [--port PORT] [--host HOST] [--cache] options: -h, --help show this help message and exit --engine ENGINE tts plugin to be used --port PORT port number --host HOST host --cache save every synth to disk eg, to use the GladosTTS plugin ovos-tts-server --engine neon-tts-plugin-glados --cache then do a get request http://192.168.1.112:9666/synthesize/hello","title":"Usage"},{"location":"tts_server/#companion-plugin","text":"Use with OpenVoiceOS companion plugin","title":"Companion plugin"},{"location":"tts_server/#docker-template","text":"you can create easily crete a docker file to serve any plugin FROM python:3.7 RUN pip3 install ovos-utils==0.0.15 RUN pip3 install ovos-plugin-manager==0.0.4 RUN pip3 install ovos-tts-server==0.0.1 RUN pip3 install {PLUGIN_HERE} ENTRYPOINT ovos-tts-server --engine {PLUGIN_HERE} --cache build it docker build . -t my_ovos_tts_plugin run it docker run -p 8080:9666 my_ovos_tts_plugin use it http://localhost:8080/synthesize/hello Each plugin can provide its own Dockerfile in its repository using ovos-tts-server","title":"Docker Template"},{"location":"vad_plugins/","text":"VAD Plugins Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not List of VAD plugins Plugin Type ovos-vad-plugin-silero model ovos-vad-plugin-webrtcvad model","title":"Voice Activity Detection"},{"location":"vad_plugins/#vad-plugins","text":"Voice Activity Detection is the process of determining when speech starts and ends in a piece of audio VAD plugins classify audio and report if it contains speech or not","title":"VAD Plugins"},{"location":"vad_plugins/#list-of-vad-plugins","text":"Plugin Type ovos-vad-plugin-silero model ovos-vad-plugin-webrtcvad model","title":"List of VAD plugins"},{"location":"why/","text":"About OpenVoiceOS Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About"},{"location":"why/#about-openvoiceos","text":"Introducing OpenVoiceOS - The Free and Open-Source Personal Assistant and Smart Speaker. OpenVoiceOS is a new player in the smart speaker market, offering a powerful and flexible alternative to proprietary solutions like Amazon Echo and Google Home. With OpenVoiceOS, you have complete control over your personal data and the ability to customize and extend the functionality of your smart speaker. Built on open-source software, OpenVoiceOS is designed to provide users with a seamless and intuitive voice interface for controlling their smart home devices, playing music, setting reminders, and much more. The platform leverages cutting-edge technology, including machine learning and natural language processing, to deliver a highly responsive and accurate experience. In addition to its voice capabilities, OpenVoiceOS features a touch-screen GUI made using QT5 and the KF5 framework. The GUI provides an intuitive, user-friendly interface that allows you to access the full range of OpenVoiceOS features and functionality. Whether you prefer voice commands or a more traditional touch interface, OpenVoiceOS has you covered. One of the key advantages of OpenVoiceOS is its open-source nature, which means that anyone with the technical skills can contribute to the platform and help shape its future. Whether you're a software developer, data scientist, or just someone with a passion for technology, you can get involved and help build the next generation of personal assistants and smart speakers. With OpenVoiceOS, you have the option to run the platform fully offline, giving you complete control over your data and ensuring that your information is never shared with third parties. This makes OpenVoiceOS the perfect choice for anyone who values privacy and security. So if you're looking for a personal assistant and smart speaker that gives you the freedom and control you deserve, be sure to check out OpenVoiceOS today! Disclaimer : This post was written in collaboration with ChatGPT","title":"About OpenVoiceOS"},{"location":"ww_plugins/","text":"Wake Word Plugins WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes List of Wake Word plugins Plugin Type ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-vosk text samples ovos-ww-plugin-snowboy model ovos-ww-plugin-precise model ovos-ww-plugin-precise-lite model ovos-ww-plugin-nyumaya model ovos-ww-plugin-nyumaya-legacy model neon_ww_plugin_efficientwordnet model mycroft-porcupine-plugin model ovos-ww-plugin-hotkeys keyboard Standalone Usage first lets get some boilerplate ouf of the way for the microphone handling logic import pyaudio # helper class class CyclicAudioBuffer: def __init__(self, duration=0.98, initial_data=None, sample_rate=16000, sample_width=2): self.size = self.duration_to_bytes(duration, sample_rate, sample_width) initial_data = initial_data or self.get_silence(self.size) # Get at most size bytes from the end of the initial data self._buffer = initial_data[-self.size:] @staticmethod def duration_to_bytes(duration, sample_rate=16000, sample_width=2): return int(duration * sample_rate) * sample_width @staticmethod def get_silence(num_bytes): return b'\\0' * num_bytes def append(self, data): \"\"\"Add new data to the buffer, and slide out data if the buffer is full Arguments: data (bytes): binary data to append to the buffer. If buffer size is exceeded the oldest data will be dropped. \"\"\" buff = self._buffer + data if len(buff) > self.size: buff = buff[-self.size:] self._buffer = buff def get(self): \"\"\"Get the binary data.\"\"\" return self._buffer # pyaudio params FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 16000 CHUNK = 1024 MAX_RECORD_SECONDS = 20 SAMPLE_WIDTH = pyaudio.get_sample_size(FORMAT) audio = pyaudio.PyAudio() # start Recording stream = audio.open(channels=CHANNELS, format=FORMAT, rate=RATE, frames_per_buffer=CHUNK, input=True) def load_plugin(): # Wake word initialization config = {\"model\": \"path/to/hey_computer.model\"} return MyHotWord(\"hey computer\", config=config) def listen_for_ww(plug): # TODO - see examples below return False plug = load_plugin() print(f\"Waiting for wake word {MAX_RECORD_SECONDS} seconds\") found = listen_for_ww(plug) if found: print(\"Found wake word!\") else: print(\"No wake word found\") # stop everything plug.stop() stream.stop_stream() stream.close() audio.terminate() new style plugins New style plugins expect to receive live audio, they may keep their own cyclic buffers internally def listen_for_ww(plug): for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # feed data directly to streaming prediction engines plug.update(data) # streaming engines return result here found = plug.found_wake_word(data) if found: return True old style plugins (DEPRECATED) Old style plugins expect to receive ~3 seconds of audio data at once def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # add data to rolling buffer, used by non-streaming engines audio_buffer.append(data) # non-streaming engines check the byte_data in audio_buffer audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True new + old style plugins (backwards compatibility) if you are unsure what kind of plugin you will be using you can be compatible with both approaches like ovos-core def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # old style engines will ignore the update plug.update(data) # streaming engines will ignore the byte_data audio_buffer.append(data) audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True Plugin Template from ovos_plugin_manager.templates.hotwords import HotWordEngine from threading import Event class MyWWPlugin(HotWordEngine): def __init__(self, key_phrase=\"hey mycroft\", config=None, lang=\"en-us\"): super().__init__(key_phrase, config, lang) self.detection = Event() # read config settings for your plugin self.sensitivity = self.config.get(\"sensitivity\", 0.5) # TODO - plugin stuff # how does your plugin work? phonemes? text? models? self.engine = MyWW(key_phrase) def found_wake_word(self, frame_data): \"\"\"Check if wake word has been found. Checks if the wake word has been found. Should reset any internal tracking of the wake word state. Arguments: frame_data (binary data): Deprecated. Audio data for large chunk of audio to be processed. This should not be used to detect audio data instead use update() to incrementally update audio Returns: bool: True if a wake word was detected, else False \"\"\" detected = self.detection.is_set() if detected: self.detection.clear() return detected def update(self, chunk): \"\"\"Updates the hotword engine with new audio data. The engine should process the data and update internal trigger state. Arguments: chunk (bytes): Chunk of audio data to process \"\"\" if self.engine.found_it(chunk): # TODO - check for wake word self.detection.set() def stop(self): \"\"\"Perform any actions needed to shut down the wake word engine. This may include things such as unloading data or shutdown external processess. \"\"\" self.engine.bye() # TODO - plugin specific shutdown","title":"Wake Word"},{"location":"ww_plugins/#wake-word-plugins","text":"WakeWord plugins classify audio and report if a certain word or sound is present or not These plugins usually correspond to the name of the voice assistant, \"hey mycroft\", but can also be used for other purposes","title":"Wake Word Plugins"},{"location":"ww_plugins/#list-of-wake-word-plugins","text":"Plugin Type ovos-ww-plugin-pocketsphinx phonemes ovos-ww-plugin-vosk text samples ovos-ww-plugin-snowboy model ovos-ww-plugin-precise model ovos-ww-plugin-precise-lite model ovos-ww-plugin-nyumaya model ovos-ww-plugin-nyumaya-legacy model neon_ww_plugin_efficientwordnet model mycroft-porcupine-plugin model ovos-ww-plugin-hotkeys keyboard","title":"List of Wake Word plugins"},{"location":"ww_plugins/#standalone-usage","text":"first lets get some boilerplate ouf of the way for the microphone handling logic import pyaudio # helper class class CyclicAudioBuffer: def __init__(self, duration=0.98, initial_data=None, sample_rate=16000, sample_width=2): self.size = self.duration_to_bytes(duration, sample_rate, sample_width) initial_data = initial_data or self.get_silence(self.size) # Get at most size bytes from the end of the initial data self._buffer = initial_data[-self.size:] @staticmethod def duration_to_bytes(duration, sample_rate=16000, sample_width=2): return int(duration * sample_rate) * sample_width @staticmethod def get_silence(num_bytes): return b'\\0' * num_bytes def append(self, data): \"\"\"Add new data to the buffer, and slide out data if the buffer is full Arguments: data (bytes): binary data to append to the buffer. If buffer size is exceeded the oldest data will be dropped. \"\"\" buff = self._buffer + data if len(buff) > self.size: buff = buff[-self.size:] self._buffer = buff def get(self): \"\"\"Get the binary data.\"\"\" return self._buffer # pyaudio params FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 16000 CHUNK = 1024 MAX_RECORD_SECONDS = 20 SAMPLE_WIDTH = pyaudio.get_sample_size(FORMAT) audio = pyaudio.PyAudio() # start Recording stream = audio.open(channels=CHANNELS, format=FORMAT, rate=RATE, frames_per_buffer=CHUNK, input=True) def load_plugin(): # Wake word initialization config = {\"model\": \"path/to/hey_computer.model\"} return MyHotWord(\"hey computer\", config=config) def listen_for_ww(plug): # TODO - see examples below return False plug = load_plugin() print(f\"Waiting for wake word {MAX_RECORD_SECONDS} seconds\") found = listen_for_ww(plug) if found: print(\"Found wake word!\") else: print(\"No wake word found\") # stop everything plug.stop() stream.stop_stream() stream.close() audio.terminate() new style plugins New style plugins expect to receive live audio, they may keep their own cyclic buffers internally def listen_for_ww(plug): for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # feed data directly to streaming prediction engines plug.update(data) # streaming engines return result here found = plug.found_wake_word(data) if found: return True old style plugins (DEPRECATED) Old style plugins expect to receive ~3 seconds of audio data at once def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # add data to rolling buffer, used by non-streaming engines audio_buffer.append(data) # non-streaming engines check the byte_data in audio_buffer audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True new + old style plugins (backwards compatibility) if you are unsure what kind of plugin you will be using you can be compatible with both approaches like ovos-core def listen_for_ww(plug): # used for old style non-streaming wakeword (deprecated) audio_buffer = CyclicAudioBuffer(plug.expected_duration, sample_rate=RATE, sample_width=SAMPLE_WIDTH) for i in range(0, int(RATE / CHUNK * MAX_RECORD_SECONDS)): data = stream.read(CHUNK) # old style engines will ignore the update plug.update(data) # streaming engines will ignore the byte_data audio_buffer.append(data) audio_data = audio_buffer.get() found = plug.found_wake_word(audio_data) if found: return True","title":"Standalone Usage"},{"location":"ww_plugins/#plugin-template","text":"from ovos_plugin_manager.templates.hotwords import HotWordEngine from threading import Event class MyWWPlugin(HotWordEngine): def __init__(self, key_phrase=\"hey mycroft\", config=None, lang=\"en-us\"): super().__init__(key_phrase, config, lang) self.detection = Event() # read config settings for your plugin self.sensitivity = self.config.get(\"sensitivity\", 0.5) # TODO - plugin stuff # how does your plugin work? phonemes? text? models? self.engine = MyWW(key_phrase) def found_wake_word(self, frame_data): \"\"\"Check if wake word has been found. Checks if the wake word has been found. Should reset any internal tracking of the wake word state. Arguments: frame_data (binary data): Deprecated. Audio data for large chunk of audio to be processed. This should not be used to detect audio data instead use update() to incrementally update audio Returns: bool: True if a wake word was detected, else False \"\"\" detected = self.detection.is_set() if detected: self.detection.clear() return detected def update(self, chunk): \"\"\"Updates the hotword engine with new audio data. The engine should process the data and update internal trigger state. Arguments: chunk (bytes): Chunk of audio data to process \"\"\" if self.engine.found_it(chunk): # TODO - check for wake word self.detection.set() def stop(self): \"\"\"Perform any actions needed to shut down the wake word engine. This may include things such as unloading data or shutdown external processess. \"\"\" self.engine.bye() # TODO - plugin specific shutdown","title":"Plugin Template"}]}